{
  "workflow": {
    "nodes": [{
      "id": "ab20c7e6-2539-4b4d-5928-86a70c06654d",
      "operation": {
        "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
        "name": "Read DataFrame"
      },
      "parameters": {
        "data source": "6f14fa48-3b95-2752-8703-5dd52ab66848"
      }
    }, {
      "id": "67b64610-52ef-3289-7b1d-a12339a7d4cb",
      "operation": {
        "id": "65240399-2987-41bd-ba7e-2944d60a3404",
        "name": "Create Custom Transformer"
      },
      "parameters": {
        "inner workflow": {
          "workflow": {
            "nodes": [{
              "id": "e8039b3f-9581-ef73-d293-03c56c620d1a",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nfrom icecream import ic\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\nfrom pathlib import Path\r\nimport pickle\r\nimport math\r\nimport pyarrow as pa\r\nfrom dask.delayed import delayed\r\nimport dask\r\nfrom datetime import datetime\r\nfrom sklearn.feature_selection import SelectFromModel\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nfrom sklearn.feature_selection import SelectKBest, chi2\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.metrics import accuracy_score\r\nfrom imblearn.under_sampling import RandomUnderSampler\r\nfrom sklearn.metrics import classification_report\r\nimport vaex\r\nimport vaex.utils\r\n\r\n\r\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\r\n    'recharge_total_cntm2',\r\n    'recharge_total_cntm3',\r\n    'm1_total_voice_usage',\r\n    'm1_total_data_usage',\r\n    'm2_total_voice_usage',\r\n    'm2_total_data_usage',\r\n    'm3_total_voice_usage',\r\n    'm3_total_data_usage',\r\n    'purchase_total_cntm1',\r\n    'purchase_total_cntm2',\r\n    'purchase_total_cntm3', \r\n    'm3_total_revenue_m2_total_revenue_pct_drop',\r\n    'm3_data_revenue_m2_data_revenue_pct_drop',\r\n    'm2_voice_rev_m1_voice_rev_pct_drop',\r\n    'm2_total_revenue_m1_total_revenue_pct_drop',\r\n    'm2_data_revenue_m1_data_revenue_pct_drop',\r\n    'm3_voice_rev_m2_voice_rev_pct_drop',\r\n    'm1_m2_m3_average_voice',\r\n    'm1_m2_m3_average_data', \r\n    'm1_no_of_days',\r\n    'm2_no_of_days',\r\n    'm3_no_of_days',\r\n    'eng_index',\r\n    'consecutive_inactive_days']\r\n\r\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\r\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\r\nMSISDN_COL_NAME = 'msisdn'\r\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\r\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\r\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\r\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \r\nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\r\n\r\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\r\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\r\nTRANSACTION_PRODUCT_NAME = 'product_id'\r\n\r\nPACK_CONTI_FEATURES = []\r\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\r\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\r\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\r\nPACK_INFO_CATEGORY = 'product_type'\r\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\r\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\r\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\r\nCUSTOMER_CATEG_FEATURES = []\r\nCUSTOMER_CONTI_FEATURES = []\r\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\r\n# 0 index for inbundle 1 index for outbundled  2 index for total\r\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\r\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\r\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\r\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\r\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\r\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\r\n    'recharge_total_cntm2',\r\n    'recharge_total_cntm3',\r\n    'm1_total_voice_usage',\r\n    'm1_total_data_usage',\r\n    'm2_total_voice_usage',\r\n    'm2_total_data_usage',\r\n    'm3_total_voice_usage',\r\n    'm3_total_data_usage',\r\n    'purchase_total_cntm1',\r\n    'purchase_total_cntm2',\r\n    'purchase_total_cntm3', \r\n    'm3_total_revenue_m2_total_revenue_pct_drop',\r\n    'm3_data_revenue_m2_data_revenue_pct_drop',\r\n    'm2_voice_rev_m1_voice_rev_pct_drop',\r\n    'm2_total_revenue_m1_total_revenue_pct_drop',\r\n    'm2_data_revenue_m1_data_revenue_pct_drop',\r\n    'm3_voice_rev_m2_voice_rev_pct_drop',\r\n    'm1_m2_m3_average_voice',\r\n    'm1_m2_m3_average_data', \r\n    'm1_no_of_days',\r\n    'm2_no_of_days',\r\n    'm3_no_of_days',\r\n    'eng_index',\r\n    'consecutive_inactive_days']\r\n\r\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\r\n\r\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\r\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\r\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\r\n\r\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\r\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\r\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\r\n\r\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\r\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\r\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\r\n\r\nLABEL1 = 'downtrend'\r\nLABEL2 = 'uptrend'\r\nLABEL3 = 'zigzag'\r\nLABEL4 = 'flat'\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_features': self.TRANSACTION_FEATURES,\r\n        'usage_features': self.CUSTOMER_FEATURES,\r\n        'pack_info_features': self.ALL_PACK_FEATURES,\r\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\r\n        'usage_dtypes': self.CUSTOMER_DTYPES\r\n\r\n    }\r\n    \r\n    \r\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\r\nsegement_names = ['trend', 'rfm']\r\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\r\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\r\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\r\netl_location = \"/home/tnmops/seahorse3_bkp/\"\r\npack_info_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location  ='/home/tnmops/seahorse3_bkp/'\r\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\r\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\r\n\r\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\r\n\r\nusage_no_months = ['m1', 'm2', 'm3']\r\nrecharge_no_months = ['m1', 'm2', 'm3']\r\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\r\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\nthreshold = 0.50\r\nnot_needed_rfm_segment = ['Lost']\r\n\r\npack_cols = ['product_id','product_type']\r\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\r\n\r\ncampaign_df= ['c4','c3']\r\ncampaign_usage= ['m4','m3']\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_location': self.purchase_location,\r\n        'usage_location': self.etl_location,\r\n        'pack_info_location': self.pack_info_location,\r\n        'ml_location': self.ml_location,\r\n        'recharge_location':self.recharge_location\r\n    }\r\n    \r\n    \r\n\r\n\r\ndef get_file_names():\r\n    return {\r\n        \"usage\": {\r\n            \"m1\": \"usage_march.csv\",\r\n            \"m2\": \"usage_feb.csv\",\r\n            \"m3\": \"usage_jan.csv\"\r\n        },\r\n        \"recharge\": {\r\n            \"m1\": \"recharge_march_20230809132937.csv\",\r\n            \"m2\": \"recharge_feb_20230809132937.csv\",\r\n            \"m3\": \"recharge_jan_20230809132937.csv\"\r\n\r\n        },\r\n        \"purchase\": {\r\n            \"m1\": \"purchase_march_full_df.csv\",\r\n            \"m2\": \"purchase_feb_full_df.csv\",\r\n            \"m3\": \"purchase_jan_full_df.csv\"\r\n\r\n        },\r\n        \"daily_summerized\": {\r\n            \"m1\": \"daily_summarized_march.csv\",\r\n            \"m2\": \"daily_summarized_feb.csv\",\r\n            \"m3\": \"daily_summarized_jan.csv\",\r\n        \r\n\r\n        },\r\n        \"weekwise\": \r\n        {\r\n        \r\n            \"m1\": \"weekwise_march_20230517180929.csv\",\r\n            \"m2\": \"weekwise_feb_20230517180929.csv\",\r\n            \"m3\": \"weekwise_jan_20230517180929.csv\",\r\n        \r\n        },\r\n        \"weekly_daily\": \r\n        {\r\n        \r\n            \"m1\": \"daily_weekly_avg_march_20230517163429.csv\",\r\n            \"m2\": \"daily_weekly_avg_feb_20230517163429.csv\",\r\n            \"m3\": \"daily_weekly_avg_jan_20230517163429.csv\",\r\n        \r\n        },\r\n        \"rfm_purchase\": \r\n        {\r\n\r\n            \"p1\": \"purchase_nov_full_df.csv\",\r\n            \"p2\": \"purchase_dec_full_df.csv\",\r\n            \"p3\": \"purchase_jan_full_df.csv\",\r\n            \"p4\": \"purchase_feb_full_df.csv\",\r\n            \"p5\": \"purchase_march_full_df.csv\"\r\n\r\n        },\r\n         \"campaign_data\": \r\n        {\r\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\r\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \r\n                        \r\n        },\r\n    \r\n        \"pack\":\r\n        {\r\n            \"pack\":\"pack_info.csv\"\r\n        },\r\n\r\n        \"profile\":\"profile_march_20230411085416.csv\"\r\n    \r\n\r\n    }\r\n    \r\n    \r\n\r\ndef segment_data(recharge_trend_usage_rfm, dag_run_id):\r\n    path_dict = {}\r\n    try:\r\n\r\n        print('inside segment_data')\r\n        #recharge_trend_usage_rfm['Segment'] = recharge_trend_usage_rfm['Segment'].apply(lambda x: str(x), vectorize=True)\r\n        for trend in recharge_trend_usage_rfm['trend'].unique():\r\n            print('inside for loop1')\r\n\r\n            for segement in recharge_trend_usage_rfm['Segment'].unique():\r\n                # if segement in cfg.Config.not_needed_rfm_segment:\r\n                #     continue\r\n                print('inside for loop2')\r\n                print('recharge_trend_usage_rfm.columns',recharge_trend_usage_rfm.columns)\r\n                for service_band in recharge_trend_usage_rfm['service_band'].unique():\r\n                    temp = recharge_trend_usage_rfm[\r\n                        (recharge_trend_usage_rfm['trend'] == trend) & (recharge_trend_usage_rfm['Segment'] == segement)& (recharge_trend_usage_rfm['service_band'] == service_band)]\r\n\r\n                    name = f\"{trend}-{segement}-{service_band}\"\r\n                    name = r\"\" + name\r\n                    file_name = f\"{name}.csv\"\r\n                    print(' file_name is ', file_name)\r\n\r\n                    length = len(temp)\r\n                    if length > 20:\r\n                        path = os.path.join(ml_location,dag_run_id,  file_name)\r\n                        print(f\"the length is suff {length} file name {name} \")\r\n                        # print('path_dict before is' ,path_dict)\r\n\r\n                        path_dict[str(name)] = str(path)\r\n                        # print('path_dict after  is' ,path_dict)\r\n\r\n   \r\n                        temp.to_csv(r\"\" + path,index=False)\r\n                        print('file_exported')\r\n\r\n                    else:\r\n                        print(f\"the length is  insuff {length} file name {name} \")\r\n        # print('here')\r\n        path_d = os.path.join(ml_location, dag_run_id,\"dict.pickle\")\r\n        print('path_d is', path_d)\r\n        print('path_dict is', path_dict)\r\n        with open(path_d, 'wb') as handle:\r\n            print('opened')\r\n            pickle.dump(path_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n            print('path_dict dumped ')\r\n\r\n    except Exception as e:\r\n        print(\"error occoured in segment_data\")\r\n        traceback.print_exc()\r\n        raise Exception(e)\r\n        \r\n        \r\ndef no_of_purchase_days_count(dag_run_id):\r\n    \r\n    msisdn_name =  MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    purchase_no_months = ['m1', 'm2', 'm3']\r\n    cdr_date = RECHARGE_TRANSACTION_PURCHASE_DATE_NAME\r\n    \r\n    print(file_name_dict)\r\n    print(\"finding no_of_purchase_days_count ongoing \")\r\n    purchase = {}\r\n    \r\n    for month in purchase_no_months:\r\n        purchase[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"purchase\").get(month)),)\r\n\r\n    purchase_3m = dd.concat(list(purchase.values()))\r\n    \r\n    purchase_3m_msisdn = purchase_3m[msisdn_name].unique().compute()\r\n    print('len(purchase_3m_msisdn) is',len(purchase_3m_msisdn) )\r\n\r\n    purchase_dict = {msisdn_name: purchase_3m_msisdn}\r\n    #print(usage_3m_dict)\r\n\r\n    print('len(usage_3m_msisdn) is',len(purchase_3m_msisdn) )\r\n        \r\n    purchase_base_df = dd.from_pandas(pd.DataFrame(purchase_dict), npartitions=1)\r\n    \r\n    \r\n    lis_df  = [purchase['m1'],purchase['m2'],purchase['m3']]\r\n    result = []\r\n    weekdays = []\r\n\r\n    def extract_day_of_week(series):\r\n            return series.dt.day_name().unique()\r\n\r\n    for i, df in enumerate(lis_df):\r\n        df[cdr_date] = dd.to_datetime(df[cdr_date])\r\n        df = df.groupby(msisdn_name)[cdr_date].apply(extract_day_of_week).reset_index()\r\n        weekdays.append(df)\r\n\r\n\r\n    for i, df in enumerate(lis_df, start =1):\r\n        new_column = f\"m{i}_no_of_days\"\r\n        df[cdr_date] = dd.to_datetime(df[cdr_date])\r\n        resultss = df.groupby(msisdn_name)[cdr_date].nunique().rename(new_column).reset_index()\r\n        result.append(resultss)\r\n    \r\n    df = purchase_base_df[[msisdn_name]].merge(result[0], on=msisdn_name, how='left').merge(result[1], on=msisdn_name, how='left').merge(result[2], on=msisdn_name, how='left').merge(weekdays[0], on=msisdn_name, how='left').merge(weekdays[1], on=msisdn_name, how='left').merge(weekdays[2], on=msisdn_name, how='left')\r\n    df = df.rename(columns={'cdr_date_x':'m1_weekdays', 'cdr_date_y': 'm2_weekdays','cdr_date': 'm3_weekdays'})\r\n    column = ['m1_weekdays','m2_weekdays','m3_weekdays']\r\n    \r\n    for col in column:\r\n        df[col] = df[col].apply(lambda x: ','.join(str(i) for i in x) if isinstance(x, list) else str(x))\r\n        df[col] = df[col].fillna(np.nan).replace('nan', '', regex=True)\r\n        \r\n    df =df.fillna(0)\r\n    \r\n    def assign_label(column):\r\n            labels = np.empty(len(column), dtype='object')\r\n            labels[(column == 0)] = 'A_[0]'\r\n            labels[(column > 0) & (column <= 5)] = 'B_[0_5]'  \r\n            labels[(column > 5 ) & (column <= 10)] = 'C_[5_10]'  \r\n            labels[(column > 10) & (column <= 15)] = 'D_[10_15]'  \r\n            labels[(column > 15) & (column <= 20)] = 'E_[15_20]'\r\n            labels[(column > 20) & (column <= 25)] = 'F_[20_25]'\r\n            labels[column >  25] = 'G_[25 +]'\r\n            return labels\r\n\r\n    columns = ['m1_no_of_days','m2_no_of_days','m3_no_of_days']\r\n    \r\n    for column in columns:\r\n        new_column = column + '_bands'\r\n        df[new_column] = df[column].map_partitions(assign_label, meta='object')\r\n    df = df.reset_index(drop =True)\r\n    \r\n    no_of_purchase_days_count_path = os.path.join(ml_location,dag_run_id, \"no_of_purchase_days_count\")\r\n    Path(no_of_purchase_days_count_path).mkdir(parents=True, exist_ok=True)\r\n    print(\"no_of_purchase_days_count   file output is ongoing \")\r\n    print('df.dtypes',df.dtypes)\r\n    df.to_parquet(no_of_purchase_days_count_path)\r\n    print(\"no_of_purchase_days_count   file output is completed \")\r\n    \r\n\r\n\r\ndef calculate_pct_drop_daily_weekly(dag_run_id):\r\n    \r\n    msisdn_name =  MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    weekly_daily_no_months = ['m1', 'm2', 'm3']\r\n    print(file_name_dict)\r\n    print(\"finding calculate_pct_drop ongoing \")\r\n    weekly_daily = {}\r\n    for month in weekly_daily_no_months:\r\n        weekly_daily[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"weekly_daily\").get(month)))\r\n                                  \r\n\r\n    usage_3m = dd.concat(list(weekly_daily.values()))\r\n    \r\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n\r\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\r\n    #print(usage_3m_dict)\r\n\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n        \r\n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\r\n    print(usage_base_df,'usage_base_df')\r\n    \r\n    m1 = weekly_daily['m1']\r\n    m2 = weekly_daily['m2']\r\n    m3 = weekly_daily['m3']\r\n   \r\n    \r\n    \r\n\r\n    etl_3_df = usage_base_df.merge(m1, on=msisdn_name, how='left').merge(m2, on=msisdn_name, how='left').merge(m3, on=msisdn_name, how='left')\r\n    print(etl_3_df)\r\n    etl_3_df = etl_3_df.fillna(0)\r\n    etl_3_df = dd.merge(dd.merge(m1, m2, on=msisdn_name), m3, on=msisdn_name)\r\n    new_column_names = {}\r\n    for col_name in etl_3_df.columns:\r\n        if col_name == msisdn_name:\r\n            new_col_name = col_name\r\n        elif col_name.endswith('_x'):\r\n            new_col_name = 'm1_' + col_name[:-2]\r\n        elif col_name.endswith('_y'):\r\n            new_col_name = 'm2_' + col_name[:-2]\r\n        else:\r\n            new_col_name = 'm3_' + col_name\r\n        new_column_names[col_name] = new_col_name\r\n\r\n    etl_3_df = etl_3_df.rename(columns=new_column_names)\r\n    etl_3_df = etl_3_df[['msisdn', 'm1_weekly_avg_voice_usage', 'm1_weekly_avg_data_usage','m1_daily_avg_voice_usage','m1_daily_avg_data_usage',\r\n                         'm2_weekly_avg_voice_usage', 'm2_weekly_avg_data_usage','m2_daily_avg_voice_usage','m2_daily_avg_data_usage',\r\n                         'm3_weekly_avg_voice_usage', 'm3_weekly_avg_data_usage','m3_daily_avg_voice_usage','m3_daily_avg_data_usage']]\r\n    df = etl_3_df.copy()\r\n    revenue_types = ['weekly_avg_voice_usage', 'weekly_avg_data_usage', 'daily_avg_voice_usage','daily_avg_data_usage']\r\n    for i in range(1, 3):\r\n        for rt in revenue_types:\r\n            col1 = f'm{i+1}_{rt}'\r\n            col2 = f'm{i}_{rt}'\r\n            pct_drop_col = f'{col1}_{col2}_pt_drop'\r\n            df[pct_drop_col] = (df[col1] - df[col2]) / df[col1] * 100\r\n            df[pct_drop_col] = df[pct_drop_col].replace([np.inf, -np.inf], np.nan)\r\n            df[pct_drop_col] = df[pct_drop_col].fillna(0)\r\n            \r\n    columns_to_select = ['msisdn'] + [col for col in df.columns if 'pt_drop' in col]\r\n    df = df.loc[:, columns_to_select]\r\n    \r\n        \r\n    def assign_label(column):\r\n        labels = np.empty(len(column), dtype='object')\r\n        labels[column == 0] = 'd)0'  \r\n        labels[(column > 0) & (column <= 50)] = 'e)0_50'  \r\n        labels[(column > 50) & (column <= 100)] = 'f)50_100'  \r\n        labels[column > 100] = 'g)>100'\r\n        labels[(column < 0) & (column >= -50)] = 'c)<0_-50' \r\n        labels[(column < -50) & (column >= -100)] = 'b)-50_-100'  \r\n        labels[column < -100] = 'a)>-100'\r\n        return labels\r\n\r\n\r\n    columns = ['m2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop','m2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop',\r\n    'm2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop','m2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop','m3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop','m3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop','m3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop','m3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop']\r\n        \r\n\r\n    for column in columns:\r\n        new_column = column + '_band'\r\n        df[new_column] = df[column].map_partitions(assign_label, meta='object')\r\n    \r\n    \r\n    daily_weekly_pct_drop = os.path.join(ml_location,dag_run_id,\"daily_weekly_pct_drop\")\r\n    Path(daily_weekly_pct_drop).mkdir(parents=True, exist_ok=True)\r\n    print(\"daily_weekly_pct_drop  file output is going on \")\r\n\r\n    df.to_parquet(daily_weekly_pct_drop)\r\n    \r\n    \r\n\r\ndef delta_calculation_weekwise(dag_run_id):\r\n    \r\n    msisdn_name =  MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    weekwise_no_months = ['m1', 'm2', 'm3']\r\n    print(file_name_dict)\r\n    print(\"finding calculate_pct_drop ongoing \")\r\n    weekwise = {}\r\n    for month in weekwise_no_months:\r\n        weekwise[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"weekwise\").get(month)))\r\n\r\n    usage_3m = dd.concat(list(weekwise.values()))\r\n    \r\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n\r\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\r\n   \r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n        \r\n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\r\n   \r\n    \r\n    \r\n\r\n    datasets = [ weekwise['m1'],weekwise['m2'],weekwise['m3']]\r\n    merged_dict = {}\r\n  \r\n    for i, df in enumerate(datasets, start=1):\r\n        dataset_prefix = f'm{i}'\r\n        voice_usage_columns = [col for col in df.columns if 'total_voice_usage' in col]\r\n        data_usage_columns = [col for col in df.columns if 'data_usage' in col]\r\n\r\n        for j in range(1, len(voice_usage_columns)):\r\n            if j != 2:  \r\n                prev_col = voice_usage_columns[j-1]\r\n                col = voice_usage_columns[j]\r\n                print('prev_col', prev_col)\r\n                print('col', col)\r\n                pct_drop_col = f'{prev_col}_{col}_pct_drop_{dataset_prefix}'\r\n                df[pct_drop_col] = ((df[prev_col] - df[col]) / df[prev_col]) * 100\r\n                df[pct_drop_col] = df[pct_drop_col].replace([np.inf, -np.inf], np.nan).fillna(0).astype(float)\r\n\r\n                prev_data_col = data_usage_columns[j-1]\r\n                data_col = data_usage_columns[j]\r\n                data_pct_drop_col = f'{prev_data_col}_{data_col}_pct_drop_{dataset_prefix}'\r\n                df[data_pct_drop_col] = ((df[prev_data_col] - df[data_col]) / df[prev_data_col]) * 100\r\n                df[data_pct_drop_col] = df[data_pct_drop_col].replace([np.inf, -np.inf], np.nan).fillna(0).astype(float)\r\n\r\n        w3_voice_pct_drop_col = f'w3_total_voice_usage_w4_total_voice_usage_pct_drop_{dataset_prefix}'\r\n        w3_data_pct_drop_col = f'w3_data_usage_w4_data_usage_pct_drop_{dataset_prefix}'\r\n\r\n        df[w3_voice_pct_drop_col] = ((df['w3_total_voice_usage'] - df['w4_total_voice_usage']) / df['w3_total_voice_usage']) * 100\r\n        df[w3_data_pct_drop_col] = ((df['w3_data_usage'] - df['w4_data_usage']) / df['w3_data_usage']) * 100\r\n\r\n        df = df.map_partitions(lambda df: df.replace([np.inf, -np.inf], np.nan).fillna(0).astype(float))\r\n\r\n       \r\n        merged_dict[dataset_prefix] = df\r\n\r\n   \r\n    merged_df = None\r\n    for key, value in merged_dict.items():\r\n        if merged_df is None:\r\n            merged_df = value\r\n        else:\r\n            merged_df = merged_df.merge(value, on=msisdn_name, suffixes=('', f'_{dataset_prefix}'))\r\n            \r\n    merged_df  =  usage_base_df[[msisdn_name]].merge(merged_df, on=msisdn_name, how='left')\r\n    merged_df = merged_df.fillna(0)\r\n    columns_to_select = ['msisdn'] + [col for col in merged_df.columns if 'pct_drop' in col]\r\n\r\n    merged_df = merged_df.loc[:, columns_to_select]\r\n    \r\n    def assign_label(column):\r\n        labels = np.empty(len(column), dtype='object')\r\n        labels[column == 0] = 'd)0'  # if 0\r\n        labels[(column > 0) & (column <= 50)] = 'e)0_50'  # 0 (exclusive) to 50 (inclusive)\r\n        labels[(column > 50) & (column <= 100)] = 'f)50_100'  # 50 (exclusive) to 100 (inclusive)\r\n        labels[column > 100] = 'g)>100'\r\n        labels[(column < 0) & (column >= -50)] = 'c)<0_-50'  # less than 0\r\n        labels[(column < -50) & (column >= -100)] = 'b)-50_-100'  # -50 (inclusive) to -100 (exclusive)\r\n        labels[column < -100] = 'a)>-100'\r\n        return labels\r\n\r\n    \r\n\r\n    \r\n    columns = ['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1',\r\n    'w1_data_usage_w2_data_usage_pct_drop_m1',\r\n    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1',\r\n    'w3_data_usage_w4_data_usage_pct_drop_m1',\r\n    'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2',\r\n    'w1_data_usage_w2_data_usage_pct_drop_m2',\r\n    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2',\r\n    'w3_data_usage_w4_data_usage_pct_drop_m2',\r\n    'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3',\r\n    'w1_data_usage_w2_data_usage_pct_drop_m3',\r\n    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3',\r\n    'w3_data_usage_w4_data_usage_pct_drop_m3']\r\n\r\n    for column in columns:\r\n        new_column = column + '_banded'\r\n        merged_df[new_column] = merged_df[column].map_partitions(assign_label, meta='object')\r\n\r\n    \r\n    weekwise_pct_drop = os.path.join(ml_location,dag_run_id,\"weekwise_pct_drop\")\r\n    Path( weekwise_pct_drop).mkdir(parents=True, exist_ok=True)\r\n    print(\" weekwise_pct_drop  file output is going on \")\r\n\r\n    merged_df.to_parquet(weekwise_pct_drop)\r\n    \r\n    \r\n\r\n\r\ndef inactive_days_band(dag_run_id):\r\n    \r\n    \r\n    date_name  =   DAILY_TRANSACTION_PURCHASE_DATE_NAME  \r\n\r\n    msisdn_name = MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    daily_summerized_month =  ['m1','m2','m3']\r\n    daily_summerized_dict = {}\r\n    for month in daily_summerized_month:\r\n        daily_summerized_dict[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"daily_summerized\").get(month)))\r\n        \r\n    m1 = daily_summerized_dict['m1']\r\n    m2 = daily_summerized_dict['m2']\r\n    m3 = daily_summerized_dict['m3']\r\n    \r\n\r\n    df = dd.concat([m1, m2, m3], axis=0)\r\n    \r\n    df[date_name] = dd.to_datetime(df[date_name])\r\n    current_date = df[date_name].max().compute()\r\n    last_active_date = df.groupby(msisdn_name)[date_name].max()\r\n    consecutive_inactive_days = (current_date - last_active_date).dt.days\r\n    inactive_days_df = consecutive_inactive_days.reset_index()\r\n    inactive_days_df.columns = ['msisdn', 'consecutive_inactive_days']\r\n    inactive_days_df = inactive_days_df[inactive_days_df['consecutive_inactive_days'] > 0]\r\n    bins = [-1, 5, 10, 15, 30, 90, float('inf')]\r\n    labels = ['a)0-5', 'b)5-10', 'c)10-15', 'd)15-30', 'e)30-90', 'f)90+']\r\n    inactive_days_df['inactive_days_band'] = inactive_days_df['consecutive_inactive_days'].map_partitions(lambda s: pd.cut(s, bins=bins, labels=labels).astype(str))\r\n    inactive_days_df['inactive_days_band'] = inactive_days_df['inactive_days_band'].astype('category')\r\n    \r\n    inactive_days_banding_path = os.path.join(ml_location,dag_run_id, \"inactive_days_banding_path\")\r\n    Path(inactive_days_banding_path).mkdir(parents=True, exist_ok=True)\r\n    print(\"inactive_days_banding_path bands  file output is going on \")\r\n\r\n    inactive_days_df.to_parquet(inactive_days_banding_path)\r\n    \r\n    \r\ndef three_month_engagement_index(dag_run_id):\r\n    msisdn_name = MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    daily_summerized_month =  ['m1','m2','m3']\r\n    daily_summerized_dict = {}\r\n    for month in daily_summerized_month:\r\n        daily_summerized_dict[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"daily_summerized\").get(month)))\r\n        \r\n    m1 = daily_summerized_dict['m1']\r\n    m2 = daily_summerized_dict['m2']\r\n    m3 = daily_summerized_dict['m3']\r\n    \r\n\r\n    df = dd.concat([m1, m2, m3], axis=0)\r\n    daily_summ_col = DAILY_TRANSACTION_PURCHASE_DATE_NAME\r\n    m1[daily_summ_col] = dd.to_datetime(m1[daily_summ_col])\r\n    m2[daily_summ_col] = dd.to_datetime(m2[daily_summ_col])\r\n    m3[daily_summ_col] = dd.to_datetime(m3[daily_summ_col])    \r\n    \r\n    \r\n    date_sum = m1[daily_summ_col].dt.day.max() + m2[daily_summ_col].dt.day.max() + m3[daily_summ_col].dt.day.max()\r\n\r\n    df_final = df[df['total_revenue'] > 0]\r\n    result = df_final.groupby(msisdn_name).fct_dt.count().reset_index()\r\n    result.columns = ['msisdn', 'fct_days']\r\n    result['eng_index'] = result['fct_days'] / date_sum\r\n    result['eng_index'] = result['eng_index'] * 100\r\n\r\n    df1 = df[df['total_revenue'] == 0]    \r\n    df1 = df1.merge(df_final[[msisdn_name]], how='left', indicator=True)\r\n    df1 = df1[df1['_merge'] == 'left_only'].drop('_merge', axis=1)\r\n\r\n\r\n    df1 = dd.from_pandas(pd.DataFrame(df1[msisdn_name].unique()), npartitions=1).reset_index(drop=True).rename(columns={0: 'msisdn'})\r\n    df1['eng_index'] = 0.0\r\n\r\n    result = result.drop('fct_days', axis=1)\r\n    f_result = dd.concat([result, df1], axis=0)\r\n    bins = [-1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 101]\r\n    labels = ['a)0_10', 'b)10_20', 'c)20_30', 'd)30_40', 'e)40_50',\r\n          'f)50_60', 'g)60_70', 'h)70_80', 'i)80_90', 'j)90_100']\r\n\r\n    f_result['eng_index_band'] = dd.from_dask_array(f_result['eng_index'].to_dask_array(), columns=['eng_index_band']).map_partitions(\r\n        lambda df: pd.cut(df['eng_index_band'], bins=bins, labels=labels))\r\n    \r\n    engagment_index_banding_path = os.path.join(ml_location, dag_run_id,\"engagment_index_banding_path\")\r\n    Path(engagment_index_banding_path).mkdir(parents=True, exist_ok=True)\r\n    print(\"engagment_index bands  file output is going on \")\r\n    f_result.to_parquet(engagment_index_banding_path)\r\n    \r\n    \r\ndef usage_banding_process(dag_run_id):\r\n\r\n    msisdn_name = MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    print(\"finding calculate_pct_drop ongoing \")\r\n    usage = {}\r\n    for month in usage_no_months:\r\n        usage[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\r\n                                    dtype=CUSTOMER_DTYPES,usecols =CUSTOMER_USAGE_COLUMNS )\r\n\r\n    usage_3m = dd.concat(list(usage.values()))\r\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n\r\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\r\n\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n        \r\n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\r\n\r\n\r\n    m1 = usage['m1']\r\n    m2 = usage['m2']\r\n    m3 = usage['m3']\r\n\r\n    \r\n    etl_3_df = usage_base_df.merge(m1, on=msisdn_name, how='left').merge(m2, on=msisdn_name, how='left').merge(m3, on=msisdn_name, how='left')\r\n    \r\n    etl_3_df = etl_3_df.fillna(0)\r\n    new_column_names = {}\r\n    for col_name in etl_3_df.columns:\r\n        if col_name == 'msisdn':\r\n            new_col_name = col_name\r\n        elif col_name.endswith('_x'):\r\n            new_col_name = 'm1_' + col_name[:-2]\r\n        elif col_name.endswith('_y'):\r\n            new_col_name = 'm2_' + col_name[:-2]\r\n        else:\r\n            new_col_name = 'm3_' + col_name\r\n        new_column_names[col_name] = new_col_name\r\n    etl_3_df = etl_3_df.rename(columns=new_column_names)\r\n    etl_3_df = etl_3_df[['msisdn', 'm1_total_voice_usage', 'm1_total_data_usage',\r\n                         'm2_total_voice_usage', 'm2_total_data_usage',\r\n                         'm3_total_voice_usage', 'm3_total_data_usage']]\r\n    df = etl_3_df.copy()\r\n    df['m1_m2_m3_average_voice'] = df[['m1_total_voice_usage', 'm2_total_voice_usage', 'm3_total_voice_usage']].mean(axis=1)\r\n    df['m1_m2_m3_average_data'] = df[['m1_total_data_usage', 'm2_total_data_usage', 'm3_total_data_usage']].mean(axis=1)\r\n    return df \r\n    \r\n    \r\n    \r\ndef usage_category(df, column_name, band_function):\r\n    df[column_name + '_band'] = df[column_name].map_partitions(lambda x: x.apply(band_function), meta=('object'))\r\n    return df[['msisdn', column_name + '_band',column_name]]\r\n\r\n\r\n\r\ndef voice_band(val):\r\n    \r\n    if val == 0.0:\r\n        return 'Zero'\r\n    elif (val > 0) and (val <= 119):\r\n        return 'low'\r\n    elif (val > 119) and (val <= 238):\r\n        return 'medium'\r\n    elif (val > 238) and (val <= 357):\r\n        return 'high'\r\n    elif val > 357:\r\n        return 'very_high'\r\n    else:\r\n        return None\r\n        \r\n        \r\ndef data_band(val):\r\n    \r\n    if val == 0.0:\r\n        return 'Zero'\r\n    elif (val > 0) and (val <= 1542.):\r\n        return 'low'\r\n    elif (val > 1542.) and (val <= 3084.):\r\n        return 'medium'\r\n    elif (val > 3084.) and (val <= 4626.):\r\n        return 'high'\r\n    elif val > 4626.:\r\n        return 'very_high'\r\n    else:\r\n        return None\r\n        \r\n        \r\n\r\ndef data_voice_usage_banding(dag_run_id):\r\n    usage_process_df = usage_banding_process(dag_run_id)\r\n    df_voice_band = usage_category(usage_process_df, 'm1_m2_m3_average_voice', voice_band)\r\n    df_data_band = usage_category(usage_process_df, 'm1_m2_m3_average_data', data_band)\r\n    usage_band_df = dd.merge(df_voice_band, df_data_band, on='msisdn')\r\n\r\n    voice_data_usage_banding_path = os.path.join(ml_location,dag_run_id, \"voice_data_usage_banding\")\r\n    Path(voice_data_usage_banding_path).mkdir(parents=True, exist_ok=True)\r\n    print(\"usage bands  file output is ongoing \")\r\n    usage_band_df.to_parquet(voice_data_usage_banding_path)\r\n    \r\n\r\ndef calculate_pct_drop(dag_run_id):\r\n\r\n    \r\n\r\n    def assign_label(column):\r\n        \r\n        labels = np.empty(len(column), dtype='object')\r\n        labels[column == 0] = 'd)0'  \r\n        labels[(column > 0) & (column <= 50)] = 'e)0_50'  \r\n        labels[(column > 50) & (column <= 100)] = 'f)50_100'  \r\n        labels[column > 100] = 'g)>100'\r\n        labels[(column < 0) & (column >= -50)] = 'c)<0_-50'  \r\n        labels[(column < -50) & (column >= -100)] = 'b)-50_-100' \r\n        labels[column < -100] = 'a)>-100'\r\n        return labels\r\n\r\n    msisdn_name = MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    print(\"finding calculate_pct_drop ongoing \")\r\n    usage = {}\r\n    for month in usage_no_months:\r\n        usage[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\r\n                                    dtype= CUSTOMER_DTYPES,usecols = CUSTOMER_DROP_COLUMNS )\r\n\r\n    usage_3m = dd.concat(list(usage.values()))\r\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n\r\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\r\n\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n        \r\n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\r\n\r\n\r\n    m1 = usage['m1']\r\n    m2 = usage['m2']\r\n    m3 = usage['m3']\r\n   \r\n    \r\n    columns = ['m2_total_revenue_m1_total_revenue_pct_drop', 'm2_voice_rev_m1_voice_rev_pct_drop', 'm2_data_revenue_m1_data_revenue_pct_drop', 'm3_total_revenue_m2_total_revenue_pct_drop', 'm3_voice_rev_m2_voice_rev_pct_drop', 'm3_data_revenue_m2_data_revenue_pct_drop']\r\n\r\n    etl_3_df = usage_base_df.merge(m1, on=msisdn_name, how='left').merge(m2, on=msisdn_name, how='left').merge(m3, on=msisdn_name, how='left')\r\n    print(etl_3_df)\r\n    etl_3_df = etl_3_df.fillna(0)\r\n    new_column_names = {}\r\n    for col_name in etl_3_df.columns:\r\n        if col_name == 'msisdn':\r\n            new_col_name = col_name\r\n        elif col_name.endswith('_x'):\r\n            new_col_name = 'm1_' + col_name[:-2]\r\n        elif col_name.endswith('_y'):\r\n            new_col_name = 'm2_' + col_name[:-2]\r\n        else:\r\n            new_col_name = 'm3_' + col_name\r\n        new_column_names[col_name] = new_col_name\r\n\r\n\r\n    etl_3_df = etl_3_df.rename(columns=new_column_names)\r\n    etl_3_df = etl_3_df[['msisdn', 'm1_total_revenue', 'm1_voice_rev', 'm1_data_revenue',\r\n                        'm2_total_revenue', 'm2_voice_rev', 'm2_data_revenue',\r\n                        'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue']]\r\n    df = etl_3_df.copy()\r\n    revenue_types = ['total_revenue', 'voice_rev', 'data_revenue']\r\n    for i in range(1, 3):\r\n        for rt in revenue_types:\r\n            col1 = f'm{i+1}_{rt}'\r\n            col2 = f'm{i}_{rt}'\r\n            pct_drop_col = f'{col1}_{col2}_pct_drop'\r\n            df[pct_drop_col] = (df[col1] - df[col2]) / df[col1] * 100\r\n            df[pct_drop_col] = df[pct_drop_col].replace([np.inf, -np.inf], np.nan)\r\n            df[pct_drop_col] = df[pct_drop_col].fillna(0)\r\n        df[pct_drop_col] = df[pct_drop_col].astype('float64')\r\n\r\n        \r\n\r\n    for column in columns:\r\n        new_column = column + '_banded'\r\n        df[new_column] = df[column].map_partitions(assign_label, meta='object')\r\n    \r\n    usage_rev_drop_path = os.path.join(ml_location,dag_run_id, \"usage_rev_drop\")\r\n    Path(usage_rev_drop_path).mkdir(parents=True, exist_ok=True)\r\n    print(\"revenue drop   file output is ongoing \")\r\n\r\n    \r\n    \r\n    expected_schema = pa.schema([\r\n    ('msisdn', pa.int64()),\r\n    ('m1_total_revenue', pa.float32()),\r\n    ('m1_voice_rev', pa.float64()),\r\n    ('m1_data_revenue', pa.float64()),\r\n    ('m2_total_revenue', pa.float32()),\r\n    ('m2_voice_rev', pa.float64()),\r\n    ('m2_data_revenue', pa.float64()),\r\n    ('m3_total_revenue', pa.float32()),\r\n    ('m3_voice_rev', pa.float64()),\r\n    ('m3_data_revenue', pa.float64()),\r\n    ('m2_total_revenue_m1_total_revenue_pct_drop', pa.float64()),\r\n    ('m2_voice_rev_m1_voice_rev_pct_drop', pa.float64()),\r\n    ('m2_data_revenue_m1_data_revenue_pct_drop', pa.float64()),\r\n    ('m3_total_revenue_m2_total_revenue_pct_drop', pa.float64()),\r\n    ('m3_voice_rev_m2_voice_rev_pct_drop', pa.float64()),\r\n    ('m3_data_revenue_m2_data_revenue_pct_drop', pa.float64()),\r\n    ('m2_total_revenue_m1_total_revenue_pct_drop_banded', pa.string()),\r\n    ('m2_voice_rev_m1_voice_rev_pct_drop_banded', pa.string()),\r\n    ('m2_data_revenue_m1_data_revenue_pct_drop_banded', pa.string()),\r\n    ('m3_total_revenue_m2_total_revenue_pct_drop_banded', pa.string()),\r\n    ('m3_voice_rev_m2_voice_rev_pct_drop_banded', pa.string()),\r\n    ('m3_data_revenue_m2_data_revenue_pct_drop_banded', pa.string()),\r\n    ('__null_dask_index__', pa.int64())\r\n    ])\r\n    \r\n    \r\n\r\n    df.to_parquet(usage_rev_drop_path, schema=expected_schema)\r\n    print(\"-------need to perquet\")\r\n    \r\n    \r\n\r\n\r\ndef get_banding_confitions():\r\n        return {\r\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\r\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\r\n                               6: \"high_high\"}\r\n        }\r\n        \r\n        \r\n\r\ndef segementation(dag_run_id):\r\n    try:\r\n        recharge_path = os.path.join(ml_location,dag_run_id,  \"recharge_band\")\r\n        recharge = vaex.open(recharge_path, )\r\n        print(\"loaded recharge\")\r\n        recharge=recharge[['msisdn','recharge_total_cntm1','recharge_total_cntm2','recharge_total_cntm3']]\r\n\r\n        path_pur = os.path.join(ml_location,dag_run_id,  \"purchase_all_months\")\r\n        pur_all_months = dd.read_parquet(path_pur)\r\n        print(\"purchase 3 months loaded\")\r\n\r\n        purchase_path = os.path.join(ml_location,dag_run_id, \"purchase_band\")\r\n        purchase = vaex.open(purchase_path)\r\n        print(\"loaded purchase\")\r\n        purchase=purchase[['msisdn','purchase_total_cntm1','purchase_total_cntm2','purchase_total_cntm3']]\r\n\r\n        path_recharge = os.path.join(ml_location,dag_run_id, \"recharge_all_months\")\r\n        recharge_all_months = dd.read_parquet(path_recharge)\r\n        print(\"recharge 3 months loaded\")\r\n\r\n        trend_path = os.path.join(ml_location,dag_run_id, \"trend_filtered\")\r\n        trend = vaex.open(trend_path)\r\n        print(\"loaded trend\")\r\n        #trend = trend[['msisdn', 'tot_revm1', 'tot_revm2', 'tot_revm3']]\r\n        trend = trend[['msisdn', 'rev_segment_m1', 'rev_segment_m2', 'rev_segment_m3', 'trend']]\r\n        replace_map = get_banding_confitions().get(\"common_reverse\")\r\n\r\n        trend['rev_segment_m1'] = trend.rev_segment_m1.map(replace_map)\r\n        trend['rev_segment_m2'] = trend.rev_segment_m2.map(replace_map)\r\n        trend['rev_segment_m3'] = trend.rev_segment_m3.map(replace_map)\r\n\r\n        usage_path = os.path.join(ml_location, dag_run_id, \"usage_bands\")\r\n        usage = vaex.open(usage_path)\r\n        usage=usage[['msisdn','m1_total_voice_usage','m1_total_data_usage','m2_total_voice_usage', 'm2_total_data_usage','m3_total_voice_usage', 'm3_total_data_usage']]\r\n        print(\"loaded usage\")\r\n\r\n        rfm_path = os.path.join(ml_location,dag_run_id, \"rfm\")\r\n        rfm = vaex.open(rfm_path)\r\n        rfm = rfm[['msisdn', 'Segment']]\r\n        print(\"loaded rfm\")\r\n        # ------------nnew logic----------------#\r\n        # trend_rfm = trend.join(rfm, on='msisdn', how=\"inner\")\r\n        trend_rfm = trend.copy()\r\n        trend_rfm=trend_rfm[['msisdn','trend']]\r\n        trend_rfm_recharge = trend_rfm.join(recharge, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge['recharge_total_cntm1'] = trend_rfm_recharge['recharge_total_cntm1'].fillna(0)\r\n        trend_rfm_recharge['recharge_total_cntm2'] = trend_rfm_recharge['recharge_total_cntm2'].fillna(0)\r\n        trend_rfm_recharge['recharge_total_cntm3'] = trend_rfm_recharge['recharge_total_cntm3'].fillna(0)\r\n        trend_rfm_recharge_usage = trend_rfm_recharge.join(usage, on='msisdn', how=\"left\")\r\n\r\n\r\n         #to fill null values if there is in data and voice band \r\n        trend_rfm_recharge_usage['m1_total_voice_usage'] = trend_rfm_recharge_usage['m1_total_voice_usage'].fillna(0)\r\n        trend_rfm_recharge_usage['m1_total_data_usage'] = trend_rfm_recharge_usage['m1_total_data_usage'].fillna(0)\r\n        trend_rfm_recharge_usage['m2_total_voice_usage'] = trend_rfm_recharge_usage['m1_total_voice_usage'].fillna(0)\r\n        trend_rfm_recharge_usage['m2_total_data_usage'] = trend_rfm_recharge_usage['m2_total_data_usage'].fillna(0)\r\n        trend_rfm_recharge_usage['m3_total_voice_usage'] = trend_rfm_recharge_usage['m1_total_voice_usage'].fillna(0)\r\n        trend_rfm_recharge_usage['m3_total_data_usage'] = trend_rfm_recharge_usage['m3_total_data_usage'].fillna(0)\r\n\r\n        #to fill null values if there is in data and voice band \r\n\r\n\r\n\r\n        trend_rfm_recharge_usage_purchase = trend_rfm_recharge_usage.join(purchase, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase['purchase_total_cntm1'] = trend_rfm_recharge_usage_purchase[\r\n            'purchase_total_cntm1'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase['purchase_total_cntm2'] = trend_rfm_recharge_usage_purchase[\r\n            'purchase_total_cntm2'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase['purchase_total_cntm3'] = trend_rfm_recharge_usage_purchase[\r\n            'purchase_total_cntm3'].fillna(0)\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase.join(rfm, on='msisdn', how=\"inner\")\r\n\r\n        #joining usage_drop_percentage\r\n        calculate_pct_drop(dag_run_id)\r\n        usage_rev_drop_path = os.path.join(ml_location,dag_run_id, \"usage_rev_drop\")\r\n        usage_drop_percentage = vaex.open(usage_rev_drop_path)\r\n        usage_drop_percentage = usage_drop_percentage[['msisdn','m3_total_revenue_m2_total_revenue_pct_drop', \r\n                                                       'm3_data_revenue_m2_data_revenue_pct_drop', \r\n                                                       'm2_voice_rev_m1_voice_rev_pct_drop', \r\n                                                       'm2_total_revenue_m1_total_revenue_pct_drop', \r\n                                                       'm2_data_revenue_m1_data_revenue_pct_drop', \r\n                                                       'm3_voice_rev_m2_voice_rev_pct_drop']]\r\n\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.join(usage_drop_percentage, on='msisdn', how=\"left\")\r\n       \r\n        #joining usage_drop_percentage\r\n\r\n        #joining usage bands\r\n        data_voice_usage_banding(dag_run_id)\r\n        voice_data_usage_banding_path = os.path.join(ml_location,dag_run_id, \"voice_data_usage_banding\")\r\n        voice_data_usage_banding = vaex.open(voice_data_usage_banding_path)\r\n        voice_data_usage_banding=voice_data_usage_banding[['msisdn','m1_m2_m3_average_voice','m1_m2_m3_average_data']]\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.join(voice_data_usage_banding, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_voice']=trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_voice'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_data']=trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_data'].fillna(0)\r\n        #joining usage bands\r\n\r\n\r\n        #joining engagement index \r\n        three_month_engagement_index(dag_run_id)\r\n        engagment_index_banding_path = os.path.join(ml_location,dag_run_id, \"engagment_index_banding_path\")\r\n        engagment_index_banding = vaex.open(engagment_index_banding_path)\r\n\r\n        engagment_index_banding=engagment_index_banding[['msisdn','eng_index']]\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.join(engagment_index_banding, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['eng_index']=trend_rfm_recharge_usage_purchase_rfm['eng_index'].fillna(0)\r\n\r\n\r\n        #joining engagement index \r\n\r\n\r\n        #joining purchase rfm\r\n\r\n        rfm__purchase_path = os.path.join(ml_location,dag_run_id, \"rfm_purchase\")\r\n        rfm_purchase = vaex.open(rfm__purchase_path)\r\n        print('type(rfm_purchase)',type(rfm_purchase))\r\n        rfm_purchase.rename('RFM_Segment', 'rfm_purchase_segment', unique=False)\r\n        rfm_purchase = rfm_purchase[['msisdn', 'rfm_purchase_segment']]\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(rfm_purchase, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['rfm_purchase_segment']=trend_rfm_recharge_usage_purchase_rfm['rfm_purchase_segment'].fillna(000)\r\n\r\n        \r\n        #joining purchase rfm\r\n\r\n\r\n         #joining inactivity \r\n\r\n        inactive_days_band(dag_run_id)\r\n        inactive_days_banding_path = os.path.join(ml_location,dag_run_id,\"inactive_days_banding_path\")\r\n        inactivity_days_df = vaex.open(inactive_days_banding_path)\r\n        inactivity_days_df=inactivity_days_df[['msisdn', 'consecutive_inactive_days']]\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(inactivity_days_df, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['consecutive_inactive_days']=trend_rfm_recharge_usage_purchase_rfm['consecutive_inactive_days'].fillna(0)\r\n\r\n\r\n        #joining inactivity \r\n\r\n        #joining weekly delta change\r\n        delta_calculation_weekwise(dag_run_id)\r\n        weekwise_pct_drop = os.path.join(ml_location,dag_run_id,\"weekwise_pct_drop\")\r\n        weekwise_pct_drop_df = vaex.open(weekwise_pct_drop)\r\n        weekwise_pct_drop_df=weekwise_pct_drop_df[['msisdn',\r\n            'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3', \r\n            'w1_data_usage_w2_data_usage_pct_drop_m3',\r\n             'w3_data_usage_w4_data_usage_pct_drop_m2',\r\n              'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3',\r\n               'w1_data_usage_w2_data_usage_pct_drop_m1',\r\n                'w1_data_usage_w2_data_usage_pct_drop_m2',\r\n                 'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1',\r\n                  'w3_data_usage_w4_data_usage_pct_drop_m1',\r\n                   'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1',\r\n                    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2',\r\n                      'w3_data_usage_w4_data_usage_pct_drop_m3',\r\n                        'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2']]\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(weekwise_pct_drop_df, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m3'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m2'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m1'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m2'].fillna(0)        \r\n        trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1'].fillna(0)        \r\n        trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m1'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m3'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2'].fillna(0)\r\n\r\n        #joining weekly delta change\r\n\r\n\r\n        #joining weekly-daily delta change\r\n\r\n\r\n        calculate_pct_drop_daily_weekly(dag_run_id)\r\n\r\n        daily_weekly_pct_drop = os.path.join(ml_location,dag_run_id,\"daily_weekly_pct_drop\")\r\n        daily_weekly_pct_drop_df = vaex.open(daily_weekly_pct_drop)\r\n        daily_weekly_pct_drop_df=daily_weekly_pct_drop_df[['msisdn',\r\n           'm2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop',\r\n             'm2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop',\r\n               'm3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop',\r\n                 'm3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop',\r\n                   'm2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop',\r\n                     'm3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop',\r\n                       'm2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop',\r\n                         'm3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop']]\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(daily_weekly_pct_drop_df, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop'].fillna(0)       \r\n        trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop'].fillna(0)\r\n\r\n\r\n        #joining weekly-daily delta change\r\n\r\n\r\n        #joining purchase number of days \r\n        no_of_purchase_days_count(dag_run_id)\r\n        no_of_purchase_days_count_path = os.path.join(ml_location, dag_run_id,\"no_of_purchase_days_count\")\r\n        print('opening no_of_purchase_days_count file'  )\r\n        no_of_purchase_days_count_df = vaex.open(no_of_purchase_days_count_path)\r\n        no_of_purchase_days_count_df=no_of_purchase_days_count_df[['msisdn', 'm1_no_of_days',\r\n       'm2_no_of_days', 'm3_no_of_days']]\r\n        \r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(no_of_purchase_days_count_df, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['m1_no_of_days']= trend_rfm_recharge_usage_purchase_rfm['m1_no_of_days'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m2_no_of_days']= trend_rfm_recharge_usage_purchase_rfm['m2_no_of_days'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m3_no_of_days']= trend_rfm_recharge_usage_purchase_rfm['m3_no_of_days'].fillna(0)\r\n        #joining purchase number of days \r\n\r\n        #adding usage service\r\n        service_path = os.path.join(ml_location,dag_run_id,\"favorite_Service.csv\")\r\n        print('opening favorite_Service file'  )\r\n        usage_service = vaex.open(service_path)\r\n        usage_service=usage_service[['msisdn','service_band']]\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(usage_service, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['service_band']=trend_rfm_recharge_usage_purchase_rfm['service_band'].fillna('no_usage')\r\n       \r\n\r\n\r\n\r\n         #adding usage service\r\n        \r\n        #exporting and adding the kpi segments to db\r\n        print('APA_kpi_segments_analysis file going to export'  )\r\n        print('trend_rfm_recharge_usage_purchase_rfm.columns',trend_rfm_recharge_usage_purchase_rfm.columns)\r\n        print('trend_rfm_recharge_usage_purchase_rfm.dtypes',trend_rfm_recharge_usage_purchase_rfm.dtypes)\r\n        trend_rfm_recharge_usage_purchase_rfm.export_csv(os.path.join(ml_location, dag_run_id,\"APA_kpi_segments_analysis.csv\"))\r\n        print('APA_kpi_segments_analysis file exported '  )\r\n        # for df_chunk in pd.read_csv(os.path.join(ml_location, dag_run_id,\"APA_kpi_segments_analysis.csv\"),\r\n        #                             chunksize=5000):\r\n        #     # insert each chunk into database table\r\n        #     df_chunk['dag_run_id']=str(dag_run_id)\r\n        #     df_chunk.to_sql('APA_kpi_segments_analysis_num', engine, if_exists='append', index=False)\r\n\r\n        \r\n\r\n\r\n       \r\n        # trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.extract()\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.to_pandas_df()\r\n        print('type(trend_rfm_recharge_usage_purchase_rfm)',type(trend_rfm_recharge_usage_purchase_rfm))\r\n        segment_data(trend_rfm_recharge_usage_purchase_rfm, dag_run_id)\r\n\r\n        ic(\"mergeing rechare and trend  and rfm \", trend_rfm_recharge_usage_purchase_rfm['msisdn'].nunique())\r\n\r\n      \r\n\r\n        df = trend_rfm_recharge_usage_purchase_rfm.groupby([\"trend\", 'Segment']).agg({\"msisdn\": \"count\"})\r\n        df.to_csv(os.path.join(ml_location,dag_run_id, \"trend_segment.csv\"),index=False)\r\n        # ------------------------inner join logic  end-------------------------------#\r\n        # df = trend_rfm_recharge_usage_purchase.groupby([\"trend\", 'Segment']).agg({\"msisdn\": \"count\"})\r\n        # df.export_csv(os.path.join(cfg.Config.ml_location, dag_run_id, \"trend_segment.csv\"))\r\n        print(\"done with segmentation\")\r\n        return df\r\n\r\n        # segment_data_with_rfm(trend_rfm_recharge_usage_purchase, recharge_all_months.copy(), dag_run_id)\r\n    except Exception as e:\r\n        print(e)\r\n        \r\n        \r\n    \r\n\r\n\r\n\r\ndef transform(dataframe):\r\n    d=segementation(\"manual__2023-07-10T11:06:51\")\r\n    return d"
              }
            }, {
              "id": "e4921898-f8ed-fb4f-c568-fa70ed6bb972",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n    \n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n    \n    \ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_march_20230809132937.csv\",\n            \"m2\": \"recharge_feb_20230809132937.csv\",\n            \"m3\": \"recharge_jan_20230809132937.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n\n\nclass RechargeBanding(object):\n    def __init__(self, data):\n        self.data = data\n        # self.categorize()\n\n    def count_category(self, x, m):\n\n        if x == 1:\n            resp = 'b)1'\n        elif 1 < x <= 4:\n            resp = 'c)1-4'\n        elif 4 < x <= 10:\n            resp = 'd)4-10'\n        elif 10 < x <= 30:\n            resp = 'e)10-30'\n        elif x > 30:\n            resp = 'f)30 above'\n        else:\n            resp = 'a)zero'\n        return m + \"_\" + resp\n\n    def categorize(self):\n\n        months = recharge_no_months\n        recharge_count_col_name = RECHARGE_COUNT_COL_NAME\n        msisdn_name = MSISDN_COL_NAME\n        temp_df = None\n        for month in months:\n            needed_col = RECHARGE_NEEDED_COL\n            dataset = self.data.get(month)[needed_col]\n            dataset = dataset.groupby([msisdn_name]).agg({recharge_count_col_name: 'sum'})\n            dataset['count_pattern'] = dataset[recharge_count_col_name].apply(self.count_category, args=(month,))\n            dataset['recharge_count_pattern_' + month] = dataset['count_pattern']\n            dataset['recharge_' + recharge_count_col_name + month] = dataset[recharge_count_col_name]\n            dataset = dataset.drop(recharge_count_col_name, axis=1)\n            dataset = dataset.drop('count_pattern', axis=1)\n            if temp_df is None:\n                temp_df = dataset\n            else:\n                temp_df = temp_df.merge(dataset, on=msisdn_name, how=\"left\")\n                temp_df[\"recharge_count_pattern_\" + month] = temp_df[\"recharge_count_pattern_\" + month].fillna(month+\"_a)zero\")\n                temp_df['recharge_' +recharge_count_col_name + month]=temp_df['recharge_' +recharge_count_col_name + month].fillna(0)\n\n        return temp_df.reset_index()\n        \n        \n\n\n\ndef recharge_process(dag_run_id):\n    try:\n        file_name_dict = get_file_names()\n        print(\"rechare preprocess  ongoing \")\n        data = {}\n        dtype_recharge = RECHARGE_DTYPES\n        for month in recharge_no_months:\n            data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"recharge\").get(month)),\n                                      dtype= RECHARGE_DTYPES)\n\n        # ic(\"the length of m1 in recharge \", len(data['m1']))\n        # ic(\"the length of m2 in recharge \", len(data['m2']))\n        # ic(\"the length of m3 in recharge \", len(data['m3']))\n        #\n        # ic(\"the length of unique msisdn m3 in recharge \", data['m1']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in recharge \", data['m2']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in recharge \", data['m3']['msisdn'].nunique().compute())\n        df_data = dd.concat(list(data.values()))\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        path_recharge = os.path.join(ml_location, dag_run_id, \"recharge_all_months\")\n        Path(path_recharge).mkdir(parents=True, exist_ok=True)\n        df_data.to_parquet(path_recharge)\n        rb = RechargeBanding(data)\n        print(\"recharge categorizing ongoing \")\n        df = rb.categorize()\n        print(\"recharge categorizing done \")\n        path = os.path.join(ml_location, dag_run_id, \"recharge_band\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(\"recharge categorizing  to file op ongoing \")\n        df.to_parquet(path)\n        print(\"recharge categorizing  to file op done  \")\n        return df.compute()\n    except Exception as e:\n        print(e)\n        \n        \n\n\ndef transform(dataframe):\n    df = recharge_process(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }, {
              "id": "d6ce0a6e-df3f-4da8-07a7-bed116711940",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom fastapi import Depends, FastAPI, HTTPException\nimport pathlib\nimport dask.dataframe as dd\nimport datetime\nimport dask.array as da\nfrom pathlib import Path\nfrom icecream import ic\nimport  traceback\nfrom datetime import datetime\ngeneric_dict = {\"numerical_col\": \"Revenue\"}\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n    \ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \ndef join_rfm(x):\n    return str(int(x['R_Score'])) + str(int(x['F_Score'])) + str(int(x['M_Score']))\n    \ndef segmentaion_fun1(r, f, m):\n    segments = []\n    for r, f, m in zip(r, f, m):\n        value = int(f\"{r}{f}{m}\")\n\n        if value in [555, 554, 544, 545, 454, 455, 445]:\n            segments.append(\"Champions\")\n        elif value in [543, 444, 435, 355, 354, 345, 344, 335]:\n            segments.append(\"Loyal_Customers\")\n        elif value in [553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323]:\n            segments.append(\"Potential_Loyalist\")\n        elif value in [512, 511, 422, 421, 412, 411, 311]:\n            segments.append(\"Recent_Customers\")\n        elif value in [525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313]:\n            segments.append(\"Promising_Customers\")\n        elif value in [535, 534, 443, 434, 343, 334, 325, 324]:\n            segments.append(\"Customers_needing_Attention\")\n        elif value in [331, 321, 312, 221, 213]:\n            segments.append(\"About_to_Sleep\")\n        elif value in [255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124]:\n            segments.append(\"At_Risk\")\n        elif value in [155, 154, 144, 214, 215, 115, 114, 113]:\n            segments.append(\"Cant_Loose_them\")\n        elif value in [332, 322, 231, 241, 251, 233, 232, 223, 222, 132, 123, 122, 212, 211]:\n            segments.append(\"Hibernating\")\n        else:\n            segments.append(\"Lost\")\n\n    return segments\n    \n    \ndef form_segements_purchase_rfm(ctm_class):\n    def process_duplicates(li):\n        for i in range(len(li) - 1):\n            print(li[i], i)\n            if i + 1 == len(li) - 1:\n                li[i] = ((li[i - 1] + li[i]) / 2)\n            elif li[i] == li[i + 1]:\n                li[i + 1] = ((li[i + 1] + li[i + 2]) / 2)\n        return li\n\n    # bins_recency = [-1,\n    #                 np.percentile(ctm_class[\"Recency\"], 20),\n    #                 np.percentile(ctm_class[\"Recency\"], 40),\n    #                 np.percentile(ctm_class[\"Recency\"], 60),\n    #                 np.percentile(ctm_class[\"Recency\"], 80),\n    #                 ctm_class[\"Recency\"].max().compute()]\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n    # # bins_recency = [-1,17,35,53,71, ctm_class[\"Recency\"].max().compute()]\n\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Recency']=ctm_class1['Recency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Recency').agg({'msisdn':'count'}).reset_index()\n    mean_recency = ctm_class1['Recency'].max().compute()/5\n    print(\"mean_recency\", mean_recency)\n    print(\"mean_recency *2\",mean_recency * 2)\n    print(\"mean_recency *3\",mean_recency * 3)\n    print(\"mean_recency *4\",mean_recency * 4)\n    \n    \n    bins_recency = [-1,\n                    int(mean_recency),\n                    int(mean_recency * 2),\n                    int(mean_recency * 3),\n                    int(mean_recency * 4),\n                    ctm_class[\"Recency\"].max().compute()]\n    \n    print('bins_recency is', bins_recency)\n\n\n    ctm_class['R_Score'] = ctm_class[\"Recency\"].map_partitions(pd.cut,\n                                                               bins=bins_recency,\n                                                               labels=[5, 4, 3, 2, 1]).astype(\"int\")\n\n    \n\n    # bins_frequency = [-1,\n    #                   np.percentile(ctm_class[\"Frequency\"], 20),\n    #                   np.percentile(ctm_class[\"Frequency\"], 40),\n    #                   np.percentile(ctm_class[\"Frequency\"], 60),\n    #                   np.percentile(ctm_class[\"Frequency\"], 80),\n    #                   ctm_class[\"Frequency\"].max().compute()]\n\n    # bins_frequency.sort()\n    # bins_frequency = process_duplicates(bins_frequency)\n\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Frequency']=ctm_class1['Frequency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Frequency').agg({'msisdn':'count'}).reset_index()\n    mean_frequency = ctm_class1['Frequency'].max().compute()/5\n    print(\"mean_frequency\", mean_frequency)\n    print(\"mean_frequency *2\",mean_frequency * 2)\n    print(\"mean_frequency *3\",mean_frequency * 3)\n    print(\"mean_frequency *4\",mean_frequency * 4)\n\n\n    bins_frequency = [-1,\n                    int(mean_frequency),\n                    int(mean_frequency * 2),\n                    int(mean_frequency * 3),\n                    int(mean_frequency * 4),\n                    ctm_class[\"Frequency\"].max().compute()]\n    \n    print('bins_frequency is', bins_frequency)\n    ctm_class['F_Score'] = ctm_class[\"Frequency\"].map_partitions(pd.cut,\n                                                                 bins=bins_frequency,\n                                                                 labels=[1, 2, 3, 4, 5]).astype(\"int\")\n\n    # bins_revenue = [-1,\n    #                 np.percentile(ctm_class[\"Revenue\"], 20),\n    #                 np.percentile(ctm_class[\"Revenue\"], 40),\n    #                 np.percentile(ctm_class[\"Revenue\"], 60),\n    #                 np.percentile(ctm_class[\"Revenue\"], 80),\n    #                 ctm_class[\"Revenue\"].max().compute()]\n    # bins_revenue.sort()\n    # bins_revenue = process_duplicates(bins_revenue)\n\n    # bins_revenue = [-1,189,380,571,762, ctm_class[\"Revenue\"].max().compute()]\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Revenue'] = ctm_class1['Revenue'].map_partitions(lambda series: series.apply(np.ceil))\n    ctm_class1['Revenue']=ctm_class1['Revenue'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Revenue').agg({'msisdn':'count'}).reset_index()\n    mean_revenue = ctm_class1['Revenue'].max().compute()/5\n    \n    print(\"mean_revenue\", mean_revenue)\n    print(\"mean_revenue *2\",mean_revenue * 2)\n    print(\"mean_revenue *3\",mean_revenue * 3)\n    print(\"mean_revenue *4\",mean_revenue * 4)\n    \n    bins_revenue = [-1,\n                    int(mean_revenue),\n                    int(mean_revenue * 2),\n                    int(mean_revenue * 3),\n                    int(mean_revenue * 4),\n                    ctm_class[\"Revenue\"].max().compute()]\n    \n    print('bins_revenue is', bins_revenue)\n\n\n    ctm_class['M_Score'] = ctm_class[\"Revenue\"].map_partitions(pd.cut,\n                                                               bins=bins_revenue,\n                                                               labels=[1, 2, 3, 4, 5]).astype(\"int\")\n    print(\"done with scoring\")\n    # Form RFM segment\n\n    ctm_class['RFM_Segment'] = ctm_class.apply(join_rfm, axis=1)\n    ctm_class['RFM_Segment'] = ctm_class['RFM_Segment'].astype(int)\n    print(\"formed rfm segement \")\n    ctm_class['R_Score'] = ctm_class['R_Score'].astype(int)\n    ctm_class['F_Score'] = ctm_class['F_Score'].astype(int)\n    ctm_class['M_Score'] = ctm_class['M_Score'].astype(int)\n    print(\"computing rfm\")\n    r = ctm_class['R_Score'].values.compute()\n    f = ctm_class['F_Score'].values.compute()\n    m = ctm_class['M_Score'].values.compute()\n    seg = segmentaion_fun1(r, f, m)\n    chunks = ctm_class.map_partitions(lambda x: len(x)).compute().to_numpy()\n    myarray = da.from_array(seg, chunks=tuple(chunks))\n    ctm_class['Segment'] = myarray\n    return ctm_class\n    \n    \ndef otliner_removal(df,col, per=0.97):\n    q = df[col].quantile(per)\n    print(f\"col is {col} and q is {q}\")\n    print(\"the length brfore is\", len(df))\n    outliers = df[df[col] > q]\n    df1 = df[~df['msisdn'].isin(outliers['msisdn'].compute().values)]\n    print(\"the length after is\", len(df1))\n    return df1\n    \n\ndef perform_rfm_on_purchase(df_data_rfm, period=95):\n\n    ic(\"inside perform_rfm\")\n    \n    df_data_rfm.purchase_date = dd.to_datetime(df_data_rfm.purchase_date)\n    df_data_rfm = df_data_rfm.fillna(0)\n    msisdn_name = MSISDN_COL_NAME\n    # current behaviour data\n    min_date = df_data_rfm['purchase_date'].min().compute()\n    cur_beh_data_rfm = df_data_rfm[(df_data_rfm.purchase_date <= pd.Timestamp(min_date) + pd.Timedelta(days=period))\n                                   & (df_data_rfm.purchase_date >= pd.Timestamp(min_date))].reset_index(drop=True)\n    # Get the maximum purchase date of each customer and create a dataframe with it together with the customer's id.\n    ctm_max_purchase = cur_beh_data_rfm.groupby(msisdn_name).purchase_date.max().reset_index()\n    ctm_max_purchase.columns = [msisdn_name, 'MaxPurchaseDate']\n\n    # Find the recency of each customer in days\n    ctm_max_purchase['Recency'] = (\n            ctm_max_purchase['MaxPurchaseDate'].max() - ctm_max_purchase['MaxPurchaseDate']).dt.days\n    ctm_max_purchase = ctm_max_purchase.drop(columns=['MaxPurchaseDate'])\n    print(\"done with recency \")\n    # frequency\n    ctm_frequency = cur_beh_data_rfm.groupby(msisdn_name).total_cnt.sum().reset_index()\n    ctm_frequency.columns = [msisdn_name, 'Frequency']\n    print(\"done with frequency \")\n    cur_beh_data_rfm['Revenue'] = cur_beh_data_rfm[RECHARGE_TRANSACTION_PRICE_COL_NAME]\n    ctm_revenue = cur_beh_data_rfm.groupby('msisdn').Revenue.sum().reset_index()\n    print(\"done with monitory \")\n    rfm_data_base = dd.concat([ctm_max_purchase, ctm_revenue, ctm_frequency], axis=1)\n    rfm_data_base = rfm_data_base.loc[:, ~rfm_data_base.columns.duplicated()]\n\n    #for tnm purpose start\n    #rfm_data_base=otliner_removal(rfm_data_base,'Frequency')\n    rfm_data_base=otliner_removal(rfm_data_base,'Revenue')\n    \n    #for tnm purpose end \n\n\n    return rfm_data_base\n    \n\n\ndef arpu_trend(data):\n    months = usage_no_months\n    temp_df = None\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    #for daily summarised\n\n    usage={}\n\n    for month in usage_no_months:\n        usage[month] = dd.read_csv(\n            os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n        \n    usage_3m = dd.concat(list(usage.values()))\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    temp_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n\n    print('len(temp_df) is',len(temp_df) )\n\n    #for daily summarised\n\n\n\n    for month in months:\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n        # total_voice_rev = f.Features.CUSTOMER_VOICE_REVENUE[2]\n        needed_col = [MSISDN_COL_NAME, total_revenue]\n        df = data.get(month)[needed_col]\n        df['tot_rev'] = df[total_revenue]\n        df = df.drop(total_revenue, axis=1)\n        df = df.fillna(0)\n        #df = otliner_removal(df)\n\n        df['rev_segment'] = df.tot_rev.apply(put_revenue_segement)\n        df = df.rename(columns={\"rev_segment\": \"rev_segment_\" + month})\n        df['tot_rev' + month] = df['tot_rev']\n\n        df = df.drop('tot_rev', axis=1)\n        \n\n        if temp_df is None:\n            temp_df = df\n        else:\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n            temp_df[\"rev_segment_\" + month] = temp_df[\"rev_segment_\" + month].fillna(\"Zero\")\n            temp_df[\"tot_rev\" + month] = temp_df[\"tot_rev\" + month].fillna(0)\n\n    temp_df['tot_sum'] = temp_df['tot_revm1'] + temp_df['tot_revm2'] + temp_df['tot_revm3']\n    temp_df_non = temp_df\n    print(\"len of temp_df_non  after filtering is is  \", len(temp_df_non))\n    replace_map = get_banding_confitions().get(\"common\")\n\n    df1 = temp_df_non.replace(replace_map)\n    print(type(temp_df_non))\n\n    #to convert rev_segment_cols to int \n    rev_segment_cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    df1[rev_segment_cols] = df1[rev_segment_cols].astype('int32')\n    #to convert rev_segment_cols to int \n\n    cols = df1.columns[df1.columns.str.startswith('tot_revm')].tolist()\n\n    #to find trend using segment\n    #cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    #to find trend using segment\n\n    for col in cols:\n        df1[col] = df1[col].astype('int')\n        df1[col] = df1[col]\n\n    output = df1[cols].map_partitions(lambda part: part.apply(lambda x: pandas_wrapper(x), axis=1), meta=tuple)\n    output1 = output.to_frame(name='trend')\n    op = dd.concat([df1, output1], axis=1)\n\n    return op\n    \n    \ndef get_banding_confitions():\n        return {\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\n                               6: \"high_high\"}\n        }\n        \n        \n\n\ndef put_revenue_segement(x):\n    y = 'Zero'\n    if x == 0.0000:\n        y = 'Zero'\n    elif (x > 0) & (x <= 16):\n        y = 'very_low'\n    elif (x > 16) & (x <= 36):\n        y = 'low'\n    elif (x > 36) & (x <= 61):\n        y = 'medium'\n    elif (x > 61) & (x <= 175):\n        y = 'high'\n    else:\n        y = 'high_high'\n    return y\n    \n    \n    \ndef trends(numbers):\n    if len(numbers) != 3:\n        return \"Invalid input\"\n\n    if numbers[0] == numbers[1] == numbers[2]:\n        return \"flat\"\n    elif numbers[0] > numbers[1] > numbers[2]:\n        return \"Downtrend\"\n    elif numbers[0] < numbers[1] < numbers[2]:\n        return \"Uptrend\"\n    else:\n        return \"Zigzag\"\n        \n        \n\n\ndef pandas_wrapper(row):\n    return trends([row[2], row[1], row[0]])\n    \n    \ndef rfm_process_quantile_method_purchase(dag_run_id):\n    try:\n        msisdn_name = MSISDN_COL_NAME\n        \n        \n        file_name_dict = get_file_names()\n        dtype_purchase = RECHARGE_TRANSACTION_DTYPES\n        data = {}\n        print('purchase is going to  read')\n        for month in purchase_no_months:\n            data[month] = dd.read_csv(\n                os.path.join(purchase_location, file_name_dict.get(\"purchase\").get(month)),\n                dtype=dtype_purchase)\n        print('purchase readed')\n        df_data = dd.concat(list(data.values()))\n\n        #for daily summarised\n\n        usage={}\n\n        for month in usage_no_months:\n            usage[month] = dd.read_csv(\n                os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n            \n        #usage_3m = dd.concat(list(usage.values()))\n        trend_df = arpu_trend(usage)\n        trend_df = trend_df.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n        df_data = df_data[df_data[msisdn_name].isin(trend_df[msisdn_name].compute().unique())]\n        # profile\n        dtype = {'account_type': 'float64',\n                 'current_state': 'float64',\n                 'no_of_balances': 'float64',\n                 'primary_offer_id': 'float64',\n                 'subscr_no': 'float64',\n                 'payment_type':'object'}\n        profile = dd.read_csv(os.path.join(purchase_location, file_name_dict.get(\"profile\")), dtype=dtype)\n        profile[PROFILE_DATE_ENTER_ACTIVE] = dd.to_datetime(profile[PROFILE_DATE_ENTER_ACTIVE],\n                                                                       errors='coerce')\n\n        date_threshold = datetime.strptime('2023-02-12', '%Y-%m-%d')\n\n        profile['aon'] = (date_threshold - profile[PROFILE_DATE_ENTER_ACTIVE]).dt.days\n\n        \n        profile = profile[profile['aon'] >=90]\n        profile = profile[profile['current_state'] ==2]\n        profile = profile[profile['payment_type'] =='Prepaid']\n        print('df.columns',df_data.columns)\n        # df_data = df_data.merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n        #                                                               how='inner')\n        df_data=df_data[df_data['msisdn'].isin(profile['msisdn'].compute().unique())]\n\n        #for daily summarised\n\n        df_data = df_data.fillna(0, )\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        ctm_class = perform_rfm_on_purchase(df_data, period=95)#rfm using daily summarised\n        path = os.path.join(ml_location,dag_run_id,\"rfm_before_segementation_purchase\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n        ctm_class = form_segements_purchase_rfm(ctm_class)\n        # Apply the function to the 'Segment' column\n        print('type(ctm_class)',type(ctm_class))\n\n        #ctm_class['Segment'] = ctm_class['Segment'].map(lambda s: 'purchase_rfm_' + s)\n\n        print(\"done with rfm outputing the file ongoing\")\n        path = os.path.join(ml_location,dag_run_id, \"rfm_purchase\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n\n    \n        ic(\"rfm purchase segement value counts\", ctm_class['Segment'].value_counts().compute())\n        print(\"done with rfm outputing the file done \")\n        return ctm_class.compute()\n    except Exception as e:\n        print(e)\n        \n        \n\n    \n\n\n\ndef transform(dataframe):\n    df = rfm_process_quantile_method_purchase(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }, {
              "id": "86784064-fbc4-140c-6c09-f005290d5e3e",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_feb.csv\",\n            \"m2\": \"purchase_m1_20230206203137.csv\",\n            \"m3\": \"purchase_m2_20230206203137.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n    \nclass UsageCategory(object):\n    def __init__(self, data):\n        self.data = data\n        self.current_iteration_month = None\n        # self.categorize()\n\n    def voice_band(self, x, m):\n        \n        if x == 0.0000:\n            y = 'a)Zero'\n        elif (x > 0) & (x <= 8):\n            y = 'b)<=8'\n        elif (x > 8) & (x <= 20):\n            y = 'c)8 - 20'\n        elif (x > 20) & (x <= 40):\n            y = 'd)20 - 40'\n        elif (x > 40) & (x <= 100):\n            y = 'e)40 - 100'\n        elif (x > 100) & (x <= 200):\n            y = 'f)100 - 200'\n        elif (x > 200) & (x <= 500):\n            y = 'g)200 - 500'\n        elif (x > 500) & (x <= 1000):\n            y = 'h)500 - 1000'\n        else:\n            y = 'i)1000+'\n        return m + \"_\" + y\n\n    def data_band(self, x, m):\n        \n        if x == 0.0000:\n            y = 'a)Zero'\n        elif (x > 0) & (x <= 50):\n            y = 'b)0-50 MB'\n        elif (x > 50) & (x <= 100):\n            y = 'c)50-100 MB'\n        elif (x > 100) & (x <= 250):\n            y = 'd)100-250 MB'\n        elif (x > 250) & (x <= 512):\n            y = 'e)250-512 MB'\n        elif (x > 512) & (x <= 1536):\n            y = 'f)512 MB-1.5 GB'\n        elif (x > 1536) & (x <= 3072):\n            y = 'g)1.5-3 GB'\n        else:\n            y = 'h)3 GB +'\n        return m + \"_\" + y\n\n    def categorize(self):\n        months = ['m1', 'm2', 'm3']\n        temp_df = None\n        for month in months:\n            self.current_iteration_month = month\n\n            needed_col = CUSTOMER_CATEG_NEEDED_COL\n            dataset = self.data.get(month)[needed_col]\n            dataset = dataset.fillna(0)\n            # 0 index for inbundle 1 index for outbundled  2 index for total\n            voice_col = CUSTOMER_VOICE_COL_NAME[0]\n            data_col = CUSTOMER_DATA_COL_NAME[0]\n            dataset[month + '_voice_band'] = dataset[voice_col].apply(self.voice_band, args=(month,))\n            dataset[month + '_data_band'] = dataset[data_col].apply(self.data_band, args=(month,))\n            dataset[month +'_'+ voice_col] = dataset[voice_col]\n            dataset[month +'_'+ data_col] = dataset[data_col]\n            dataset = dataset.drop(columns=[voice_col, data_col])\n            if temp_df is None:\n                temp_df = dataset\n            else:\n                temp_df = temp_df.merge(dataset, on='msisdn', how=\"left\")\n                temp_df[month + '_voice_band'] = temp_df[month + '_voice_band'].fillna(month + \"_\"'a)Zero')\n                temp_df[month + '_data_band'] = temp_df[month + '_data_band'].fillna(month + \"_\"'a)Zero')\n                dataset[month +'_'+ voice_col] = dataset[month +'_'+ voice_col].fillna(0)\n                dataset[month +'_'+ data_col] = dataset[month +'_'+ data_col].fillna(0)\n\n        return temp_df\n        \n        \n\n\ndef arpu_trend(data):\n    months = usage_no_months\n    temp_df = None\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    #for daily summarised\n\n    usage={}\n\n    for month in usage_no_months:\n        usage[month] = dd.read_csv(\n            os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n        \n    usage_3m = dd.concat(list(usage.values()))\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    temp_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n\n    print('len(temp_df) is',len(temp_df) )\n\n    #for daily summarised\n\n\n\n    for month in months:\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n        # total_voice_rev = CUSTOMER_VOICE_REVENUE[2]\n        needed_col = [MSISDN_COL_NAME, total_revenue]\n        df = data.get(month)[needed_col]\n        df['tot_rev'] = df[total_revenue]\n        df = df.drop(total_revenue, axis=1)\n        df = df.fillna(0)\n        #df = otliner_removal(df)\n\n        df['rev_segment'] = df.tot_rev.apply(put_revenue_segement)\n        df = df.rename(columns={\"rev_segment\": \"rev_segment_\" + month})\n        df['tot_rev' + month] = df['tot_rev']\n\n        df = df.drop('tot_rev', axis=1)\n        \n\n        if temp_df is None:\n            temp_df = df\n        else:\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n            temp_df[\"rev_segment_\" + month] = temp_df[\"rev_segment_\" + month].fillna(\"Zero\")\n            temp_df[\"tot_rev\" + month] = temp_df[\"tot_rev\" + month].fillna(0)\n\n    temp_df['tot_sum'] = temp_df['tot_revm1'] + temp_df['tot_revm2'] + temp_df['tot_revm3']\n    temp_df_non = temp_df\n    print(\"len of temp_df_non  after filtering is is  \", len(temp_df_non))\n    replace_map = get_banding_confitions().get(\"common\")\n\n    df1 = temp_df_non.replace(replace_map)\n    print(type(temp_df_non))\n\n    #to convert rev_segment_cols to int \n    rev_segment_cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    df1[rev_segment_cols] = df1[rev_segment_cols].astype('int32')\n    #to convert rev_segment_cols to int \n\n    cols = df1.columns[df1.columns.str.startswith('tot_revm')].tolist()\n\n    #to find trend using segment\n    #cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    #to find trend using segment\n\n    for col in cols:\n        df1[col] = df1[col].astype('int')\n        df1[col] = df1[col]\n\n    output = df1[cols].map_partitions(lambda part: part.apply(lambda x: pandas_wrapper(x), axis=1), meta=tuple)\n    output1 = output.to_frame(name='trend')\n    op = dd.concat([df1, output1], axis=1)\n\n    return op\n    \n    \n\n\ndef get_banding_confitions():\n        return {\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\n                               6: \"high_high\"}\n        }\n        \n        \n        \n\n\n\n\ndef put_revenue_segement(x):\n    y = 'Zero'\n    if x == 0.0000:\n        y = 'Zero'\n    elif (x > 0) & (x <= 16):\n        y = 'very_low'\n    elif (x > 16) & (x <= 36):\n        y = 'low'\n    elif (x > 36) & (x <= 61):\n        y = 'medium'\n    elif (x > 61) & (x <= 175):\n        y = 'high'\n    else:\n        y = 'high_high'\n    return y\n    \n    \ndef trends(numbers):\n    if len(numbers) != 3:\n        return \"Invalid input\"\n\n    if numbers[0] == numbers[1] == numbers[2]:\n        return \"flat\"\n    elif numbers[0] > numbers[1] > numbers[2]:\n        return \"Downtrend\"\n    elif numbers[0] < numbers[1] < numbers[2]:\n        return \"Uptrend\"\n    else:\n        return \"Zigzag\"\n        \n        \n\n\ndef pandas_wrapper(row):\n    return trends([row[2], row[1], row[0]])\n    \n    \n\ndef usage_process(dag_run_id):\n    try:\n\n\n\n        file_name_dict = get_file_names()\n        print(\"finding trend ongoing \")\n        data = {}\n        for month in usage_no_months:\n            data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\n                                      dtype= CUSTOMER_DTYPES)\n\n        # ic(\"the length of m1 in usage \", len(data['m1']))\n        # ic(\"the length of m2 in usage \", len(data['m2']))\n        # ic(\"the length of m3 in usage \", len(data['m3']))\n        # ic(\"the length of unique msisdn m3 in usage \", data['m1']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in usage \", data['m2'\n        #                                                     '']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in usage \", data['m3']['msisdn'].nunique().compute())\n\n        trend_df = arpu_trend(data)\n\n        # ic(\"the trend value counts is \", trend_df['trend'].value_counts().compute())\n\n        path = os.path.join(ml_location,dag_run_id, \"trend\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        #trend_df = trend_df.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n\n        trend_df.to_parquet(path)\n        uc = UsageCategory(data)\n        path = os.path.join(ml_location, dag_run_id, \"usage_bands\")\n        print(\"finding usage band ongoing \")\n        df = uc.categorize()\n\n        #to fill null values if there is in data and voice band \n        # df['m1_voice_band'] = df['m1_voice_band'].fill('m1_Zero')\n        # df['m1_data_band '] = df['m1_data_band'].fill('m1_Zero')\n        # df['m2_voice_band'] = df['m2_voice_band'].fill('m2_Zero')\n        # df['m2_data_band '] = df['m2_data_band'].fill('m2_Zero')\n        # df['m3_voice_band'] = df['m3_voice_band'].fill('m3_Zero')\n        # df['m3_data_band '] = df['m3_data_band'].fill('m3_Zero')\n\n        #to fill null values if there is in data and voice band \n\n\n\n\n        df.to_parquet(path)\n        print(\"finding usage band done \")\n        return df.compute()\n    except Exception as e:\n        print(e)\n        \n        \n\n\n\n\n\n\n\ndef transform(dataframe):\n    df = usage_process(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }, {
              "id": "b0e7a499-fd37-a90c-60e4-c7add77d7b70",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom fastapi import Depends, FastAPI, HTTPException\nimport pathlib\nimport dask.dataframe as dd\nimport datetime\nimport dask.array as da\nfrom pathlib import Path\nfrom icecream import ic\nimport  traceback\nfrom datetime import datetime\ngeneric_dict = {\"numerical_col\": \"Revenue\"}\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n\n\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_feb.csv\",\n            \"m2\": \"purchase_m1_20230206203137.csv\",\n            \"m3\": \"purchase_m2_20230206203137.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n\n\ndef join_rfm(x):\n    return str(int(x['R_Score'])) + str(int(x['F_Score'])) + str(int(x['M_Score']))\n    \n\n\ndef segmentaion_fun1(r, f, m):\n    segments = []\n    for r, f, m in zip(r, f, m):\n        value = int(f\"{r}{f}{m}\")\n\n        if value in [555, 554, 544, 545, 454, 455, 445]:\n            segments.append(\"Champions\")\n        elif value in [543, 444, 435, 355, 354, 345, 344, 335]:\n            segments.append(\"Loyal_Customers\")\n        elif value in [553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323]:\n            segments.append(\"Potential_Loyalist\")\n        elif value in [512, 511, 422, 421, 412, 411, 311]:\n            segments.append(\"Recent_Customers\")\n        elif value in [525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313]:\n            segments.append(\"Promising_Customers\")\n        elif value in [535, 534, 443, 434, 343, 334, 325, 324]:\n            segments.append(\"Customers_needing_Attention\")\n        elif value in [331, 321, 312, 221, 213]:\n            segments.append(\"About_to_Sleep\")\n        elif value in [255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124]:\n            segments.append(\"At_Risk\")\n        elif value in [155, 154, 144, 214, 215, 115, 114, 113]:\n            segments.append(\"Cant_Loose_them\")\n        elif value in [332, 322, 231, 241, 251, 233, 232, 223, 222, 132, 123, 122, 212, 211]:\n            segments.append(\"Hibernating\")\n        else:\n            segments.append(\"Lost\")\n\n    return segments\n    \n\n\n\ndef form_segements(ctm_class):\n    def process_duplicates(li):\n        for i in range(len(li) - 1):\n            print(li[i], i)\n            if i + 1 == len(li) - 1:\n                li[i] = ((li[i - 1] + li[i]) / 2)\n            elif li[i] == li[i + 1]:\n                li[i + 1] = ((li[i + 1] + li[i + 2]) / 2)\n        return li\n\n    # bins_recency = [-1,\n    #                 np.percentile(ctm_class[\"Recency\"], 20),\n    #                 np.percentile(ctm_class[\"Recency\"], 40),\n    #                 np.percentile(ctm_class[\"Recency\"], 60),\n    #                 np.percentile(ctm_class[\"Recency\"], 80),\n    #                 ctm_class[\"Recency\"].max().compute()]\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n    # bins_recency = [-1,17,35,53,71, ctm_class[\"Recency\"].max().compute()]\n\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Recency']=ctm_class1['Recency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Recency').agg({'msisdn':'count'}).reset_index()\n    mean_recency = ctm_class1['Recency'].max().compute()/5\n    print(\"mean_recency\", mean_recency)\n    print(\"mean_recency *2\",mean_recency * 2)\n    print(\"mean_recency *3\",mean_recency * 3)\n    print(\"mean_recency *4\",mean_recency * 4)\n    \n    \n    bins_recency = [-1,\n                    int(mean_recency),\n                    int(mean_recency * 2),\n                    int(mean_recency * 3),\n                    int(mean_recency * 4),\n                    ctm_class[\"Recency\"].max().compute()]\n    \n    print('bins_recency is', bins_recency)\n    \n\n    ctm_class['R_Score'] = ctm_class[\"Recency\"].map_partitions(pd.cut,\n                                                               bins=bins_recency,\n                                                               labels=[5, 4, 3, 2, 1]).astype(\"int\")\n\n    \n\n    # bins_frequency = [-1,\n    #                   np.percentile(ctm_class[\"Frequency\"], 20),\n    #                   np.percentile(ctm_class[\"Frequency\"], 40),\n    #                   np.percentile(ctm_class[\"Frequency\"], 60),\n    #                   np.percentile(ctm_class[\"Frequency\"], 80),\n    #                   ctm_class[\"Frequency\"].max().compute()]\n\n    # bins_frequency.sort()\n    # bins_frequency = process_duplicates(bins_frequency)\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Frequency']=ctm_class1['Frequency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Frequency').agg({'msisdn':'count'}).reset_index()\n    mean_frequency = ctm_class1['Frequency'].max().compute()/5\n    print(\"mean_frequency\", mean_frequency)\n    print(\"mean_frequency *2\",mean_frequency * 2)\n    print(\"mean_frequency *3\",mean_frequency * 3)\n    print(\"mean_frequency *4\",mean_frequency * 4)\n\n\n    bins_frequency = [-1,\n                    int(mean_frequency),\n                    int(mean_frequency * 2),\n                    int(mean_frequency * 3),\n                    int(mean_frequency * 4),\n                    ctm_class[\"Frequency\"].max().compute()]\n    \n    print('bins_frequency is', bins_frequency)\n    ctm_class['F_Score'] = ctm_class[\"Frequency\"].map_partitions(pd.cut,\n                                                                 bins=bins_frequency,\n                                                                 labels=[1, 2, 3, 4, 5]).astype(\"int\")\n\n    \n    \n    # bins_revenue = [-1,\n    #                 np.percentile(ctm_class[\"Revenue\"], 20),\n    #                 np.percentile(ctm_class[\"Revenue\"], 40),\n    #                 np.percentile(ctm_class[\"Revenue\"], 60),\n    #                 np.percentile(ctm_class[\"Revenue\"], 80),\n    #                 ctm_class[\"Revenue\"].max().compute()]\n    # bins_revenue.sort()\n    # bins_revenue = process_duplicates(bins_revenue)\n\n    #bins_revenue = [-1,189,380,571,762, ctm_class[\"Revenue\"].max().compute()]\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Revenue'] = ctm_class1['Revenue'].map_partitions(lambda series: series.apply(np.ceil))\n    ctm_class1['Revenue']=ctm_class1['Revenue'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Revenue').agg({'msisdn':'count'}).reset_index()\n    mean_revenue = ctm_class1['Revenue'].max().compute()/5\n    \n    print(\"mean_revenue\", mean_revenue)\n    print(\"mean_revenue *2\",mean_revenue * 2)\n    print(\"mean_revenue *3\",mean_revenue * 3)\n    print(\"mean_revenue *4\",mean_revenue * 4)\n    \n    bins_revenue = [-1,\n                    int(mean_revenue),\n                    int(mean_revenue * 2),\n                    int(mean_revenue * 3),\n                    int(mean_revenue * 4),\n                    ctm_class[\"Revenue\"].max().compute()]\n    \n    print('bins_revenue is', bins_revenue)\n\n    ctm_class['M_Score'] = ctm_class[\"Revenue\"].map_partitions(pd.cut,\n                                                               bins=bins_revenue,\n                                                               labels=[1, 2, 3, 4, 5]).astype(\"int\")\n    print(\"done with scoring\")\n    # Form RFM segment\n\n    ctm_class['RFM_Segment'] = ctm_class.apply(join_rfm, axis=1)\n    ctm_class['RFM_Segment'] = ctm_class['RFM_Segment'].astype(int)\n    print(\"formed rfm segement \")\n    ctm_class['R_Score'] = ctm_class['R_Score'].astype(int)\n    ctm_class['F_Score'] = ctm_class['F_Score'].astype(int)\n    ctm_class['M_Score'] = ctm_class['M_Score'].astype(int)\n    print(\"computing rfm\")\n    r = ctm_class['R_Score'].values.compute()\n    f = ctm_class['F_Score'].values.compute()\n    m = ctm_class['M_Score'].values.compute()\n    seg = segmentaion_fun1(r, f, m)\n    chunks = ctm_class.map_partitions(lambda x: len(x)).compute().to_numpy()\n    myarray = da.from_array(seg, chunks=tuple(chunks))\n    ctm_class['Segment'] = myarray\n    return ctm_class\n    \n    \n\ndef otliner_removal(df,col, per=0.97):\n    q = df[col].quantile(per)\n    print(f\"col is {col} and q is {q}\")\n    print(\"the length brfore is\", len(df))\n    outliers = df[df[col] > q]\n    df1 = df[~df['msisdn'].isin(outliers['msisdn'].compute().values)]\n    print(\"the length after is\", len(df1))\n    return df1\n    \n    \ndef get_banding_confitions():\n        return {\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\n                               6: \"high_high\"}\n        }\n        \n        \n\n\ndef arpu_trend(data):\n    months = usage_no_months\n    temp_df = None\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    #for daily summarised\n\n    usage={}\n\n    for month in usage_no_months:\n        usage[month] = dd.read_csv(\n            os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n        \n    usage_3m = dd.concat(list(usage.values()))\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    temp_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n\n    print('len(temp_df) is',len(temp_df) )\n\n    #for daily summarised\n\n\n\n    for month in months:\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n        # total_voice_rev = f.Features.CUSTOMER_VOICE_REVENUE[2]\n        needed_col = [MSISDN_COL_NAME, total_revenue]\n        df = data.get(month)[needed_col]\n        df['tot_rev'] = df[total_revenue]\n        df = df.drop(total_revenue, axis=1)\n        df = df.fillna(0)\n        #df = otliner_removal(df)\n\n        df['rev_segment'] = df.tot_rev.apply(put_revenue_segement)\n        df = df.rename(columns={\"rev_segment\": \"rev_segment_\" + month})\n        df['tot_rev' + month] = df['tot_rev']\n\n        df = df.drop('tot_rev', axis=1)\n        \n\n        if temp_df is None:\n            temp_df = df\n        else:\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n            temp_df[\"rev_segment_\" + month] = temp_df[\"rev_segment_\" + month].fillna(\"Zero\")\n            temp_df[\"tot_rev\" + month] = temp_df[\"tot_rev\" + month].fillna(0)\n\n    temp_df['tot_sum'] = temp_df['tot_revm1'] + temp_df['tot_revm2'] + temp_df['tot_revm3']\n    temp_df_non = temp_df\n    print(\"len of temp_df_non  after filtering is is  \", len(temp_df_non))\n    replace_map = get_banding_confitions().get(\"common\")\n\n    df1 = temp_df_non.replace(replace_map)\n    print(type(temp_df_non))\n\n    #to convert rev_segment_cols to int \n    rev_segment_cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    df1[rev_segment_cols] = df1[rev_segment_cols].astype('int32')\n    #to convert rev_segment_cols to int \n\n    cols = df1.columns[df1.columns.str.startswith('tot_revm')].tolist()\n\n    #to find trend using segment\n    #cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    #to find trend using segment\n\n    for col in cols:\n        df1[col] = df1[col].astype('int')\n        df1[col] = df1[col]\n\n    output = df1[cols].map_partitions(lambda part: part.apply(lambda x: pandas_wrapper(x), axis=1), meta=tuple)\n    output1 = output.to_frame(name='trend')\n    op = dd.concat([df1, output1], axis=1)\n\n    return op\n    \n    \ndef perform_rfm_on_daily_summ(df_data_rfm, period=95):\n\n    ic(\"inside perform_rfm in daily summarised\")\n\n    df_data_rfm.purchase_date = dd.to_datetime(df_data_rfm.purchase_date)\n    df_data_rfm = df_data_rfm.fillna(0)\n    msisdn_name = MSISDN_COL_NAME\n    # current behaviour data\n    min_date = df_data_rfm['purchase_date'].min().compute()\n    cur_beh_data_rfm = df_data_rfm[(df_data_rfm.purchase_date <= pd.Timestamp(min_date) + pd.Timedelta(days=period))\n                                   & (df_data_rfm.purchase_date >= pd.Timestamp(min_date))].reset_index(drop=True)\n    # Get the maximum purchase date of each customer and create a dataframe with it together with the customer's id.\n    ctm_max_purchase = cur_beh_data_rfm.groupby(msisdn_name).purchase_date.max().reset_index()\n    ctm_max_purchase.columns = [msisdn_name, 'MaxPurchaseDate']\n\n    # Find the recency of each customer in days\n    ctm_max_purchase['Recency'] = (\n            ctm_max_purchase['MaxPurchaseDate'].max() - ctm_max_purchase['MaxPurchaseDate']).dt.days\n    ctm_max_purchase = ctm_max_purchase.drop(columns=['MaxPurchaseDate'])\n    print(\"done with recency \")\n    # frequency\n    ctm_frequency = cur_beh_data_rfm.groupby(msisdn_name).size().reset_index()\n    ctm_frequency.columns = [msisdn_name, 'Frequency']\n    print(\"done with frequency \")\n    cur_beh_data_rfm['Revenue'] = cur_beh_data_rfm[DAILY_TRANSACTION_PRICE_COL_NAME]\n    ctm_revenue = cur_beh_data_rfm.groupby('msisdn').Revenue.sum().reset_index()\n    print(\"done with monitory \")\n    rfm_data_base = dd.concat([ctm_max_purchase, ctm_revenue, ctm_frequency], axis=1)\n    rfm_data_base = rfm_data_base.loc[:, ~rfm_data_base.columns.duplicated()]\n\n    #for btc purpose start\n    #rfm_data_base=otliner_removal(rfm_data_base,'Frequency')\n    rfm_data_base=otliner_removal(rfm_data_base,'Revenue')\n    \n    #for btc purpose end \n\n\n    return rfm_data_base\n    \n\n\ndef put_revenue_segement(x):\n    y = 'Zero'\n    if x == 0.0000:\n        y = 'Zero'\n    elif (x > 0) & (x <= 16):\n        y = 'very_low'\n    elif (x > 16) & (x <= 36):\n        y = 'low'\n    elif (x > 36) & (x <= 61):\n        y = 'medium'\n    elif (x > 61) & (x <= 175):\n        y = 'high'\n    else:\n        y = 'high_high'\n    return y\n    \n\n\ndef trends(numbers):\n    if len(numbers) != 3:\n        return \"Invalid input\"\n\n    if numbers[0] == numbers[1] == numbers[2]:\n        return \"flat\"\n    elif numbers[0] > numbers[1] > numbers[2]:\n        return \"Downtrend\"\n    elif numbers[0] < numbers[1] < numbers[2]:\n        return \"Uptrend\"\n    else:\n        return \"Zigzag\"\n        \n\n\ndef pandas_wrapper(row):\n    return trends([row[2], row[1], row[0]])\n    \n\n\ndef rfm_process_quantile_method_daily_summerized(dag_run_id):\n    try:\n        msisdn_name = MSISDN_COL_NAME\n        \n        \n        file_name_dict = get_file_names()\n        dtype_purchase = DAILY_TRANSACTION_DTYPES\n        data = {}\n        print('purchase is going to  read')\n        for month in purchase_no_months:\n            data[month] = dd.read_csv(\n                os.path.join(purchase_location, file_name_dict.get(\"daily_summerized\").get(month)),\n                dtype=dtype_purchase)\n        print('purchase readed')\n        df_data = dd.concat(list(data.values()))\n        \n        #for daily summarised\n\n        usage={}\n\n        for month in usage_no_months:\n            usage[month] = dd.read_csv(\n                os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n            \n        #usage_3m = dd.concat(list(usage.values()))\n        trend_df = arpu_trend(usage)\n        trend_df = trend_df.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n        df_data = df_data[df_data[msisdn_name].isin(trend_df[msisdn_name].compute().unique())]\n        print(\"usage filtered---\")\n        \n        # profile\n        dtype = {'account_type': 'float64',\n                 'current_state': 'float64',\n                 'no_of_balances': 'float64',\n                 'primary_offer_id': 'float64',\n                 'subscr_no': 'float64',\n                 'payment_type':'object'}\n        profile = dd.read_csv(os.path.join(purchase_location, file_name_dict.get(\"profile\")), dtype=dtype)\n        profile[PROFILE_DATE_ENTER_ACTIVE] = dd.to_datetime(profile[PROFILE_DATE_ENTER_ACTIVE],\n                                                                       errors='coerce')\n\n        date_threshold = datetime.strptime('2023-02-12', '%Y-%m-%d')\n\n        profile['aon'] = (date_threshold - profile[PROFILE_DATE_ENTER_ACTIVE]).dt.days\n\n        \n        profile = profile[profile['aon'] >=90]\n        profile = profile[profile['current_state'] ==2]\n        profile = profile[profile['payment_type'] =='Prepaid']\n        print('df.columns',df_data.columns)\n        # df_data = df_data.merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n        #                                                               how='inner')\n        df_data=df_data[df_data['msisdn'].isin(profile['msisdn'].compute().unique())]\n        df_data=df_data[['msisdn', 'fct_dt', 'total_revenue', 'avg_total_revenue']]\n\n\n        #for daily summarised\n\n        df_data = df_data.fillna(0 )\n        df_data = df_data.rename(columns={DAILY_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        ctm_class = perform_rfm_on_daily_summ(df_data, period=95)#rfm using daily summarised\n        path = os.path.join(ml_location,dag_run_id, \"rfm_before_segementation\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n        ctm_class = form_segements(ctm_class)\n\n        print(\"done with rfm outputing the file ongoing\")\n        path = os.path.join(ml_location,dag_run_id, \"rfm\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n\n        ic(\"rfm segement value counts\", ctm_class['Segment'].value_counts().compute())\n        print(\"done with rfm outputing the file done \")\n\n    except Exception as e:\n        print(e)\n        \n\ndef transform(dataframe):\n    rfm_process_quantile_method_daily_summerized(\"manual__2023-07-10T11:06:51\")\n    return dataframe"
              }
            }, {
              "id": "e3299f76-e8e7-0f6d-e7f4-e6fbb25d5909",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\nimport vaex\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n\n\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n\n\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_march_20230809132937.csv\",\n            \"m2\": \"recharge_feb_20230809132937.csv\",\n            \"m3\": \"recharge_jan_20230809132937.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_march_20230517180929.csv\",\n            \"m2\": \"weekwise_feb_20230517180929.csv\",\n            \"m3\": \"weekwise_jan_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_march_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_jan_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_nov_full_df.csv\",\n            \"p2\": \"purchase_dec_full_df.csv\",\n            \"p3\": \"purchase_jan_full_df.csv\",\n            \"p4\": \"purchase_feb_full_df.csv\",\n            \"p5\": \"purchase_march_full_df.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n\n\n\ndef service_band(dataset,name):   \n    \n\n    dataset =  dataset[['msisdn','total_voice_usage_total','total_data_usage_total','total_sms_usage_total']]  \n    \n    \n    dataset['total_voice_usage_total'].fillna(0,inplace=True)\n    dataset['total_data_usage_total'].fillna(0,inplace=True)\n    dataset['total_sms_usage_total'].fillna(0,inplace=True)\n    \n    \n    \n    dataset[name]= np.where((dataset['total_voice_usage_total']>0)&\n                              (dataset['total_data_usage_total']>0)&\n                              (dataset['total_sms_usage_total']>0),'VDS','nill')\n    \n    dataset[name]= np.where((dataset['total_voice_usage_total']>0)&\n                              (dataset['total_data_usage_total']>0)&\n                              (dataset['total_sms_usage_total']<=0),'VD',dataset[name])\n    \n    dataset[name]= np.where((dataset['total_voice_usage_total']>0)&\n                              (dataset['total_sms_usage_total']>0)&\n                              (dataset['total_data_usage_total']<=0),'VS',dataset[name])\n    \n    dataset[name]= np.where((dataset['total_data_usage_total']>0)&\n                              (dataset['total_sms_usage_total']>0)&\n                             (dataset['total_voice_usage_total']<=0),'DS',dataset[name])\n    \n    dataset[name]=np.where((dataset['total_voice_usage_total']>0)&\n                             (dataset['total_sms_usage_total']<=0)&\n                             (dataset['total_data_usage_total']<=0),'V',dataset[name])\n    \n    dataset[name]=np.where((dataset['total_data_usage_total']>0)&\n                             (dataset['total_sms_usage_total']<=0)&\n                             (dataset['total_voice_usage_total']<=0),'D',dataset[name])\n    \n    dataset[name]=np.where((dataset['total_sms_usage_total']>0)&\n                             (dataset['total_data_usage_total']<=0)&\n                             (dataset['total_voice_usage_total']<=0),'S',dataset[name])\n    \n    dataset[name]=np.where((dataset['total_sms_usage_total']==0)&\n                             (dataset['total_data_usage_total']==0)&\n                             (dataset['total_voice_usage_total']==0),'no_usage',dataset[name])\n    \n    \n    return dataset\n    \n    \n\n\n\ndef user_service(dag_run_id):\n\n    try:\n\n        file_name_dict = get_file_names()\n        data={}\n        li=[]\n        final= pd.DataFrame({})\n\n        columns = ['msisdn','total_voice_usage' ,'total_data_usage' ,'total_sms_usage']\n        for month in usage_no_months:      \n\n                data = pd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),usecols= columns)\n\n                for i in columns:\n                    if i!='msisdn':\n                        data = data.rename(columns= {i:month+'_'+i})\n\n\n                if final.empty:\n                    final =data\n                else:\n                    final = pd.merge(final,data,on='msisdn',how='left')\n    \n\n        final = final.fillna(0)\n        columns.remove('msisdn')\n        li =[]\n        \n        for i in columns:\n            for j in usage_no_months:\n                li.append(j + '_' + i)\n\n            name = i+'_total'    \n            final[name] = (final[li[0]]+final[li[1]]+final[li[2]])/3\n            li=[]\n\n        r = service_band(final,'service_band') \n\n        path = os.path.join(ml_location, dag_run_id,\"favorite_Service.csv\")\n        r.to_csv(path,index=False,header=True)    \n        return r\n    except Exception as e:\n        print(e)\n        \n\n\n\n\n\n\ndef transform(dataframe):\n    df = user_service(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }, {
              "id": "c282b1c3-2e9b-70f6-5b65-25abe55855cb",
              "operation": {
                "id": "e652238f-7415-4da6-95c6-ee33808561b2",
                "name": "Sink"
              },
              "parameters": {

              }
            }, {
              "id": "c190bad0-86b9-a3e1-911e-7cc9322deafd",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_march_20230809132937.csv\",\n            \"m2\": \"recharge_feb_20230809132937.csv\",\n            \"m3\": \"recharge_jan_20230809132937.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n\n\nclass purchaseBanding(object):\n    def __init__(self, data):\n        self.data = data\n        # self.categorize()\n\n    def count_category(self, x, m):\n\n        if x == 1:\n            resp = 'b)1'\n        elif 1 < x <= 4:\n            resp = 'c)1-4'\n        elif 4 < x <= 10:\n            resp = 'd)4-10'\n        elif 10 < x <= 30:\n            resp = 'e)10-30'\n        elif x > 30:\n            resp = 'f)30 above'\n        else:\n            resp = 'a)zero'\n        return m + \"_\" + resp\n\n    def categorize(self):\n\n        months = purchase_no_months_seg\n        purchase_count_col_name = TRANSACTION_COUNT_COL_NAME\n        msisdn_name = MSISDN_COL_NAME\n        temp_df = None\n        for month in months:\n            needed_col = TRANSACTION_NEEDED_COL\n            dataset = self.data.get(month)[needed_col]\n            dataset = dataset.groupby([msisdn_name]).agg({purchase_count_col_name: 'sum'})\n            dataset['count_pattern'] = dataset[purchase_count_col_name].apply(self.count_category, args=(month,))\n            dataset['purchase_count_pattern_' + month] = dataset['count_pattern']\n            dataset['purchase_'+purchase_count_col_name + month] = dataset[purchase_count_col_name]\n            dataset = dataset.drop(purchase_count_col_name, axis=1)\n            dataset = dataset.drop('count_pattern', axis=1)\n            if temp_df is None:\n                temp_df = dataset\n            else:\n                temp_df = temp_df.merge(dataset, on=msisdn_name, how=\"left\")\n                temp_df[\"purchase_count_pattern_\" + month] = temp_df[\"purchase_count_pattern_\" + month].fillna(month+\"_a)zero\")\n                temp_df['purchase_'+purchase_count_col_name + month]=temp_df['purchase_'+purchase_count_col_name + month].fillna(0)\n        return temp_df.reset_index()\n        \n\n\n\ndef purchase_process(dag_run_id):\n    try:\n        file_name_dict = get_file_names()\n        print(\"purchase preprocess  ongoing \")\n        data = {}\n        dtype_purchase = RECHARGE_TRANSACTION_DTYPES\n        for month in purchase_no_months_seg:\n            data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"purchase\").get(month)),\n                                      dtype= RECHARGE_TRANSACTION_DTYPES)\n\n        df_data = dd.concat(list(data.values()))\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        path_pur = os.path.join(ml_location, dag_run_id,\"purchase_all_months\")\n        Path(path_pur).mkdir(parents=True, exist_ok=True)\n        df_data.to_parquet(path_pur)\n        # ic(\"the length of m1 in purchase \", len(data['m1']))\n        # ic(\"the length of m2 in purchase \", len(data['m2']))\n        # ic(\"the length of m3 in purchase \", len(data['m3']))\n        #\n        # ic(\"the length of unique msisdn m3 in purchase \", data['m1']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in purchase \", data['m2']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in purchase \", data['m3']['msisdn'].nunique().compute())\n        rb = purchaseBanding(data)\n        print(\"purchase categorizing ongoing \")\n        df = rb.categorize()\n        print(\"purchase categorizing done \")\n        path = os.path.join(ml_location, dag_run_id,\"purchase_band\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(\"purchase categorizing  to file op ongoing \")\n        df.to_parquet(path)\n        print(\"purchase categorizing  to file op done  \")\n        return df.compute()\n    except Exception as e:\n        print(e)\n        \n\n\n\n\n\ndef transform(dataframe):\n    df  = purchase_process(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }, {
              "id": "fb3137bd-e8bc-3c4c-4afe-93def0579cf8",
              "operation": {
                "id": "f94b04d7-ec34-42f7-8100-93fe235c89f8",
                "name": "Source"
              },
              "parameters": {

              }
            }, {
              "id": "75230ec4-261d-3453-abfd-c3446184262c",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n    \n    \ndef status_process(dag_run_id):\n    try:\n        print(\" inside status_process\")\n        file_name_dict = get_file_names()\n        trend_path = os.path.join(ml_location,dag_run_id,  \"trend\")\n        trend_filtered_path = os.path.join(ml_location, dag_run_id,  \"trend_filtered\")\n        trend_profie_active_inactive_trend_eda_path= os.path.join(ml_location,dag_run_id,   \"trend_profie_active_inactive_trend_eda\")\n        rfm_path = os.path.join(ml_location, dag_run_id, \"rfm\")\n        print(\"reading perquet\")\n        trend = dd.read_parquet(trend_path)\n        print(\"reading perquet cmpt\")\n\n        print('len of trend is ',len(trend))\n        trend_filter = trend.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n        print('len of trend_filter after filter either 3 month total rev grater that 0  is ',len(trend_filter))\n\n\n        # profile\n        dtype = {'account_type': 'float64',\n                 'current_state': 'float64',\n                 'no_of_balances': 'float64',\n                 'primary_offer_id': 'float64',\n                 'subscr_no': 'float64',\n                 'payment_type':'object'}\n        profile = dd.read_csv(os.path.join(purchase_location, file_name_dict.get(\"profile\")), dtype=dtype)\n        profile[PROFILE_DATE_ENTER_ACTIVE] = dd.to_datetime(profile[PROFILE_DATE_ENTER_ACTIVE],\n                                                                       errors='coerce')\n\n        date_threshold = datetime.strptime('2023-07-30', '%Y-%m-%d')\n\n        profile['aon'] = (date_threshold - profile[PROFILE_DATE_ENTER_ACTIVE]).dt.days\n        # trend = trend.merge(trend_filter, on='msisdn', how='left', indicator=True)\n        # trend['active_inactive'] = trend['_merge'].apply(lambda x: 'active' if x == 'both' else 'inactive',\n        #                                                  meta=('str'))\n        # trend = trend.drop(columns=['_merge'])\n\n        # trend_active_aon = trend[['msisdn', 'active_inactive']].merge(profile[['msisdn', 'aon']], on='msisdn',\n        #                                                               how='left')\n        # trend_active_aon = trend_active_aon.fillna(0)\n\n        # print(' head is going to add')\n\n        # trend_active_aon1 = trend_active_aon.head(0)\n        # print('trend_active_aon1 is' ,trend_active_aon1 )\n        # print('type of trend_active_aon1 is' ,type(trend_active_aon1 ))\n        # #trend_active_aon1=trend_active_aon1.compute()\n        # print('trend_active_aon1 is' ,trend_active_aon1 )\n        # trend_active_aon['dag_run_id']=str(dag_run_id)\n\n        # trend_active_aon.head(0).to_sql(\"trend_active_aon\", engine, if_exists=\"append\", index=False)\n        # print(' head added')\n        # @delayed\n        # def insert_partition(partition):\n        #     partition.to_sql(\"trend_active_aon\", engine, if_exists=\"append\", index=False)\n\n        # insertions = [insert_partition(partition) for partition in trend_active_aon.to_delayed()]\n        # dask.compute(*insertions)\n        print(' testing 1  ')\n\n\n        trend_active_aon_f = trend_filter.merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n                                                                      how='inner')\n\n        trend_active_aon_f = trend_active_aon_f.fillna(0)\n        trend_active_aon_f = trend_active_aon_f[trend_active_aon_f['aon'] >=90]\n        trend_active_aon_f = trend_active_aon_f[trend_active_aon_f['current_state'] ==2]\n        trend_active_aon_f = trend_active_aon_f[trend_active_aon_f['payment_type'] =='Prepaid']\n\n        print('len of trend_active_aon_f after aon,current_state and payment_type filter is ',len(trend_active_aon_f))\n        \n\n        #trend_active_aon_f_pd = trend_active_aon_f.to_pandas_df()\n\n        # create a dask dataframe from the pandas dataframe\n        #trend_active_aon_f_dd = trend_active_aon_f.compute()\n\n        profile = profile.compute()\n        trend=trend.compute()\n\n        print(' testing 2 ')\n\n        trend_profie = trend[['msisdn','trend']].merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n                                                                      how='left')\n        \n        trend_profie[[ 'aon','current_state']]=trend_profie[[ 'aon','current_state']].fillna(0)\n        trend_profie['payment_type']=trend_profie['payment_type'].fillna('nil')\n\n\n\n       \n\n        #rend_active_aon_f_msisdn=trend_active_aon_f['msisdn'].tolist()\n\n        trend_profie_active = trend_profie[(trend_profie['payment_type'] == 'Prepaid') & (trend_profie['current_state'] == 2)]\n        trend_profie_active= trend_profie_active[trend_profie_active['msisdn'].isin(trend_filter['msisdn'].compute())]\n        trend_profie_inactive = trend_profie[~trend_profie['msisdn'].isin(trend_profie_active['msisdn'])]\n\n        trend_profie_active['active_inactive'] = 'active'\n        \n        trend_profie_inactive['active_inactive'] = 'inactive'\n   \n        trend_profie_active = dd.from_pandas(trend_profie_active, npartitions=2)\n        trend_profie_inactive = dd.from_pandas(trend_profie_inactive, npartitions=2)\n\n        trend_profie_active_inactive = dd.concat([trend_profie_active,trend_profie_inactive])\n        \n       \n        trend_profie_active_inactive['dag_run_id']=str(dag_run_id)\n\n\n       \n        \n\n        print('type(trend_profie_active_inactive',type(trend_profie_active_inactive))\n  \n#         print(' head added')\n#         @delayed\n#         def insert_partition(partition):\n#             partition.to_sql(\"APA_trend_profie_active_inactive\", engine, if_exists=\"append\", index=False)\n\n#         insertions = [insert_partition(partition) for partition in trend_profie_active_inactive.to_delayed()]\n#         dask.compute(*insertions)\n#         print(' trend_profie_active_inactive added to sql table ')\n\n        rfm = dd.read_parquet(rfm_path)\n\n        trend_profie_active_inactive_trend_rfm = trend_profie_active_inactive.merge(rfm[['msisdn', 'Segment']], on='msisdn',\n                                                                      how='left')\n        trend_profie_active_inactive_trend_rfm['Segment']=trend_profie_active_inactive_trend_rfm['Segment'].fillna('nil')\n\n        trend_profie_active_inactive_trend_rfm.to_parquet(trend_profie_active_inactive_trend_eda_path)\n        \n        trend_active_aon_f.to_parquet(trend_filtered_path)\n\n        # ic(\"the length of trend df \", len(trend))\n        # ic(\"the unique msisdn in trend df \", trend['msisdn'].nunique().compute())\n\n        # ic(\"the length of trend df  after all 3 months active \", len(trend_filter))\n        # ic(\"the unique msisdn in trend df  all 3 months active \", trend_filter['msisdn'].nunique().compute())\n        return trend_active_aon_f.compute()\n        pass\n    except Exception as e:\n        print(e)\n        \n\ndef transform(dataframe):\n    df = status_process(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }],
            "connections": [{
              "from": {
                "nodeId": "fb3137bd-e8bc-3c4c-4afe-93def0579cf8",
                "portIndex": 0
              },
              "to": {
                "nodeId": "b0e7a499-fd37-a90c-60e4-c7add77d7b70",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "86784064-fbc4-140c-6c09-f005290d5e3e",
                "portIndex": 0
              },
              "to": {
                "nodeId": "75230ec4-261d-3453-abfd-c3446184262c",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "e8039b3f-9581-ef73-d293-03c56c620d1a",
                "portIndex": 0
              },
              "to": {
                "nodeId": "c282b1c3-2e9b-70f6-5b65-25abe55855cb",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "e3299f76-e8e7-0f6d-e7f4-e6fbb25d5909",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e8039b3f-9581-ef73-d293-03c56c620d1a",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "e4921898-f8ed-fb4f-c568-fa70ed6bb972",
                "portIndex": 0
              },
              "to": {
                "nodeId": "c190bad0-86b9-a3e1-911e-7cc9322deafd",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "d6ce0a6e-df3f-4da8-07a7-bed116711940",
                "portIndex": 0
              },
              "to": {
                "nodeId": "86784064-fbc4-140c-6c09-f005290d5e3e",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "b0e7a499-fd37-a90c-60e4-c7add77d7b70",
                "portIndex": 0
              },
              "to": {
                "nodeId": "d6ce0a6e-df3f-4da8-07a7-bed116711940",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "75230ec4-261d-3453-abfd-c3446184262c",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e4921898-f8ed-fb4f-c568-fa70ed6bb972",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "c190bad0-86b9-a3e1-911e-7cc9322deafd",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e3299f76-e8e7-0f6d-e7f4-e6fbb25d5909",
                "portIndex": 0
              }
            }]
          },
          "thirdPartyData": {
            "gui": {
              "name": "Inner workflow of custom transformer",
              "nodes": {
                "e8039b3f-9581-ef73-d293-03c56c620d1a": {
                  "uiName": "segmentation",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5425,
                    "y": 5657
                  }
                },
                "e3299f76-e8e7-0f6d-e7f4-e6fbb25d5909": {
                  "uiName": "user_service",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5317,
                    "y": 5561
                  }
                },
                "c282b1c3-2e9b-70f6-5b65-25abe55855cb": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5567,
                    "y": 5751
                  }
                },
                "75230ec4-261d-3453-abfd-c3446184262c": {
                  "uiName": "active_inactive",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4978,
                    "y": 5265
                  }
                },
                "c190bad0-86b9-a3e1-911e-7cc9322deafd": {
                  "uiName": "purchase_process",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5209,
                    "y": 5462
                  }
                },
                "fb3137bd-e8bc-3c4c-4afe-93def0579cf8": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 4636,
                    "y": 4874
                  }
                },
                "86784064-fbc4-140c-6c09-f005290d5e3e": {
                  "uiName": "usage_process",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4869,
                    "y": 5171
                  }
                },
                "e4921898-f8ed-fb4f-c568-fa70ed6bb972": {
                  "uiName": "recharge_process",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5090,
                    "y": 5364
                  }
                },
                "d6ce0a6e-df3f-4da8-07a7-bed116711940": {
                  "uiName": "rfm_purchase",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4771,
                    "y": 5080
                  }
                },
                "b0e7a499-fd37-a90c-60e4-c7add77d7b70": {
                  "uiName": "rfm_daily_summerised",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4669,
                    "y": 4984
                  }
                }
              }
            }
          },
          "publicParams": []
        }
      }
    }, {
      "id": "dfb08402-f48a-6a2e-7954-a923c156d847",
      "operation": {
        "id": "643d8706-24db-4674-b5b4-10b5129251fc",
        "name": "Transform"
      },
      "parameters": {
        "Parameters of input Transformer": {

        }
      }
    }, {
      "id": "165d23d1-e101-ea4a-b0fe-bb99fc03adcc",
      "operation": {
        "id": "65240399-2987-41bd-ba7e-2944d60a3404",
        "name": "Create Custom Transformer"
      },
      "parameters": {
        "inner workflow": {
          "workflow": {
            "nodes": [{
              "id": "91ffda5a-084a-685b-88f4-f9241d240e72",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nimport pickle\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\n\r\n\r\nfrom sqlalchemy import create_engine\r\nfrom sqlalchemy.ext.declarative import declarative_base\r\nfrom sqlalchemy.orm import sessionmaker\r\nfrom urllib.parse import quote  \r\n\r\n\r\n\r\nfrom sqlalchemy.orm import Session\r\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\r\nfrom sqlalchemy.orm import relationship\r\nimport datetime\r\nfrom sqlalchemy.dialects.mysql import LONGTEXT\r\n\r\n\r\nfrom typing import List, Optional\r\n\r\nfrom pydantic import BaseModel\r\n\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nfrom sklearn.feature_selection import SelectFromModel\r\n\r\n\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.metrics import classification_report\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.feature_selection import SelectFromModel\r\nimport sklearn\r\nimport json\r\nimport dask.dataframe as dd\r\nimport os\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\nfrom icecream import ic\r\nimport dask.dataframe as dd\r\nimport os\r\n# import configuration.config as cfg\r\n# import configuration.features as f\r\nimport traceback\r\nimport numpy as np\r\nfrom fastapi import Depends, FastAPI, HTTPException\r\nfrom pathlib import Path\r\nimport pickle\r\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\r\nfrom pathlib import Path\r\nimport requests\r\n# from sql_app.repositories import AssociationRepo\r\n\r\n# config = cfg.Config().to_json()\r\n# features = f.Features().to_json()\r\n\r\n\r\n\r\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\r\n'recharge_total_cntm2',\r\n'recharge_total_cntm3',\r\n'm1_total_voice_usage',\r\n'm1_total_data_usage',\r\n'm2_total_voice_usage',\r\n'm2_total_data_usage',\r\n'm3_total_voice_usage',\r\n'm3_total_data_usage',\r\n'purchase_total_cntm1',\r\n'purchase_total_cntm2',\r\n'purchase_total_cntm3', \r\n'm3_total_revenue_m2_total_revenue_pct_drop',\r\n'm3_data_revenue_m2_data_revenue_pct_drop',\r\n'm2_voice_rev_m1_voice_rev_pct_drop',\r\n'm2_total_revenue_m1_total_revenue_pct_drop',\r\n'm2_data_revenue_m1_data_revenue_pct_drop',\r\n'm3_voice_rev_m2_voice_rev_pct_drop',\r\n'm1_m2_m3_average_voice',\r\n'm1_m2_m3_average_data', \r\n'm1_no_of_days',\r\n'm2_no_of_days',\r\n'm3_no_of_days',\r\n'eng_index',\r\n'consecutive_inactive_days']\r\n\r\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\r\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\r\nMSISDN_COL_NAME = 'msisdn'\r\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\r\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\r\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\r\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \r\nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\r\n\r\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\r\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\r\nTRANSACTION_PRODUCT_NAME = 'product_id'\r\n\r\nPACK_CONTI_FEATURES = []\r\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\r\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\r\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\r\nPACK_INFO_CATEGORY = 'product_type'\r\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\r\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\r\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\r\nCUSTOMER_CATEG_FEATURES = []\r\nCUSTOMER_CONTI_FEATURES = []\r\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\r\n# 0 index for inbundle 1 index for outbundled  2 index for total\r\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\r\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\r\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\r\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\r\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\r\nCUSTOMER_NEEDED_COLUMN = [\r\n'onnet_revenue',  'onnet_usage', 'onnet_voice_count', \r\n'offnet_revenue', 'offnet_usage', 'offnet_voice_count',  \r\n'idd_revenue', 'idd_usage', 'idd_voice_count',\r\n'voice_rmg_revenue', 'voice_rmg_usage', 'voice_rmg_count', \r\n'data_rmg_revenue',  'data_rmg_usage',  \r\n'data_revenue', 'data_usage', 'sms_revenue', 'sms_usage',  'sms_idd_revenue', \r\n'sms_idd_usage', 'magik_voice_amount', 'rbt_subscription_rev', 'emergency_credit_rev', \r\n'package_revenue', 'voice_rev', 'sms_rev', 'onn_rev', 'off_rev', 'total_data_rev', \r\n'vas_rev', 'vas_rev_others', 'total_revenue', 'total_voice_count', 'total_voice_duration', \r\n'total_mainaccount_data_usage', 'total_sms_count', 'total_package_count', 'total_other_vas_count', \r\n'total_voice_usage', 'total_data_usage', 'total_sms_usage'\r\n]\r\n\r\n\r\n\r\n\r\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\r\n\r\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\r\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\r\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\r\n\r\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\r\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\r\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\r\n\r\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\r\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\r\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\r\n\r\nLABEL1 = 'downtrend'\r\nLABEL2 = 'uptrend'\r\nLABEL3 = 'zigzag'\r\nLABEL4 = 'flat'\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_features': self.TRANSACTION_FEATURES,\r\n        'usage_features': self.CUSTOMER_FEATURES,\r\n        'pack_info_features': self.ALL_PACK_FEATURES,\r\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\r\n        'usage_dtypes': self.CUSTOMER_DTYPES\r\n\r\n    }\r\n    \r\n    \r\n\r\n\r\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\r\nsegement_names = ['trend', 'rfm']\r\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\r\npurchase_location = '/home/tnmops/seahorse3_bkp/'\r\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\r\netl_location = '/home/tnmops/seahorse3_bkp/'\r\npack_info_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location = '/home/tnmops/seahorse3_bkp/'\r\nlog_file = '/home/tnmops/seahorse3_bkp/'\r\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\r\n\r\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\r\n\r\nusage_no_months = ['m1', 'm2', 'm3']\r\nrecharge_no_months = ['m1', 'm2', 'm3']\r\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\r\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\nthreshold = 0.50\r\nnot_needed_rfm_segment = ['Lost']\r\n\r\npack_cols = ['product_id','product_type']\r\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\r\n\r\ncampaign_df= ['c4','c3']\r\ncampaign_usage= ['m4','m3']\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_location': self.purchase_location,\r\n        'usage_location': self.etl_location,\r\n        'pack_info_location': self.pack_info_location,\r\n        'ml_location': self.ml_location,\r\n        'recharge_location':self.recharge_location\r\n    }\r\n\r\nmsisdn_name = MSISDN_COL_NAME\r\n\r\n\r\n\r\ndef matrix_filter(dag_run_id):\r\n    try:\r\n\r\n        # read purchase information\r\n        path = os.path.join(ml_location, dag_run_id,\"purchase_filtered\")\r\n        matrix_path = os.path.join(ml_location,dag_run_id, \"matrix.csv\")\r\n        matrix_basic_path = os.path.join(ml_location,dag_run_id, \"propensity_matrix_basic.csv\")\r\n        matrix_addon_path = os.path.join(ml_location,dag_run_id, \"propensity_matrix_addon.csv\")\r\n        purchase_filter_path = os.path.join(ml_location, dag_run_id,\"purchased_for_association\")\r\n\r\n        Path(purchase_filter_path).mkdir(parents=True, exist_ok=True)\r\n\r\n        purchase = dd.read_parquet(path)\r\n        path_d = os.path.join(ml_location,dag_run_id, \"dict.pickle\")\r\n        with open(path_d, 'rb') as handle:\r\n            data = pickle.load(handle)\r\n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\r\n        # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\r\n        \r\n\r\n        # for key, value in data.items():\r\n        #     if value is None:\r\n        #         continue\r\n        #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\r\n            \r\n        print(\"dict pickle file loaded\")\r\n        filtered_dict = {k: v for k, v in data.items()}\r\n        result = {}\r\n        purchase_list = []\r\n        for item, val in filtered_dict.items():\r\n            # val is the path item is the segment name\r\n\r\n            df = pd.read_csv(val)\r\n            purchase_filter = purchase[purchase[msisdn_name].isin(df[msisdn_name])]\r\n           \r\n            # get all the unique products  in the filtered purchase\r\n            products = purchase_filter[TRANSACTION_PRODUCT_NAME].unique()\r\n            print(products.dtype)\r\n            products = products.astype(int)\r\n            purchase_list = []\r\n            print(\"products are ==>\", products.compute())\r\n            for product in products:\r\n                purchase_filter_one_product = purchase_filter[\r\n                    purchase_filter[TRANSACTION_PRODUCT_NAME] == product]\r\n                # read the mathix\r\n                product = str(product)\r\n                matrix_check = pd.read_csv(matrix_path, nrows=2)\r\n                print(\"matrix dataframe \\n\", matrix_check)\r\n                if product not in matrix_check.columns:\r\n                    print(\"product is not present\")\r\n                    continue\r\n\r\n                matrix = dd.read_csv(matrix_path, usecols=[MSISDN_COL_NAME, product])\r\n                purchase_filter1 = purchase_filter_one_product.merge(matrix, on=msisdn_name, how='inner')\r\n                purchase_filter2 = purchase_filter1[purchase_filter1[product] > threshold]\r\n                purchase_filter2 = purchase_filter2.drop(columns=[product])\r\n                purchase_list.append(purchase_filter2)\r\n\r\n            #20-4-23 change after manjus changes\r\n            if purchase_list is None or len(purchase_list)==0:\r\n                result[item] = None\r\n                result_path = os.path.join(purchase_filter_path, 'dict.pickle')\r\n                with open(result_path, 'wb') as handle:\r\n                    pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n                continue\r\n\r\n            #20-4-23 change after manjus changes\r\n\r\n            purchase_final = dd.concat(purchase_list)\r\n\r\n            print('purchase_final.isnull().sum()', purchase_final.isnull().sum().compute())\r\n\r\n            op_path = os.path.join(purchase_filter_path, item + \".csv\")\r\n            purchase_final = purchase_final.compute()\r\n            purchase_final.to_csv(op_path)\r\n            result[item] = op_path\r\n            result_path = os.path.join(purchase_filter_path, 'dict.pickle')\r\n            with open(result_path, 'wb') as handle:\r\n                pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n            \r\n            print(\"file added successfully\")\r\n            \r\n        return matrix\r\n        \r\n    except Exception as e:\r\n        print(e)\r\n        \r\n\r\n\r\n\r\ndef transform(dataframe):\r\n    df = matrix_filter(\"manual__2023-07-10T11:06:51\")\r\n    return df.compute()"
              }
            }, {
              "id": "b9ecbc38-079e-64bd-976f-b60677c77622",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from datetime import datetime, timedelta,date\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.metrics import accuracy_score\r\nfrom imblearn.under_sampling import RandomUnderSampler\r\nfrom sklearn.metrics import classification_report\r\nfrom collections import Counter\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.multioutput import MultiOutputClassifier\r\nfrom sklearn.ensemble import GradientBoostingClassifier\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.linear_model import LogisticRegression\r\nimport xgboost as xgb\r\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\r\nimport pandas as pd\r\nimport pickle\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\nfrom functools import reduce\r\n\r\nfrom icecream import ic\r\nfrom pathlib import Path\r\nimport dask.array as da\r\nfrom dask.dataframe import merge\r\nimport pickle\r\nimport pathlib\r\nimport dask.dataframe as dd\r\n\r\nimport pandas as pd\r\nimport pickle\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\n\r\n\r\nfrom sqlalchemy import create_engine\r\nfrom sqlalchemy.ext.declarative import declarative_base\r\nfrom sqlalchemy.orm import sessionmaker\r\nfrom urllib.parse import quote  \r\n\r\n\r\n\r\nfrom sqlalchemy.orm import Session\r\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\r\nfrom sqlalchemy.orm import relationship\r\nimport datetime\r\nfrom sqlalchemy.dialects.mysql import LONGTEXT\r\n\r\n\r\nfrom typing import List, Optional\r\n\r\nfrom pydantic import BaseModel\r\n\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nfrom sklearn.feature_selection import SelectFromModel\r\n\r\n\r\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\r\n\r\nengine = create_engine(\r\n    SQLALCHEMY_DATABASE_URL,\r\n)\r\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\r\n\r\nBase = declarative_base()\r\n\r\ndef get_db():\r\n    db = SessionLocal()\r\n    try:\r\n        return db\r\n    finally:\r\n        db.close()\r\n        \r\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\r\nsegement_names = ['trend', 'rfm']\r\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\r\npurchase_location = '/home/tnmops/seahorse3_bkp/'\r\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\r\netl_location = '/home/tnmops/seahorse3_bkp/'\r\npack_info_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location = '/home/tnmops/seahorse3_bkp/'\r\nlog_file = '/home/tnmops/seahorse3_bkp/'\r\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\r\n\r\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\r\n\r\nusage_no_months = ['m1', 'm2', 'm3']\r\nrecharge_no_months = ['m1', 'm2', 'm3']\r\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\r\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\nthreshold = 0.50\r\nnot_needed_rfm_segment = ['Lost']\r\n\r\npack_cols = ['product_id','product_type']\r\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\r\n\r\ncampaign_df= ['c4','c3']\r\ncampaign_usage= ['m4','m3']\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_location': self.purchase_location,\r\n        'usage_location': self.etl_location,\r\n        'pack_info_location': self.pack_info_location,\r\n        'ml_location': self.ml_location,\r\n        'recharge_location':self.recharge_location\r\n    }\r\n    \r\n    \r\n    \r\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\r\n    'recharge_total_cntm2',\r\n    'recharge_total_cntm3',\r\n    'm1_total_voice_usage',\r\n    'm1_total_data_usage',\r\n    'm2_total_voice_usage',\r\n    'm2_total_data_usage',\r\n    'm3_total_voice_usage',\r\n    'm3_total_data_usage',\r\n    'purchase_total_cntm1',\r\n    'purchase_total_cntm2',\r\n    'purchase_total_cntm3', \r\n    'm3_total_revenue_m2_total_revenue_pct_drop',\r\n    'm3_data_revenue_m2_data_revenue_pct_drop',\r\n    'm2_voice_rev_m1_voice_rev_pct_drop',\r\n    'm2_total_revenue_m1_total_revenue_pct_drop',\r\n    'm2_data_revenue_m1_data_revenue_pct_drop',\r\n    'm3_voice_rev_m2_voice_rev_pct_drop',\r\n    'm1_m2_m3_average_voice',\r\n    'm1_m2_m3_average_data', \r\n    'm1_no_of_days',\r\n    'm2_no_of_days',\r\n    'm3_no_of_days',\r\n    'eng_index',\r\n    'consecutive_inactive_days']\r\n\r\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\r\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\r\nMSISDN_COL_NAME = 'msisdn'\r\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\r\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\r\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\r\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \r\nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\r\n\r\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\r\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\r\nTRANSACTION_PRODUCT_NAME = 'product_id'\r\n\r\nPACK_CONTI_FEATURES = []\r\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\r\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\r\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\r\nPACK_INFO_CATEGORY = 'product_type'\r\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\r\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\r\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\r\nCUSTOMER_CATEG_FEATURES = []\r\nCUSTOMER_CONTI_FEATURES = []\r\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\r\n# 0 index for inbundle 1 index for outbundled  2 index for total\r\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\r\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\r\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\r\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\r\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\r\nCUSTOMER_NEEDED_COLUMN = [\r\n        'onnet_revenue',  'onnet_usage', 'onnet_voice_count', \r\n        'offnet_revenue', 'offnet_usage', 'offnet_voice_count',  \r\n        'idd_revenue', 'idd_usage', 'idd_voice_count',\r\n        'voice_rmg_revenue', 'voice_rmg_usage', 'voice_rmg_count', \r\n        'data_rmg_revenue',  'data_rmg_usage',  \r\n        'data_revenue', 'data_usage', 'sms_revenue', 'sms_usage',  'sms_idd_revenue', \r\n        'sms_idd_usage', 'magik_voice_amount', 'rbt_subscription_rev', 'emergency_credit_rev', \r\n        'package_revenue', 'voice_rev', 'sms_rev', 'onn_rev', 'off_rev', 'total_data_rev', \r\n        'vas_rev', 'vas_rev_others', 'total_revenue', 'total_voice_count', 'total_voice_duration', \r\n        'total_mainaccount_data_usage', 'total_sms_count', 'total_package_count', 'total_other_vas_count', \r\n        'total_voice_usage', 'total_data_usage', 'total_sms_usage'\r\n        ]\r\n\r\n\r\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\r\n\r\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\r\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\r\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\r\n\r\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\r\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\r\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\r\n\r\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\r\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\r\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\r\n\r\nLABEL1 = 'downtrend'\r\nLABEL2 = 'uptrend'\r\nLABEL3 = 'zigzag'\r\nLABEL4 = 'flat'\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_features': self.TRANSACTION_FEATURES,\r\n        'usage_features': self.CUSTOMER_FEATURES,\r\n        'pack_info_features': self.ALL_PACK_FEATURES,\r\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\r\n        'usage_dtypes': self.CUSTOMER_DTYPES\r\n\r\n    }\r\n    \r\n    \r\nimport logging\r\nimport os\r\n\r\ndef get_file_names():\r\n    return  { \r\n        \"purchase\": {\r\n            \"m1\": \"purchase_march_full_df.csv\",\r\n            \"m2\": \"purchase_feb_full_df.csv\",\r\n            \"m3\": \"purchase_jan_full_df.csv\"\r\n\r\n        },\r\n        \"rfm_purchase\":   {\r\n        \"p1\": \"purchase_nov_full_df.csv\",\r\n        \"p2\": \"purchase_dec_full_df.csv\",\r\n        \"p3\": \"purchase_jan_full_df.csv\",\r\n        \"p4\": \"purchase_feb_full_df.csv\",\r\n        \"p5\": \"purchase_march_full_df.csv\" },\r\n        \r\n        \"pack\":\r\n        {\r\n            \"pack\":\"pack_info.csv\"\r\n        }\r\n        }\r\n\r\n\r\n\r\n    # source_purchase_and_etl_location = \"/log/magikuser/tnm_autopilot_calender_month\"\r\n    # purchase_location = \"/log/magikuser/autopilot_data/purchase\"\r\n    # etl_location = \"/log/magikuser/autopilot_data/etl/etl_with_130_columns\"\r\n    # pack_info_location = '/log/magikuser/autopilot_data/packinfo'\r\n    # ml_location = '/log/magikuser/autopilot_data/ml'\r\n    #\r\n    \r\n\r\n\r\nclass SegmentInformation(Base):\r\n    __tablename__ = \"APA_segment_information_new\"\r\n    id = Column(Integer, primary_key=True, index=True)\r\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\r\n    end_date = Column(DateTime)\r\n    current_product = Column(String(80), nullable=True, unique=False)\r\n    current_products_names = Column(String(200), nullable=True, unique=False)\r\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\r\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\r\n    predicted_arpu = Column(Integer, nullable=True)\r\n    current_arpu = Column(Integer, nullable=True)\r\n    segment_length = Column(String(80), nullable=True, unique=False)\r\n    rule = Column(LONGTEXT, nullable=True)\r\n    actual_rule = Column(LONGTEXT, nullable=True)\r\n    uplift_percent = Column(Float(precision=2), nullable=True)\r\n    incremental_revenue = Column(Float(precision=2), nullable=True)\r\n    campaign_type = Column(String(80), nullable=True, unique=False)\r\n    campaign_name = Column(String(80), nullable=True, unique=False)\r\n    action_key = Column(String(80), nullable=True, unique=False)\r\n    robox_id = Column(String(80), nullable=True, unique=False)\r\n    dag_run_id = Column(String(80), nullable=True, unique=False)\r\n    samples = Column(Integer, nullable=False)\r\n    segment_name = Column(String(80), nullable=True, unique=False)\r\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\r\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\r\n    customer_status = Column(String(80), nullable=True, unique=False)\r\n    query = Column(LONGTEXT, nullable=True, unique=False)\r\n    cluster_no = Column(Integer, nullable=True)\r\n    confidence = Column(Float(precision=2), nullable=True)\r\n    recommendation_type = Column(String(80), nullable=True, unique=False)\r\n    cluster_description = Column(LONGTEXT, nullable=True)\r\n    actual_target_count = Column(Integer, nullable=True)\r\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\r\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\r\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\r\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\r\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\r\n    total_revenue= Column(Integer, nullable=True)\r\n    uplift_revenue=Column(Integer, nullable=True)\r\n\r\n    def __repr__(self):\r\n        return 'SegmentInformation(name=%s)' % self.name\r\n\r\nclass SegementInfo(BaseModel):\r\n    end_date: Optional[str] = None\r\n    dag_run_id: Optional[str] = None\r\n    current_product: Optional[str] = None\r\n    current_products_names: Optional[str] = None\r\n    recommended_product_id: Optional[str] = None\r\n    recommended_product_name: Optional[str] = None\r\n    predicted_arpu: Optional[int] = None\r\n    current_arpu: Optional[int] = None\r\n    segment_length: Optional[str] = None\r\n    rule: Optional[str] = None\r\n    actual_rule: Optional[str] = None\r\n    uplift_percent: Optional[float] = None\r\n    incremental_revenue: Optional[float] = None\r\n    campaign_type: Optional[str] = None\r\n    campaign_name: Optional[str] = None\r\n    action_key: Optional[str] = None\r\n    robox_id: Optional[str] = None\r\n    samples: Optional[int] = None\r\n    segment_name: Optional[str] = None\r\n    current_ARPU_band: Optional[str] = None\r\n    current_revenue_impact: Optional[str] = None\r\n    customer_status: Optional[str] = None\r\n    query: Optional[str] = None\r\n    cluster_no: Optional[int] = None\r\n    confidence: Optional[float] = None\r\n    recommendation_type: Optional[str] = None\r\n    cluster_description: Optional[str] = None\r\n    actual_target_count: Optional[str] = None\r\n    top_purchased_day_1: Optional[str] = None\r\n    top_purchased_day_2: Optional[str] = None\r\n    top_purchased_day_3: Optional[str] = None\r\n    next_purchase_date_range: Optional[str] = None\r\n    campaign_response_percentage: Optional[str] = None\r\n    total_revenue: Optional[int] = None\r\n    uplift_revenue: Optional[int] = None\r\n\r\nclass SegementRepo:\r\n    def create(db: Session, segement: SegementInfo):\r\n        db_item = SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\r\n                                            campaign_type=segement.campaign_type,\r\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\r\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\r\n                                            rule=segement.rule, samples=segement.samples,\r\n                                            campaign_name=segement.campaign_name,\r\n                                            recommended_product_id=segement.recommended_product_id,\r\n                                            recommended_product_name=segement.recommended_product_name,\r\n                                            current_product=segement.current_product,\r\n                                            current_products_names=segement.current_products_names,\r\n                                            segment_length=segement.segment_length,\r\n                                            current_ARPU_band=segement.current_ARPU_band,\r\n                                            current_revenue_impact=segement.current_revenue_impact,\r\n                                            customer_status=segement.customer_status,\r\n                                            segment_name=segement.segment_name, query=segement.query,\r\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\r\n                                            recommendation_type=segement.recommendation_type,\r\n                                            cluster_description=segement.cluster_description,\r\n                                            actual_target_count=segement.actual_target_count,\r\n                                            top_purchased_day_1=segement.top_purchased_day_1,\r\n                                            top_purchased_day_2=segement.top_purchased_day_2,\r\n                                            top_purchased_day_3=segement.top_purchased_day_3,\r\n                                            next_purchase_date_range=segement.next_purchase_date_range,\r\n                                            campaign_response_percentage=segement.campaign_response_percentage,\r\n                                            total_revenue=segement.total_revenue,\r\n                                            uplift_revenue=segement.uplift_revenue,\r\n                                            )\r\n        db.add(db_item)\r\n        db.commit()\r\n        db.refresh(db_item)\r\n        return db_item\r\n\r\n    def findByAutoPilotIdAndClusterNo(db: Session, _id, cluster_no):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_no) \\\r\n            .all()\r\n\r\n    def findByAutoPilotIdAndSegementName(db: Session, _id, segement_name, cluster_number):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_number).first()\r\n\r\n    def findByAutoPilotIdAndSegementNameAll(db: Session, _id, segement_name, cluster_number):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_number).all()\r\n    \r\n    def findByAutoPilotIdAndSegementNamewithoutcluster(db: Session, _id, segement_name):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name).all()\r\n            \r\n\r\n\r\n    def findByAutoPilotId(db: Session, _id):\r\n        return db.query(SegmentInformation).filter(SegmentInformation.dag_run_id == _id).all()\r\n\r\n    def findByAutoPilotIdRecommendedId(db: Session, _id,recommended_product_id):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.recommended_product_id == recommended_product_id).all()\r\n\r\n    def deleteById(db: Session, _ids):\r\n        for id in _ids:\r\n            db.query(SegmentInformation).filter(SegmentInformation.id == id).delete()\r\n            db.commit()\r\n\r\n    def update(db: Session, item_data):\r\n        updated_item = db.merge(item_data)\r\n        db.commit()\r\n        return updated_item\r\n\r\ndef weekday_product_purchase_count(dag_run_id):\r\n    \r\n    msisdn_name =  MSISDN_COL_NAME\r\n    purchase_no_months = ['m1', 'm2', 'm3']\r\n    file_name_dict = get_file_names()\r\n    print(file_name_dict)\r\n    print(\"finding weekday_puchase_count ongoing \")\r\n    purchase = {}\r\n    \r\n    for month in purchase_no_months:\r\n        purchase[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"purchase\").get(month)),)\r\n\r\n    \r\n    df_list = [purchase['m1'],purchase['m2'],purchase['m3']]\r\n    pivoted_dict = {}\r\n    for i, df in enumerate(df_list, 1):\r\n        df['cdr_date'] = dd.to_datetime(df['cdr_date'])  \r\n        df = df[df['cdr_date'].dt.dayofweek < 7]\r\n        df['day_of_week'] = df['cdr_date'].dt.day_name()\r\n        df['day_of_week'] = df['day_of_week'].astype('category').cat.as_known()\r\n        pivoted = df.pivot_table(index='product_id', columns='day_of_week', values='total_cnt', aggfunc='sum')\r\n        pivoted_dict[f\"m{i}\"] = pivoted\r\n    pivot_tables = [pivoted_dict['m1'],pivoted_dict['m2'], pivoted_dict['m3']]\r\n    \r\n    merged_pivot = reduce(lambda left, right: dd.merge(left, right, on='product_id', how='outer'), pivot_tables)\r\n    merged_pivot = merged_pivot.fillna(0)\r\n    \r\n    columns_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\r\n    \r\n    for day in columns_order:\r\n        merged_pivot[f\"{day.lower()}\"] = merged_pivot[f\"{day}_x\"] + merged_pivot[f\"{day}_y\"] + merged_pivot[day]\r\n    columns  = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\r\n    merged_pivot['total'] = merged_pivot[columns].sum(axis=1)\r\n    \r\n    merged_pivot = merged_pivot.sort_values(by='total', ascending=False)\r\n    \r\n    merged_pivot = merged_pivot.drop(columns=['Monday_x', 'Tuesday_x', 'Wednesday_x', 'Thursday_x', 'Friday_x', 'Saturday_x', 'Sunday_x',\r\n                           'Monday_y', 'Tuesday_y', 'Wednesday_y', 'Thursday_y', 'Friday_y', 'Saturday_y', 'Sunday_y',\r\n                           'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\r\n    merged_pivot['top_purchased_day_1'] = merged_pivot[columns].idxmax(axis=1)\r\n    merged_pivot['top_purchased_day_2'] = merged_pivot[columns].apply(lambda x: x.nlargest(2).index[1], axis=1)\r\n    merged_pivot['top_purchased_day_3'] = merged_pivot[columns].apply(lambda x: x.nlargest(3).index[2], axis=1)\r\n    merged_pivot = merged_pivot.reset_index()\r\n\r\n    merged_pivot_pd =merged_pivot.compute()\r\n    \r\n    weekday_product_purchase_count_path = os.path.join(ml_location,dag_run_id,  \"weekday_product_purchase_count.csv\")\r\n    \r\n    print(\"weekday_product_purchase_count   file output is ongoing \")\r\n    merged_pivot_pd.to_csv(weekday_product_purchase_count_path,header=True,index=False)\r\n    \r\n    \r\n    \r\ndef recommended_id_prediction(dag_run_id):\r\n    \r\n    file_path = ml_location+'/{}/predicted_result'.format(dag_run_id)\r\n    s = pd.read_parquet(file_path, engine='pyarrow').reset_index(drop=True)\r\n    \r\n    def transform_value(value):\r\n        \r\n        if value ==0:\r\n            return '0-7'\r\n        elif value==1:\r\n            return '7-14'\r\n        elif value ==2:\r\n            return '14-30'\r\n        else:\r\n            return '30+'\r\n\r\n    # Apply the transformation using the apply function\r\n    s['PredictedValues'] = s['PredictedValues'].fillna(0)\r\n    s['PredictedValues'] = s['PredictedValues'].astype(int)\r\n    s['range'] = s['PredictedValues'].apply(transform_value)\r\n    \r\n    result = s.groupby(['recommended_product_id', 'PredictedValues']).size().reset_index(name='PredictedValues_counts')\r\n    max_counts = result.groupby('recommended_product_id')['PredictedValues_counts'].idxmax()\r\n    result = result.loc[max_counts]\r\n    result['range'] = result['PredictedValues'].apply(transform_value)\r\n\r\n    path = os.path.join(ml_location,dag_run_id,  'recommended_id_prediction.csv')   \r\n    result.to_csv(path,index=False,header=True)    \r\n    return result\r\n    \r\ndef nextthreemonth(purchase, dag_run_id):\r\n\r\n\r\n    file_path = ml_location+'/{}/output_data/APA_output.csv'.format(dag_run_id)\r\n\r\n    fin = dd.read_csv(file_path)\r\n    li = fin['recommended_product_id'].unique().compute().tolist()\r\n\r\n    file_name_dict = get_file_names()\r\n\r\n    pack = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"pack\").get('pack')),usecols=pack_cols)#packinfo loading\r\n    \r\n    source1 = dd.concat([purchase['p2'], purchase['p3'], purchase['p4']], axis=0, ignore_index=True, verify_integrity=False)\r\n    target1 = dd.concat([purchase['p5']])\r\n    \r\n    \r\n    result = pd.DataFrame({'msisdn': [], 'PredictedValues': [],'recommended_product_id': [], 'product_type': []})\r\n    result = dd.from_pandas(result, npartitions=2)\r\n\r\n    for i in li:\r\n        \r\n        msi = fin[fin['recommended_product_id'] == i][['msisdn']]                \r\n        \r\n        source =  merge(source1,msi,how='inner',on='msisdn')\r\n        target =  merge(target1,msi,how='inner',on='msisdn')\r\n        if len(source)<50   or len(target)<50:\r\n            continue     \r\n            \r\n        print('len source ',len(source))\r\n        print('source.column ',source.columns)\r\n        print('len target ',len(target))\r\n        print('target.column ',target.columns)\r\n        final = nextpurchase(source,target,'file',dag_run_id)\r\n\r\n        x,x1= final.drop(['NextPurchaseDayRange','msisdn','NextPurchaseDay'],axis=1),final['msisdn']   \r\n                        \r\n\r\n        c = pack[pack['product_id'] == i]['product_type'].compute().tolist()           \r\n            \r\n            \r\n        feature_path = os.path.join(ml_location,dag_run_id,  c[0]+\"_feature.pkl\")  \r\n        columns = pickle.load(open(feature_path, 'rb'))\r\n            \r\n        output_list = [j for j in columns if j not in x.columns]\r\n            \r\n        for k in output_list:\r\n            x[k]=0 \r\n            \r\n        pickel_path = os.path.join(ml_location,dag_run_id,c[0]+\".pkl\")\r\n        loaded_model = pickle.load(open(pickel_path, 'rb'))\r\n        feature_names = loaded_model.get_booster().feature_names\r\n        x=x[feature_names]\r\n            \r\n        y = loaded_model.predict(x)    \r\n        y_df = dd.from_array(y, columns=['PredictedValues'])\r\n\r\n        predict = dd.concat([x1, y_df], axis=1)\r\n        predict['recommended_product_id'] = i\r\n        predict['product_type'] = c[0]\r\n\r\n        result = dd.concat([result, predict], axis=0)            \r\n        \r\n    path = os.path.join(ml_location, dag_run_id,'predicted_result') \r\n    result.to_parquet(path,schema='infer') \r\n    \r\n    \r\ndef xgmodel(final, name, dag_run_id):\r\n\r\n\r\n\r\n    X, y = final.drop(['NextPurchaseDayRange','msisdn','NextPurchaseDay'],axis=1), final.NextPurchaseDayRange\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)\r\n    \r\n    \r\n    path = os.path.join(ml_location,dag_run_id,  name+\"_feature.pkl\")\r\n    pickle.dump(X.columns.tolist(), open(path, 'wb'))\r\n        \r\n    \r\n    rus = RandomUnderSampler(random_state=42)\r\n    X_train, y_train = rus.fit_resample(X, y)\r\n    print('Resampled dataset shape %s' % Counter(y_train))    \r\n    \r\n\r\n    xgb_model = xgb.XGBClassifier().fit(X_train, y_train)\r\n    \r\n    print('Accuracy of XGB classifier on training set: {:.2f}'\r\n           .format(xgb_model.score(X_train, y_train)))\r\n    print('Accuracy of XGB classifier on test set: {:.2f}'\r\n           .format(xgb_model.score(X_test[X_train.columns], y_test)))\r\n\r\n\r\n    y_pred = xgb_model.predict(X_test)\r\n    print(classification_report(y_test, y_pred))\r\n \r\n    \r\n    path = os.path.join(ml_location,dag_run_id,  name+\".pkl\")    \r\n    pickle.dump(xgb_model, open(path, 'wb'))\r\n    print('model created ')\r\n    \r\ndef segmentaion_fun1(r, f, m):\r\n    segments = []\r\n    for r, f, m in zip(r, f, m):\r\n        value = int(f\"{r}{f}{m}\")\r\n\r\n        if value in [555, 554, 544, 545, 454, 455, 445]:\r\n            segments.append(\"Champions\")\r\n        elif value in [543, 444, 435, 355, 354, 345, 344, 335]:\r\n            segments.append(\"Loyal_Customers\")\r\n        elif value in [553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323]:\r\n            segments.append(\"Potential_Loyalist\")\r\n        elif value in [512, 511, 422, 421, 412, 411, 311]:\r\n            segments.append(\"Recent_Customers\")\r\n        elif value in [525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313]:\r\n            segments.append(\"Promising_Customers\")\r\n        elif value in [535, 534, 443, 434, 343, 334, 325, 324]:\r\n            segments.append(\"Customers_needing_Attention\")\r\n        elif value in [331, 321, 312, 221, 213]:\r\n            segments.append(\"About_to_Sleep\")\r\n        elif value in [255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124]:\r\n            segments.append(\"At_Risk\")\r\n        elif value in [155, 154, 144, 214, 215, 115, 114, 113]:\r\n            segments.append(\"Cant_Loose_them\")\r\n        elif value in [332, 322, 231, 241, 251, 233, 232, 223, 222, 132, 123, 122, 212, 211]:\r\n            segments.append(\"Hibernating\")\r\n        else:\r\n            segments.append(\"Lost\")\r\n\r\n    return segments\r\n    \r\n\r\n\r\ndef join_rfm(x):\r\n    return str(int(x['R_Score'])) + str(int(x['F_Score'])) + str(int(x['M_Score']))\r\n    \r\n\r\ndef form_segements_purchase_rfm(ctm_class):\r\n    def process_duplicates(li):\r\n        for i in range(len(li) - 1):\r\n            print(li[i], i)\r\n            if i + 1 == len(li) - 1:\r\n                li[i] = ((li[i - 1] + li[i]) / 2)\r\n            elif li[i] == li[i + 1]:\r\n                li[i + 1] = ((li[i + 1] + li[i + 2]) / 2)\r\n        return li\r\n\r\n    # bins_recency = [-1,\r\n    #                 np.percentile(ctm_class[\"Recency\"], 20),\r\n    #                 np.percentile(ctm_class[\"Recency\"], 40),\r\n    #                 np.percentile(ctm_class[\"Recency\"], 60),\r\n    #                 np.percentile(ctm_class[\"Recency\"], 80),\r\n    #                 ctm_class[\"Recency\"].max().compute()]\r\n    # bins_recency.sort()\r\n    # bins_recency = process_duplicates(bins_recency)\r\n    # # bins_recency = [-1,17,35,53,71, ctm_class[\"Recency\"].max().compute()]\r\n\r\n    # bins_recency.sort()\r\n    # bins_recency = process_duplicates(bins_recency)\r\n\r\n    ctm_class1=ctm_class.copy()\r\n    ctm_class1['Recency']=ctm_class1['Recency'].astype('int')\r\n    ctm_class1 = ctm_class1.groupby('Recency').agg({'msisdn':'count'}).reset_index()\r\n    mean_recency = ctm_class1['Recency'].max().compute()/5\r\n    print(\"mean_recency\", mean_recency)\r\n    print(\"mean_recency *2\",mean_recency * 2)\r\n    print(\"mean_recency *3\",mean_recency * 3)\r\n    print(\"mean_recency *4\",mean_recency * 4)\r\n    \r\n    \r\n    bins_recency = [-1,\r\n                    int(mean_recency),\r\n                    int(mean_recency * 2),\r\n                    int(mean_recency * 3),\r\n                    int(mean_recency * 4),\r\n                    ctm_class[\"Recency\"].max().compute()]\r\n    \r\n    print('bins_recency is', bins_recency)\r\n\r\n\r\n    ctm_class['R_Score'] = ctm_class[\"Recency\"].map_partitions(pd.cut,\r\n                                                               bins=bins_recency,\r\n                                                               labels=[5, 4, 3, 2, 1]).astype(\"int\")\r\n\r\n    \r\n\r\n    # bins_frequency = [-1,\r\n    #                   np.percentile(ctm_class[\"Frequency\"], 20),\r\n    #                   np.percentile(ctm_class[\"Frequency\"], 40),\r\n    #                   np.percentile(ctm_class[\"Frequency\"], 60),\r\n    #                   np.percentile(ctm_class[\"Frequency\"], 80),\r\n    #                   ctm_class[\"Frequency\"].max().compute()]\r\n\r\n    # bins_frequency.sort()\r\n    # bins_frequency = process_duplicates(bins_frequency)\r\n\r\n\r\n    ctm_class1=ctm_class.copy()\r\n    ctm_class1['Frequency']=ctm_class1['Frequency'].astype('int')\r\n    ctm_class1 = ctm_class1.groupby('Frequency').agg({'msisdn':'count'}).reset_index()\r\n    mean_frequency = ctm_class1['Frequency'].max().compute()/5\r\n    print(\"mean_frequency\", mean_frequency)\r\n    print(\"mean_frequency *2\",mean_frequency * 2)\r\n    print(\"mean_frequency *3\",mean_frequency * 3)\r\n    print(\"mean_frequency *4\",mean_frequency * 4)\r\n\r\n\r\n    bins_frequency = [-1,\r\n                    int(mean_frequency),\r\n                    int(mean_frequency * 2),\r\n                    int(mean_frequency * 3),\r\n                    int(mean_frequency * 4),\r\n                    ctm_class[\"Frequency\"].max().compute()]\r\n    \r\n    print('bins_frequency is', bins_frequency)\r\n    ctm_class['F_Score'] = ctm_class[\"Frequency\"].map_partitions(pd.cut,\r\n                                                                 bins=bins_frequency,\r\n                                                                 labels=[1, 2, 3, 4, 5]).astype(\"int\")\r\n\r\n    # bins_revenue = [-1,\r\n    #                 np.percentile(ctm_class[\"Revenue\"], 20),\r\n    #                 np.percentile(ctm_class[\"Revenue\"], 40),\r\n    #                 np.percentile(ctm_class[\"Revenue\"], 60),\r\n    #                 np.percentile(ctm_class[\"Revenue\"], 80),\r\n    #                 ctm_class[\"Revenue\"].max().compute()]\r\n    # bins_revenue.sort()\r\n    # bins_revenue = process_duplicates(bins_revenue)\r\n\r\n    # bins_revenue = [-1,189,380,571,762, ctm_class[\"Revenue\"].max().compute()]\r\n\r\n    ctm_class1=ctm_class.copy()\r\n    ctm_class1['Revenue'] = ctm_class1['Revenue'].map_partitions(lambda series: series.apply(np.ceil))\r\n    ctm_class1['Revenue']=ctm_class1['Revenue'].astype('int')\r\n    ctm_class1 = ctm_class1.groupby('Revenue').agg({'msisdn':'count'}).reset_index()\r\n    mean_revenue = ctm_class1['Revenue'].max().compute()/5\r\n    \r\n    print(\"mean_revenue\", mean_revenue)\r\n    print(\"mean_revenue *2\",mean_revenue * 2)\r\n    print(\"mean_revenue *3\",mean_revenue * 3)\r\n    print(\"mean_revenue *4\",mean_revenue * 4)\r\n    \r\n    bins_revenue = [-1,\r\n                    int(mean_revenue),\r\n                    int(mean_revenue * 2),\r\n                    int(mean_revenue * 3),\r\n                    int(mean_revenue * 4),\r\n                    ctm_class[\"Revenue\"].max().compute()]\r\n    \r\n    print('bins_revenue is', bins_revenue)\r\n\r\n\r\n    ctm_class['M_Score'] = ctm_class[\"Revenue\"].map_partitions(pd.cut,\r\n                                                               bins=bins_revenue,\r\n                                                               labels=[1, 2, 3, 4, 5]).astype(\"int\")\r\n    print(\"done with scoring\")\r\n    # Form RFM segment\r\n\r\n    ctm_class['RFM_Segment'] = ctm_class.apply(join_rfm, axis=1)\r\n    ctm_class['RFM_Segment'] = ctm_class['RFM_Segment'].astype(int)\r\n    print(\"formed rfm segement \")\r\n    ctm_class['R_Score'] = ctm_class['R_Score'].astype(int)\r\n    ctm_class['F_Score'] = ctm_class['F_Score'].astype(int)\r\n    ctm_class['M_Score'] = ctm_class['M_Score'].astype(int)\r\n    print(\"computing rfm\")\r\n    r = ctm_class['R_Score'].values.compute()\r\n    f = ctm_class['F_Score'].values.compute()\r\n    m = ctm_class['M_Score'].values.compute()\r\n    seg = segmentaion_fun1(r, f, m)\r\n    chunks = ctm_class.map_partitions(lambda x: len(x)).compute().to_numpy()\r\n    myarray = da.from_array(seg, chunks=tuple(chunks))\r\n    ctm_class['Segment'] = myarray\r\n    return ctm_class\r\n    \r\n    \r\ndef otliner_removal(df,col, per=0.97):\r\n    q = df[col].quantile(per)\r\n    print(f\"col is {col} and q is {q}\")\r\n    print(\"the length brfore is\", len(df))\r\n    outliers = df[df[col] > q]\r\n    df1 = df[~df['msisdn'].isin(outliers['msisdn'].compute().values)]\r\n    print(\"the length after is\", len(df1))\r\n    return df1\r\n    \r\ndef perform_rfm_on_purchase(df_data_rfm, period=95):\r\n\r\n    ic(\"inside perform_rfm\")\r\n    \r\n    df_data_rfm.purchase_date = dd.to_datetime(df_data_rfm.purchase_date)\r\n    df_data_rfm = df_data_rfm.fillna(0)\r\n    msisdn_name = MSISDN_COL_NAME\r\n    # current behaviour data\r\n    min_date = df_data_rfm['purchase_date'].min().compute()\r\n    cur_beh_data_rfm = df_data_rfm[(df_data_rfm.purchase_date <= pd.Timestamp(min_date) + pd.Timedelta(days=period))\r\n                                   & (df_data_rfm.purchase_date >= pd.Timestamp(min_date))].reset_index(drop=True)\r\n    # Get the maximum purchase date of each customer and create a dataframe with it together with the customer's id.\r\n    ctm_max_purchase = cur_beh_data_rfm.groupby(msisdn_name).purchase_date.max().reset_index()\r\n    ctm_max_purchase.columns = [msisdn_name, 'MaxPurchaseDate']\r\n\r\n    # Find the recency of each customer in days\r\n    ctm_max_purchase['Recency'] = (\r\n            ctm_max_purchase['MaxPurchaseDate'].max() - ctm_max_purchase['MaxPurchaseDate']).dt.days\r\n    ctm_max_purchase = ctm_max_purchase.drop(columns=['MaxPurchaseDate'])\r\n    print(\"done with recency \")\r\n    # frequency\r\n    ctm_frequency = cur_beh_data_rfm.groupby(msisdn_name).total_cnt.sum().reset_index()\r\n    ctm_frequency.columns = [msisdn_name, 'Frequency']\r\n    print(\"done with frequency \")\r\n    cur_beh_data_rfm['Revenue'] = cur_beh_data_rfm[RECHARGE_TRANSACTION_PRICE_COL_NAME]\r\n    ctm_revenue = cur_beh_data_rfm.groupby('msisdn').Revenue.sum().reset_index()\r\n    print(\"done with monitory \")\r\n    rfm_data_base = dd.concat([ctm_max_purchase, ctm_revenue, ctm_frequency], axis=1)\r\n    rfm_data_base = rfm_data_base.loc[:, ~rfm_data_base.columns.duplicated()]\r\n\r\n    #for tnm purpose start\r\n    #rfm_data_base=otliner_removal(rfm_data_base,'Frequency')\r\n    rfm_data_base=otliner_removal(rfm_data_base,'Revenue')\r\n    \r\n    #for tnm purpose end \r\n\r\n\r\n    return rfm_data_base\r\n    \r\ndef rfm_process_quantile_method_purchase(df_data):\r\n    try: \r\n        \r\n        df_data = df_data.fillna(0, )\r\n\r\n        print(len(df_data))\r\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\r\n        ctm_class = perform_rfm_on_purchase(df_data, period=95)\r\n\r\n        print(len(ctm_class))\r\n        \r\n        ctm_class = form_segements_purchase_rfm(ctm_class)\r\n\r\n        print(len(ctm_class))\r\n        \r\n        return ctm_class\r\n\r\n    except Exception as e:\r\n        print(e) \r\n        \r\n\r\ndef NextPurchaseDayRange(tx_class):\r\n    \r\n    tx_class['NextPurchaseDayRange'] = 0\r\n\r\n    # Use Dask function 'where' to perform conditional assignments\r\n    tx_class['NextPurchaseDayRange'] = tx_class['NextPurchaseDayRange'].where(\r\n        ~((tx_class['NextPurchaseDay'] > 7) & (tx_class['NextPurchaseDay'] <= 14)), 1\r\n    )\r\n    tx_class['NextPurchaseDayRange'] = tx_class['NextPurchaseDayRange'].where(\r\n        ~((tx_class['NextPurchaseDay'] > 14) & (tx_class['NextPurchaseDay'] <= 30)), 2\r\n    )\r\n    tx_class['NextPurchaseDayRange'] = tx_class['NextPurchaseDayRange'].where(\r\n        ~(tx_class['NextPurchaseDay'] > 30), 3\r\n    )\r\n\r\n    tx_class = tx_class.fillna(0)\r\n    \r\n    return tx_class\r\n    \r\n    \r\ndef nextpurchase(source, target,filename, dag_run_id):    \r\n    \r\n    \r\n    source['cdr_date'] = dd.to_datetime(source['cdr_date'])\r\n    target['cdr_date'] = dd.to_datetime(target['cdr_date'])\r\n\r\n    tx_user = source['msisdn'].unique().to_frame(name='msisdn')\r\n\r\n    # Last month\r\n    tx_next_first_purchase = target.groupby('msisdn').cdr_date.min().reset_index()\r\n    tx_next_first_purchase.columns = ['msisdn', 'MinPurchaseDate']\r\n\r\n    # Two-month data\r\n    tx_last_purchase = source.groupby('msisdn').cdr_date.max().reset_index()\r\n    tx_last_purchase.columns = ['msisdn', 'MaxPurchaseDate']\r\n\r\n    # Merge two data\r\n    tx_purchase_dates = merge(tx_last_purchase, tx_next_first_purchase, on='msisdn', how='left')\r\n\r\n    # Calculating difference\r\n    tx_purchase_dates['NextPurchaseDay'] = (tx_purchase_dates['MinPurchaseDate'] - tx_purchase_dates['MaxPurchaseDate']).dt.days\r\n\r\n    # Merge with tx_user\r\n    tx_user = merge(tx_user, tx_purchase_dates[['msisdn', 'NextPurchaseDay']], on='msisdn', how='left')\r\n    tx_user = tx_user.fillna(999)\r\n\r\n    # Create a Dask dataframe with msisdn and cdr_date\r\n    tx_day_order = source[['msisdn', 'cdr_date']].compute()\r\n\r\n    # Convert cdr_date to day\r\n    tx_day_order['InvoiceDay'] = tx_day_order['cdr_date'].dt.date\r\n    tx_day_order = tx_day_order.sort_values(['msisdn', 'cdr_date'])\r\n\r\n    # Drop duplicates\r\n    tx_day_order = tx_day_order.drop_duplicates(subset=['msisdn', 'cdr_date'], keep='first')\r\n\r\n    # Shifting last 3 purchase dates\r\n    tx_day_order['PrevInvoiceDate'] = tx_day_order.groupby('msisdn')['InvoiceDay'].shift(1)\r\n    tx_day_order['T2InvoiceDate'] = tx_day_order.groupby('msisdn')['InvoiceDay'].shift(2)\r\n    tx_day_order['T3InvoiceDate'] = tx_day_order.groupby('msisdn')['InvoiceDay'].shift(3)\r\n\r\n    tx_day_order['DayDiff'] = (tx_day_order['InvoiceDay'] - tx_day_order['PrevInvoiceDate']).dt.days\r\n    tx_day_order['DayDiff2'] = (tx_day_order['InvoiceDay'] - tx_day_order['T2InvoiceDate']).dt.days\r\n    tx_day_order['DayDiff3'] = (tx_day_order['InvoiceDay'] - tx_day_order['T3InvoiceDate']).dt.days\r\n    \r\n    \r\n\r\n    tx_day_diff = tx_day_order.groupby('msisdn').agg({'DayDiff': ['mean', 'std']}).reset_index()\r\n    tx_day_diff.columns = ['msisdn', 'DayDiffMean', 'DayDiffStd']\r\n\r\n    tx_day_order_last = tx_day_order.drop_duplicates(subset=['msisdn'], keep='last')\r\n    tx_day_order_last = tx_day_order_last.fillna(0)\r\n\r\n    tx_day_order_last = merge(tx_day_order_last, tx_day_diff, on='msisdn')\r\n    tx_user = merge(tx_user, tx_day_order_last[['msisdn','DayDiff', 'DayDiff2', 'DayDiff3', 'DayDiffMean', 'DayDiffStd']], on='msisdn')\r\n    \r\n    \r\n    \r\n    #create tx_class as a copy of tx_user before applying get_dummies\r\n    tx_class = tx_user.copy()\r\n\r\n    tx_class = NextPurchaseDayRange(tx_class)   \r\n\r\n    \r\n    rfm = rfm_process_quantile_method_purchase(source)\r\n\r\n    rfm = rfm.compute()\r\n    final= merge(rfm,tx_class,how='inner',on='msisdn')    \r\n    \r\n    final = final.categorize(columns=['Segment'])\r\n    final = dd.get_dummies(final, columns=['Segment'])\r\n    \r\n    path = os.path.join(ml_location, dag_run_id, filename)   \r\n    Path(path).mkdir(parents=True, exist_ok=True)\r\n    final.to_parquet(path)\r\n    \r\n    \r\n    return final.compute()\r\n    \r\n\r\n\r\ndef next_purchase_date_model_creation(dag_run_id, db):\r\n        file_name_dict = get_file_names()\r\n        \r\n        purchase = {}\r\n        for month in purchase_no_months_for_next_purchase:\r\n            purchase[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"rfm_purchase\").get(month)))\r\n\r\n        for key, value in purchase.items():\r\n            value = value.reset_index(drop=True)    \r\n        \r\n        \r\n        source = dd.concat([purchase['p1'], purchase['p2'], purchase['p3']], axis=0, ignore_index=True, verify_integrity=False)\r\n        target = dd.concat([purchase['p4']])    \r\n        \r\n        \r\n        pack = dd.read_csv(\"/home/tnmops/seahorse3_bkp/pack_info.csv\",usecols=['product_id','product_type'])\r\n        f = merge(source,pack,how='inner',on='product_id')\r\n        f1 = merge(target,pack,how='inner',on='product_id')\r\n        \r\n        \r\n        p_type = ['DATA','VOICE','COMBO']\r\n        \r\n        for i in p_type:\r\n\r\n            print('the product type is------------ ',i)\r\n            \r\n            source = f[f['product_type']==i]\r\n            target = f1[f1['product_type']==i]\r\n        \r\n        \r\n            final = nextpurchase(source,target,i+\"_model\",dag_run_id)\r\n            xgmodel(final,i, dag_run_id)\r\n        nextthreemonth(purchase, dag_run_id)\r\n        recommended_id_prediction(dag_run_id)\r\n\r\n\r\n        weekday_product_purchase_count_path = os.path.join(ml_location, dag_run_id, \"weekday_product_purchase_count.csv\")\r\n        weekday_product_purchase_count(dag_run_id)\r\n        weekday_product_purchase_count_df = pd.read_csv(weekday_product_purchase_count_path)\r\n        print('readed weekday_product_purchase_count_df')\r\n        next_purchase_date_path = os.path.join(ml_location,dag_run_id, 'recommended_id_prediction.csv')   \r\n        \r\n        next_purchase_date_df = pd.read_csv(next_purchase_date_path)\r\n        print('readed next_purchase_date_df')\r\n        next_purchase_date_df['recommended_product_id']=next_purchase_date_df['recommended_product_id'].astype(int)\r\n        print('recommended_product_id converted to int')\r\n        segements_two = SegementRepo.findByAutoPilotId(db=db, _id=dag_run_id)\r\n        for seg in segements_two:\r\n            if seg.recommended_product_id is  None or seg.recommended_product_id ==0:\r\n                ic(\"deleteing segement if recommended_product_id is none \")\r\n                print(\"seg.id 1  is \",seg.id)\r\n                SegementRepo.deleteById(db=db, _ids=[seg.id])\r\n        segements_three = SegementRepo.findByAutoPilotId(db=db, _id=dag_run_id)\r\n     \r\n        for seg in segements_three:   \r\n            print(\"seg.id 2  is \",seg.id)\r\n            print('seg.recommended_product_id',seg.recommended_product_id)\r\n            \r\n            recommended_pid = int(seg.recommended_product_id)\r\n            print('recommended_pid is ',recommended_pid)\r\n            print('type recommended_pid is ',type(recommended_pid))\r\n\r\n            print('type next_purchase_date_df is ',next_purchase_date_df.dtypes)\r\n\r\n            print('len is ', weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id'] == recommended_pid])\r\n            print(\"weekday is \", weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0])\r\n     \r\n            seg.top_purchased_day_1 = weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0]\r\n            seg.top_purchased_day_2=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_2'].iloc[0]\r\n            seg.top_purchased_day_3=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_3'].iloc[0]\r\n            if recommended_pid in next_purchase_date_df['recommended_product_id'].unique():\r\n                seg.next_purchase_date_range=next_purchase_date_df[next_purchase_date_df['recommended_product_id']==recommended_pid]['range'].iloc[0]\r\n            else:\r\n                seg.next_purchase_date_range=\"30+\" #default\r\n            SegementRepo.update(db=db, item_data=seg)\r\n            \r\n        print(\"next purchase is completed\")\r\n        return pack\r\n    \r\n\r\ndef transform(dataframe):\r\n    db = get_db()\r\n    df = next_purchase_date_model_creation(\"manual__2023-07-10T11:06:51\",db)\r\n    return dataframe"
              }
            }, {
              "id": "f8c1f429-01a6-c2eb-b884-23a343381446",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nimport pickle\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\n\r\n\r\nfrom sqlalchemy import create_engine\r\nfrom sqlalchemy.ext.declarative import declarative_base\r\nfrom sqlalchemy.orm import sessionmaker\r\nfrom urllib.parse import quote  \r\n\r\n\r\n\r\nfrom sqlalchemy.orm import Session\r\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\r\nfrom sqlalchemy.orm import relationship\r\nimport datetime\r\nfrom sqlalchemy.dialects.mysql import LONGTEXT\r\n\r\n\r\nfrom typing import List, Optional\r\n\r\nfrom pydantic import BaseModel\r\n\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nfrom sklearn.feature_selection import SelectFromModel\r\n\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.metrics import classification_report\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.feature_selection import SelectFromModel\r\nimport sklearn\r\nimport json\r\nimport dask.dataframe as dd\r\nimport os\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\nfrom icecream import ic\r\nimport dask.dataframe as dd\r\nimport os\r\n# import configuration.config as cfg\r\n# import configuration.features as f\r\nimport traceback\r\nimport numpy as np\r\nfrom pathlib import Path\r\nimport pickle\r\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\r\nfrom pathlib import Path\r\nimport requests\r\n# from sql_app.repositories import AssociationRepo\r\n\r\n# config = cfg.Config().to_json()\r\n# features = f.Features().to_json()\r\n\r\n\r\n\r\n\r\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\r\n'recharge_total_cntm2',\r\n'recharge_total_cntm3',\r\n'm1_total_voice_usage',\r\n'm1_total_data_usage',\r\n'm2_total_voice_usage',\r\n'm2_total_data_usage',\r\n'm3_total_voice_usage',\r\n'm3_total_data_usage',\r\n'purchase_total_cntm1',\r\n'purchase_total_cntm2',\r\n'purchase_total_cntm3', \r\n'm3_total_revenue_m2_total_revenue_pct_drop',\r\n'm3_data_revenue_m2_data_revenue_pct_drop',\r\n'm2_voice_rev_m1_voice_rev_pct_drop',\r\n'm2_total_revenue_m1_total_revenue_pct_drop',\r\n'm2_data_revenue_m1_data_revenue_pct_drop',\r\n'm3_voice_rev_m2_voice_rev_pct_drop',\r\n'm1_m2_m3_average_voice',\r\n'm1_m2_m3_average_data', \r\n'm1_no_of_days',\r\n'm2_no_of_days',\r\n'm3_no_of_days',\r\n'eng_index',\r\n'consecutive_inactive_days']\r\n\r\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\r\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\r\nMSISDN_COL_NAME = 'msisdn'\r\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\r\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\r\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\r\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \r\nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\r\n\r\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\r\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\r\nTRANSACTION_PRODUCT_NAME = 'product_id'\r\n\r\nPACK_CONTI_FEATURES = []\r\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\r\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\r\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\r\nPACK_INFO_CATEGORY = 'product_type'\r\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\r\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\r\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\r\nCUSTOMER_CATEG_FEATURES = []\r\nCUSTOMER_CONTI_FEATURES = []\r\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\r\n# 0 index for inbundle 1 index for outbundled  2 index for total\r\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\r\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\r\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\r\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\r\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\r\nCUSTOMER_NEEDED_COLUMN = [\r\n'onnet_revenue',  'onnet_usage', 'onnet_voice_count', \r\n'offnet_revenue', 'offnet_usage', 'offnet_voice_count',  \r\n'idd_revenue', 'idd_usage', 'idd_voice_count',\r\n'voice_rmg_revenue', 'voice_rmg_usage', 'voice_rmg_count', \r\n'data_rmg_revenue',  'data_rmg_usage',  \r\n'data_revenue', 'data_usage', 'sms_revenue', 'sms_usage',  'sms_idd_revenue', \r\n'sms_idd_usage', 'magik_voice_amount', 'rbt_subscription_rev', 'emergency_credit_rev', \r\n'package_revenue', 'voice_rev', 'sms_rev', 'onn_rev', 'off_rev', 'total_data_rev', \r\n'vas_rev', 'vas_rev_others', 'total_revenue', 'total_voice_count', 'total_voice_duration', \r\n'total_mainaccount_data_usage', 'total_sms_count', 'total_package_count', 'total_other_vas_count', \r\n'total_voice_usage', 'total_data_usage', 'total_sms_usage'\r\n]\r\n\r\n\r\n\r\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\r\n\r\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\r\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\r\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\r\n\r\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\r\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\r\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\r\n\r\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\r\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\r\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\r\n\r\nLABEL1 = 'downtrend'\r\nLABEL2 = 'uptrend'\r\nLABEL3 = 'zigzag'\r\nLABEL4 = 'flat'\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_features': self.TRANSACTION_FEATURES,\r\n        'usage_features': self.CUSTOMER_FEATURES,\r\n        'pack_info_features': self.ALL_PACK_FEATURES,\r\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\r\n        'usage_dtypes': self.CUSTOMER_DTYPES\r\n\r\n    }\r\n\r\n\r\nimport logging\r\nimport os\r\n\r\ndef get_file_names():\r\n    return {\r\n        \"usage\": {\r\n            \"m1\": \"usage_jan.csv\",\r\n            \"m2\": \"usage_feb.csv\",\r\n            \"m3\": \"usage_march.csv\"},\r\n\r\n        \"recharge\": {\r\n            \"m1\": \"recharge_july_20230411084648.csv\",\r\n            \"m2\": \"recharge_june_20230411084648.csv\",\r\n            \"m3\": \"recharge_may_20230411084648.csv\"\r\n        },\r\n        \"purchase\": {\r\n            \"m1\": \"purchase_july_20230411090513.csv\",\r\n            \"m2\": \"purchase_june_20230411090513.csv\",\r\n            \"m3\": \"purchase_may_20230411090513.csv\"\r\n        },\r\n         \"daily_summerized\": {\r\n            \"m1\": \"daily_summarized_july_20230517122646.csv\",\r\n            \"m2\": \"daily_summarized_june_20230517122646.csv\",\r\n            \"m3\": \"daily_summarized_may_20230517122646.csv\",  \r\n          \r\n        },\r\n        \"weekwise\": \r\n        {\r\n          \r\n           \"m1\": \"weekwise_july_20230517180929.csv\",\r\n           \"m2\": \"weekwise_june_20230517180929.csv\",\r\n           \"m3\": \"weekwise_may_20230517180929.csv\", \r\n          \r\n        },\r\n        \"weekly_daily\": \r\n        {\r\n          \r\n            \"m1\": \"daily_weekly_avg_july_20230517163429.csv\",\r\n            \"m2\": \"daily_weekly_avg_june_20230517163429.csv\",\r\n            \"m3\": \"daily_weekly_avg_may_20230517163429.csv\" ,  \r\n          \r\n        },\r\n        \"rfm_purchase\": \r\n        {\r\n  \r\n            \"p1\": \"purchase_march_20230411090513.csv\",\r\n            \"p2\": \"purchase_april_20230411090513.csv\",\r\n            \"p3\": \"purchase_may_20230411090513.csv\",\r\n            \"p4\": \"purchase_june_20230411090513.csv\",\r\n            \"p5\": \"purchase_july_20230411090513.csv\"\r\n  \r\n        },\r\n         \"campaign_data\": \r\n        {\r\n            \"c3\": \"campaign_detailed_fct_july_20230517092908.csv\",\r\n            \"c4\": \"campaign_detailed_fct_june_20230517092908.csv\" ,            \r\n                      \r\n        },\r\n      \r\n        \"pack\":\r\n        {\r\n            \"pack\":\"pack_info.csv\"\r\n        },\r\n        \"profile\":\"profile_july_20230801055632.csv\"\r\n      \r\n    }\r\n\r\n\r\n\r\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\r\nsegement_names = ['trend', 'rfm']\r\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\r\npurchase_location = '/home/tnmops/seahorse3_bkp/'\r\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\r\netl_location = '/home/tnmops/seahorse3_bkp/'\r\npack_info_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location = '/home/tnmops/seahorse3_bkp/'\r\nlog_file = '/home/tnmops/seahorse3_bkp/'\r\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\r\n\r\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\r\n\r\nusage_no_months = ['m1', 'm2', 'm3']\r\nrecharge_no_months = ['m1', 'm2', 'm3']\r\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\r\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\nthreshold = 0.50\r\nnot_needed_rfm_segment = ['Lost']\r\n\r\npack_cols = ['product_id','product_type']\r\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\r\n\r\ncampaign_df= ['c4','c3']\r\ncampaign_usage= ['m4','m3']\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_location': self.purchase_location,\r\n        'usage_location': self.etl_location,\r\n        'pack_info_location': self.pack_info_location,\r\n        'ml_location': self.ml_location,\r\n        'recharge_location':self.recharge_location\r\n    }\r\n\r\nmsisdn_name = MSISDN_COL_NAME\r\n    \r\n\r\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\r\n\r\nengine = create_engine(\r\n    SQLALCHEMY_DATABASE_URL,\r\n)\r\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\r\n\r\nBase = declarative_base()\r\n\r\ndef get_db():\r\n    db = SessionLocal()\r\n    try:\r\n        return db\r\n    finally:\r\n        db.close()\r\n\r\nclass SegmentInformation(Base):\r\n    __tablename__ = \"APA_segment_information_new\"\r\n    id = Column(Integer, primary_key=True, index=True)\r\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\r\n    end_date = Column(DateTime)\r\n    current_product = Column(String(80), nullable=True, unique=False)\r\n    current_products_names = Column(String(200), nullable=True, unique=False)\r\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\r\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\r\n    predicted_arpu = Column(Integer, nullable=True)\r\n    current_arpu = Column(Integer, nullable=True)\r\n    segment_length = Column(String(80), nullable=True, unique=False)\r\n    rule = Column(LONGTEXT, nullable=True)\r\n    actual_rule = Column(LONGTEXT, nullable=True)\r\n    uplift_percent = Column(Float(precision=2), nullable=True)\r\n    incremental_revenue = Column(Float(precision=2), nullable=True)\r\n    campaign_type = Column(String(80), nullable=True, unique=False)\r\n    campaign_name = Column(String(80), nullable=True, unique=False)\r\n    action_key = Column(String(80), nullable=True, unique=False)\r\n    robox_id = Column(String(80), nullable=True, unique=False)\r\n    dag_run_id = Column(String(80), nullable=True, unique=False)\r\n    samples = Column(Integer, nullable=False)\r\n    segment_name = Column(String(80), nullable=True, unique=False)\r\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\r\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\r\n    customer_status = Column(String(80), nullable=True, unique=False)\r\n    query = Column(LONGTEXT, nullable=True, unique=False)\r\n    cluster_no = Column(Integer, nullable=True)\r\n    confidence = Column(Float(precision=2), nullable=True)\r\n    recommendation_type = Column(String(80), nullable=True, unique=False)\r\n    cluster_description = Column(LONGTEXT, nullable=True)\r\n    actual_target_count = Column(Integer, nullable=True)\r\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\r\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\r\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\r\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\r\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\r\n    total_revenue= Column(Integer, nullable=True)\r\n    uplift_revenue=Column(Integer, nullable=True)\r\n\r\n    def __repr__(self):\r\n        return 'SegmentInformation(name=%s)' % self.name\r\n\r\n\r\n\r\nclass SegementInfo(BaseModel):\r\n    end_date: Optional[str] = None\r\n    dag_run_id: Optional[str] = None\r\n    current_product: Optional[str] = None\r\n    current_products_names: Optional[str] = None\r\n    recommended_product_id: Optional[str] = None\r\n    recommended_product_name: Optional[str] = None\r\n    predicted_arpu: Optional[int] = None\r\n    current_arpu: Optional[int] = None\r\n    segment_length: Optional[str] = None\r\n    rule: Optional[str] = None\r\n    actual_rule: Optional[str] = None\r\n    uplift_percent: Optional[float] = None\r\n    incremental_revenue: Optional[float] = None\r\n    campaign_type: Optional[str] = None\r\n    campaign_name: Optional[str] = None\r\n    action_key: Optional[str] = None\r\n    robox_id: Optional[str] = None\r\n    samples: Optional[int] = None\r\n    segment_name: Optional[str] = None\r\n    current_ARPU_band: Optional[str] = None\r\n    current_revenue_impact: Optional[str] = None\r\n    customer_status: Optional[str] = None\r\n    query: Optional[str] = None\r\n    cluster_no: Optional[int] = None\r\n    confidence: Optional[float] = None\r\n    recommendation_type: Optional[str] = None\r\n    cluster_description: Optional[str] = None\r\n    actual_target_count: Optional[str] = None\r\n    top_purchased_day_1: Optional[str] = None\r\n    top_purchased_day_2: Optional[str] = None\r\n    top_purchased_day_3: Optional[str] = None\r\n    next_purchase_date_range: Optional[str] = None\r\n    campaign_response_percentage: Optional[str] = None\r\n    total_revenue: Optional[int] = None\r\n    uplift_revenue: Optional[int] = None\r\n\r\n\r\nclass SegementRepo:\r\n    def create(db: Session, segement: SegementInfo):\r\n        db_item = SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\r\n                                            campaign_type=segement.campaign_type,\r\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\r\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\r\n                                            rule=segement.rule, samples=segement.samples,\r\n                                            campaign_name=segement.campaign_name,\r\n                                            recommended_product_id=segement.recommended_product_id,\r\n                                            recommended_product_name=segement.recommended_product_name,\r\n                                            current_product=segement.current_product,\r\n                                            current_products_names=segement.current_products_names,\r\n                                            segment_length=segement.segment_length,\r\n                                            current_ARPU_band=segement.current_ARPU_band,\r\n                                            current_revenue_impact=segement.current_revenue_impact,\r\n                                            customer_status=segement.customer_status,\r\n                                            segment_name=segement.segment_name, query=segement.query,\r\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\r\n                                            recommendation_type=segement.recommendation_type,\r\n                                            cluster_description=segement.cluster_description,\r\n                                            actual_target_count=segement.actual_target_count,\r\n                                            top_purchased_day_1=segement.top_purchased_day_1,\r\n                                            top_purchased_day_2=segement.top_purchased_day_2,\r\n                                            top_purchased_day_3=segement.top_purchased_day_3,\r\n                                            next_purchase_date_range=segement.next_purchase_date_range,\r\n                                            campaign_response_percentage=segement.campaign_response_percentage,\r\n                                            total_revenue=segement.total_revenue,\r\n                                            uplift_revenue=segement.uplift_revenue,\r\n                                            )\r\n        db.add(db_item)\r\n        db.commit()\r\n        db.refresh(db_item)\r\n        return db_item\r\n\r\n    def findByAutoPilotIdAndClusterNo(db: Session, _id, cluster_no):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_no) \\\r\n            .all()\r\n\r\n    def findByAutoPilotIdAndSegementName(db: Session, _id, segement_name, cluster_number):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_number).first()\r\n\r\n    def findByAutoPilotIdAndSegementNameAll(db: Session, _id, segement_name, cluster_number):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_number).all()\r\n    \r\n    def findByAutoPilotIdAndSegementNamewithoutcluster(db: Session, _id, segement_name):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name).all()\r\n            \r\n\r\n\r\n    def findByAutoPilotId(db: Session, _id):\r\n        return db.query(SegmentInformation).filter(SegmentInformation.dag_run_id == _id).all()\r\n\r\n    def findByAutoPilotIdRecommendedId(db: Session, _id,recommended_product_id):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.recommended_product_id == recommended_product_id).all()\r\n\r\n    def deleteById(db: Session, _ids):\r\n        for id in _ids:\r\n            db.query(SegmentInformation).filter(SegmentInformation.id == id).delete()\r\n            db.commit()\r\n\r\n    def update(db: Session, item_data):\r\n        updated_item = db.merge(item_data)\r\n        db.commit()\r\n        return updated_item\r\n\r\ndef form_data(p2, df, anti_conci):\r\n    try:\r\n        product_id = PACK_INFO_PACK_COLUMN_NAME\r\n        purchase = p2[p2[product_id].isin(anti_conci)]\r\n        pgp = purchase.copy()\r\n        # pgp = purchase.groupby(['msisdn', 'product_id']).agg({f.Features.PA: \"sum\"}).reset_index()\r\n\r\n        anti_df = pgp[pgp[product_id].isin(anti_conci[:-1])]\r\n        conci_df = pgp[pgp[product_id] == anti_conci[-1]]\r\n\r\n        anti_df_msisdn = anti_df[~anti_df['msisdn'].isin(conci_df['msisdn'].values)]['msisdn'].unique()\r\n        conci_df_msisdn = conci_df[~conci_df['msisdn'].isin(anti_df['msisdn'].values)]['msisdn'].unique()\r\n        anti_data = df[df['msisdn'].isin(anti_df_msisdn)]\r\n        conci_data = df[df['msisdn'].isin(conci_df_msisdn)]\r\n        print(f\"the length of anti data ios {len(anti_data)} and unique is {anti_data['msisdn'].nunique()}\")\r\n        print(f\"the length of conci data ios {len(conci_data)} and unique is {conci_data['msisdn'].nunique()}\")\r\n        anti_data['label'] = 1\r\n        conci_data['label'] = 0\r\n        data = pd.concat([anti_data, conci_data], axis=0)\r\n        print(\"label counts\", data['label'].value_counts())\r\n        return data\r\n    except Exception as e:\r\n        print(\"the error occoured in form_data\", e)\r\n        raise ValueError(e)\r\n\r\n\r\n\r\nclass DecisionTreeConverter(object):\r\n\r\n    def __init__(self, my_tree=None, features=None, class_names=None, df=None):\r\n        self.my_tree = my_tree\r\n        self.features = features\r\n        self.class_names = class_names\r\n        self.df = df\r\n        self.json = None\r\n\r\n        self.json_string = \"\"\r\n\r\n        # self.recursion(self.my_tree.tree_, 0)\r\n\r\n        self.recurse(self.my_tree.tree_, 0)\r\n\r\n    def node_to_str(self, tree, node_id, criterion):\r\n        if True:\r\n            criterion = \"impurity\"\r\n\r\n        value = tree.value[node_id]\r\n        if tree.n_outputs == 1:\r\n            value = value[0, :]\r\n\r\n        jsonValue = ', '.join([str(x) for x in value])\r\n\r\n        if tree.children_left[node_id] == sklearn.tree._tree.TREE_LEAF:\r\n            l = 1\r\n            try:\r\n\r\n                probablity = np.round(100.0 * value[l] / np.sum(value), 2)\r\n            except:\r\n                probablity = np.round(100.0 * value[0] / np.sum(value), 2)\r\n            return '\"id\": \"%s\", \"criterion\": \"%s\", \"impurity\": \"%s\", \"samples\": \"%s\",\"recomentedPackProbablity\":%s, ' \\\r\n                   '\"value\": [%s]' \\\r\n                   % (node_id,\r\n                      criterion,\r\n                      tree.impurity[node_id],\r\n                      tree.n_node_samples[node_id],\r\n                      probablity,\r\n                      jsonValue)\r\n        else:\r\n\r\n            if self.features is not None:\r\n                feature = self.features[tree.feature[node_id]]\r\n            else:\r\n                feature = tree.feature[node_id]\r\n\r\n            if \"=\" in feature:\r\n                ruleType = \"=\"\r\n                ruleValue = \"false\"\r\n            else:\r\n                ruleType = \"<=\"\r\n                ruleValue = \"%.2f\" % tree.threshold[node_id]\r\n\r\n            return '\"id\": \"%s\", \"rule\": \"%s %s %s\", \"%s\": \"%s\", \"samples\": \"%s\"' \\\r\n                   % (node_id,\r\n                      feature,\r\n                      ruleType,\r\n                      ruleValue,\r\n                      criterion,\r\n                      tree.impurity[node_id],\r\n                      tree.n_node_samples[node_id])\r\n\r\n    def recurse(self, tree, node_id, criterion='impurity', parent=None, depth=0):\r\n        tabs = \"  \" * depth\r\n        self.json_string = \"\"\r\n\r\n        left_child = tree.children_left[node_id]\r\n        right_child = tree.children_right[node_id]\r\n\r\n        self.json_string = self.json_string + \"\\n\" + \\\r\n                           tabs + \"{\\n\" + \\\r\n                           tabs + \"  \" + self.node_to_str(tree, node_id, criterion)\r\n\r\n        if left_child != sklearn.tree._tree.TREE_LEAF:\r\n            self.json_string = self.json_string + \",\\n\" + \\\r\n                               tabs + '  \"left\": ' + \\\r\n                               self.recurse(tree, left_child, criterion=criterion, parent=node_id,\r\n                                            depth=depth + 1) + \",\\n\" + \\\r\n                               tabs + '  \"right\": ' + \\\r\n                               self.recurse(tree,\r\n                                            right_child,\r\n                                            criterion=criterion,\r\n                                            parent=node_id,\r\n                                            depth=depth + 1)\r\n\r\n        self.json_string = self.json_string + tabs + \"\\n\" + \\\r\n                           tabs + \"}\"\r\n\r\n        return self.json_string\r\n\r\n    def recursion(self, tree, node_id, parent=None, depth=0, location=None):\r\n        tabs = \" \" * depth\r\n        self.json = \"\"\r\n        left_child = tree.children_left[node_id]\r\n        right_child = tree.children_right[node_id]\r\n        self.json = self.json + tabs + \"{\" + tabs + \" \" + self.get_node_to_string(tree, node_id, location)\r\n        print(f\"the json got in recursion is {self.json}\")\r\n        if left_child != sklearn.tree._tree.TREE_LEAF:\r\n            self.json = self.json + \",\" + tabs + '\"left\": ' + self.recursion(tree, left_child, node_id, depth + 1,\r\n                                                                             \"left\") + \",\" + \\\r\n                        tabs + '\"right\": ' + self.recursion(tree, right_child, node_id, depth + 1, \"right\")\r\n            print(\"json formed inside resursion when not tree leaf  \", self.json)\r\n        self.json = self.json + tabs + tabs + \"}\"\r\n        print(\"json formed inside resursion when  tree leaf  \", self.json)\r\n        return self.json\r\n\r\n    def get_node_to_string(self, tree, node_id, location):\r\n        value = tree.value[node_id]\r\n        print(f\"the value of the node  {node_id}  is -- {value}\")\r\n        if tree.n_outputs == 1:\r\n            value = value[0, :]\r\n            print(f\" the type of value is {type(value)}\")\r\n        json_val = \", \".join([str(x) for x in value])\r\n\r\n        if tree.children_left[node_id] == sklearn.tree._tree.TREE_LEAF:\r\n            # l = np.argmax(value)\r\n            l = 1\r\n            try:\r\n\r\n                probablity = np.round(100.0 * value[l] / np.sum(value), 2)\r\n            except:\r\n                probablity = np.round(100.0 * value[0] / np.sum(value), 2)\r\n\r\n            print(f\"the left child is a leaf node \")\r\n            return_val = f' \"id\": {node_id}, \"class\":\"{self.class_names[l]}\" ,\"samples\":{tree.n_node_samples[node_id]} , \"recomentedPackProbablity\": {probablity}'\r\n            print(f\"the return value of convert tree to json when left child is a leaf node is {return_val}\")\r\n            return return_val\r\n\r\n        else:\r\n            if self.features is not None:\r\n                print(f\" the features are  {self.features} ,   the node is is {node_id}\")\r\n                print(location)\r\n\r\n                feature = self.features[tree.feature[node_id]]\r\n            else:\r\n                print(\"no features list given \")\r\n\r\n            rule_val = \"%.2f\" % tree.threshold[node_id]\r\n\r\n            if location is not None and location == 'left':\r\n                minimum_value = self.df[feature].min()\r\n                if minimum_value == 0:\r\n                    operator = f\"<= {rule_val}\"\r\n                else:\r\n                    operator = f'between {round(float(minimum_value), 2)} and {round(float(rule_val), 2)} '\r\n\r\n            else:\r\n\r\n                maximum_value = self.df[feature].max()\r\n                operator = f'between {round(float(rule_val), 2)} and {round(float(maximum_value), 2)} '\r\n\r\n            # no need of samples and values\r\n\r\n            return_val = f' \"id\":{node_id}, \"rule\": \"{feature} {operator} \" '\r\n            print(f\"the return value of convert tree to json when left child is a  not leaf node is {return_val}\")\r\n            return return_val\r\n\r\n    def get_json(self):\r\n        if self.json_string is not None:\r\n            return self.json_string\r\n\r\n\r\n\r\ndef replace_rule(rule_condition, columns_for_dummies):\r\n    try:\r\n        if rule_condition is None:\r\n            return rule_condition\r\n        name = rule_condition.split(\"<=\")[0]\r\n        if name is None or len(name) == 0:\r\n            return rule_condition\r\n\r\n        split_name = name.split(\":\")\r\n        key = split_name[0]\r\n        if key not in columns_for_dummies:\r\n            return rule_condition\r\n        value = split_name[1]\r\n        value = value.strip()\r\n        rule_m = f\"{key} == '{round(float(value), 2)}' \"\r\n        return rule_m\r\n\r\n    except Exception as e:\r\n        print(\"the error in replace rule is \", e)\r\n        return rule_condition\r\n\r\n\r\ndef pruneTree(root, columns_for_dummies):\r\n    if root.get('recomentedPackProbablity') is not None and root.get('recomentedPackProbablity') < 50:\r\n        return None\r\n\r\n    if root.get('left') is not None:\r\n        root['rule'] = replace_rule(root.get('rule'), columns_for_dummies)\r\n        root['left'] = pruneTree(root.get('left'), columns_for_dummies)\r\n    if root.get('right') is not None:\r\n        root['rule'] = replace_rule(root.get('rule'), columns_for_dummies)\r\n        root['right'] = pruneTree(root.get('right'), columns_for_dummies)\r\n    if root.get('left') is not None or root.get('right') is not None or root.get(\r\n            'recomentedPackProbablity') is not None and root.get('recomentedPackProbablity') >= 50:\r\n        # samples =samples +int(root.get('samples'))\r\n        return root\r\n    else:\r\n        return None\r\n\r\n\r\ndef generate_boundries(features, data_json, tree):\r\n    def add_conditions(feature, count):\r\n        rule_json = {}\r\n        rule_json['id'] = -1\r\n\r\n        min_max = data_json.get(feature)\r\n        if min_max.get('min') == 0:\r\n            rule = f\"{feature} <= {round(float(min_max.get('max')), 2)}\"\r\n\r\n\r\n        else:\r\n            rule = f\"{feature} between {round(float(min_max.get('min')), 2)} and {round(float(min_max.get('max')), 2)}\"\r\n\r\n        rule_json['rule'] = rule\r\n        if count < len(features) - 1:\r\n            rule_json['left'] = add_conditions(features[count + 1], count + 1)\r\n        else:\r\n            rule_json['left'] = tree\r\n        return rule_json\r\n\r\n    try:\r\n        print(\"inside generate_boundries\")\r\n        return add_conditions(features[0], 0)\r\n\r\n    except Exception as e:\r\n        print(f\"error occoured in generating boundries {e}\")\r\n\r\n    \r\n    \r\n    \r\ndef negate(rule):\r\n    mapper = ((\"<=\", \">\"), (\">=\", \"<\"), (\">\", \"<=\"), (\"<\", \">=\"), (\"==\", '!='))\r\n    for operator, negated in mapper:\r\n        if operator in rule:\r\n            return rule.replace(operator, negated)\r\n    return \"not (\" + rule + \")\"  # Default when operator not recognised\r\n\r\n\r\ndef nodeToLeafPaths(node):\r\n    if not node:  # base case: nothing in this direction\r\n        return\r\n    rule = node.get('rule')\r\n    if rule is None:  # base case: a leaf with a value\r\n        # value_list.append(node.get('value'))\r\n        yield [], int(node.get('samples'))  # empty path\r\n        return\r\n\r\n    negated_rule = negate(rule)\r\n    for path, value in nodeToLeafPaths(node.get('left')):\r\n        # print(f\"the left path {path} and value {value} \")\r\n        if 'between' in rule:\r\n            between_split = rule.split('between')\r\n            key = between_split[0].strip()\r\n            value_temp = between_split[1].strip()\r\n            ant_split = value_temp.split(\"and\")\r\n            value1 = ant_split[0].strip()\r\n            value2 = ant_split[1].strip()\r\n\r\n            rule = f\"{key} > {round(float(value1), 2)} and {key} <= {round(float(value2), 2)}\"\r\n\r\n        yield [rule, *path], value  # Extend path with current rule\r\n    for path, value in nodeToLeafPaths(node.get('right')):\r\n        yield [negated_rule, *path], value\r\n\r\n\r\ndef rootToLeafConjugations(root):\r\n    print('inside rootToLeafConjugations ')\r\n    final_dic = {}\r\n    for path, value in nodeToLeafPaths(root):\r\n        final_dic[\" and \".join(path)] = value\r\n        # print({\" AND \".join(path):value  for path ,value in nodeToLeafPaths(root) })\r\n    return final_dic\r\n\r\n\r\ndef get_prod_diff(product_id_1, product_id_2, packinfo_df):\r\n    print('packinfo_df.columns', packinfo_df.columns)\r\n    diff = 0\r\n    try:\r\n        if isinstance(product_id_1, str):\r\n            split = product_id_1.split(\"|\")\r\n            print('split is ', split)\r\n            split_length = len(split)\r\n            if split_length > 1:\r\n                max = 0\r\n                for i in range(split_length):\r\n                    product = int(float(split[i]))\r\n                    print('product is ', product)\r\n                    price = packinfo_df[packinfo_df['product_id'] == product].iloc[0][\"price\"]\r\n                    print('price is', price)\r\n                    if price > max:\r\n                        max = price\r\n\r\n                product_id_1 = packinfo_df[packinfo_df['price'] == max].iloc[0][\"product_id\"]\r\n                print('product_id_1 is', product_id_1)\r\n\r\n            else:\r\n                product_id_1 = int(split[0])\r\n\r\n        if packinfo_df is not None:\r\n            print('product_id_1 is ', product_id_1)\r\n            print('product_id_2 is ', product_id_2)\r\n            #print(\"packinfo_df['product_id'].values\", packinfo_df['product_id'].values)\r\n            if product_id_1 in packinfo_df['product_id'].values:\r\n                print('product_id_1 ture')\r\n\r\n            print('packinfo_df.columns', packinfo_df.columns)\r\n            if product_id_2 in packinfo_df['product_id'].values:\r\n                print('product_id_2 ture')\r\n\r\n            else:\r\n                print('product_id_2 fasle')\r\n\r\n            if ((product_id_1 in packinfo_df['product_id'].values) and (\r\n                    product_id_2 in packinfo_df['product_id'].values)):\r\n\r\n                print('going to calcualte the diff ')\r\n\r\n                diff = abs(int(float(packinfo_df[packinfo_df['product_id'] == product_id_1].iloc[0][\"price\"]) -\r\n                               float(packinfo_df[packinfo_df['product_id'] == product_id_2].iloc[0][\"price\"])))\r\n\r\n                print(\"the difference is \", diff)\r\n            else:\r\n                print(\"product_id is not in dataframe  \")\r\n        else:\r\n            print(\"some problem with the  dataframe  \")\r\n\r\n    except Exception as e:\r\n        print(\"error occurred in get_prod_diff \", e)\r\n\r\n    return diff\r\n\r\n\r\n\r\ndef make_request(body):\r\n    rule_main = None\r\n    try:\r\n        url = rule_converter_url\r\n        x = requests.post(url, json=body)\r\n        response_json = x.json()\r\n        if response_json.get('respCode') is None or response_json.get('respCode') != 'SUCCESS':\r\n            raise ValueError(\"rule engine gave error respose \" + x.text)\r\n\r\n        return json.dumps(response_json.get('guiRequest'))\r\n\r\n\r\n    except Exception as e:\r\n        print(\"error occoured in http requerst\", e)\r\n        raise RuntimeError(e)\r\n\r\n\r\ndef add_clusters_rules(features, features_val, tree):\r\n    def add_conditions(feature, count):\r\n        rule_json = {}\r\n        rule_json['id'] = -1\r\n\r\n        rule = f\"{features[count]} == {features_val[count]}\"\r\n        rule_json['rule'] = rule\r\n        if count < len(features) - 1:\r\n            rule_json['left'] = add_conditions(features[count + 1], count + 1)\r\n        else:\r\n            rule_json['left'] = tree\r\n        return rule_json\r\n\r\n    try:\r\n        print(\"inside generate_boundries\")\r\n        return add_conditions(features[0], 0)\r\n\r\n    except Exception as e:\r\n        print(f\"error occoured in generating boundries {e}\")\r\n\r\n\r\ndef GetCurrentPackprice(product_id, packinfo_df):\r\n    price = 0\r\n    \r\n    try:\r\n        print('product_id in GetCurrentPackprice is ',product_id)\r\n        print('type of product_id in GetCurrentPackprice is ',type(product_id))\r\n        if (packinfo_df is not None):\r\n            if isinstance(product_id, str):\r\n                split = product_id.split(\"|\")\r\n                \r\n                print('split in GetCurrentPackprice is ', split)\r\n                split_length = len(split)\r\n                if split_length > 1:\r\n                    highest = 0\r\n                    for i in range(split_length):\r\n                        product = int(float(split[i]))\r\n                        print('product is ', product)\r\n                        price = packinfo_df[packinfo_df['product_id'] == product].iloc[0][\"price\"]\r\n                        print('price is', price)\r\n                        if price > highest:\r\n                            highest = price\r\n                \r\n                    print(\"the price  of \", product_id, \" is \", price)\r\n                \r\n                else:\r\n                    \r\n                    product_id = int(split[0])\r\n                    \r\n                    price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\r\n                    highest = price\r\n            else:\r\n                print(\"product_id is not in dataframe  \")\r\n        else:\r\n            print(\"some problem with the  dataframe  \")\r\n\r\n    except Exception as e:\r\n        print(\"error occurred in getpackprice \", e)\r\n\r\n    return highest\r\n    \r\n    \r\ndef getpackprice(product_id, packinfo_df):\r\n    price = 0\r\n    try:\r\n        if (packinfo_df is not None):\r\n            if (product_id in packinfo_df['product_id'].values):\r\n\r\n                price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\r\n                print(\"the price  of \", product_id, \" is \", price)\r\n            else:\r\n                print(\"product_id is not in dataframe  \")\r\n        else:\r\n            print(\"some problem with the  dataframe  \")\r\n\r\n    except Exception as e:\r\n        print(\"error occurred in getpackprice \", e)\r\n\r\n    return price\r\n    \r\ndef train_model(data):\r\n    def run_decision_tree(X_train, X_test, y_train, y_test):\r\n        clf = DecisionTreeClassifier(max_leaf_nodes=12)\r\n        clf.fit(X_train, y_train)\r\n        y_pred = clf.predict(X_test)\r\n        print('Accuracy run_decision_tree: ', accuracy_score(y_test, y_pred))\r\n        #print(\"confusion matrix run_decision_tree\", confusion_matrix(y_test, y_pred))\r\n        print(classification_report(y_test, y_pred))\r\n        return clf\r\n\r\n    X = data.drop(['msisdn', 'label'], axis=1)\r\n    y = data['label']\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n    print(f\"X_train.shape = {X_train.shape}, X_test.shape =  {X_test.shape}\")\r\n    # sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\r\n    sel = SelectFromModel(DecisionTreeClassifier(max_leaf_nodes=12))\r\n    sel.fit(X_train, y_train)\r\n    sel.get_support()\r\n    features = X_train.columns[sel.get_support()]\r\n    print(\"the feature selection features are \", features)\r\n    X_train_rfc = sel.transform(X_train)\r\n    X_test_rfc = sel.transform(X_test)\r\n    clf1 = run_decision_tree(X_train_rfc, X_test_rfc, y_train, y_test)\r\n    features_importance = list(clf1.feature_importances_)\r\n    dic = {features[i]: features_importance[i] for i in range(len(features))}\r\n    t = None\r\n    try:\r\n        t = pd.read_csv('temp.csv')\r\n        tt = t.append(dic, ignore_index=True)\r\n    except:\r\n        t = pd.DataFrame(columns=list(X_train.columns))\r\n        tt = t.append(dic, ignore_index=True)\r\n\r\n    print(tt.head(5))\r\n    tt.to_csv('temp.csv', header=True, index=False)\r\n    return clf1, features\r\n\r\n\r\n\r\n\r\n\r\n\r\nclass RuleGenerator(object):\r\n    def __init__(self, df, dag_run_id, cluster_name, cluster_number, purchase_filtered, pack_info):\r\n\r\n        self.df = df\r\n        self.dag_run_id = dag_run_id\r\n        self.cluster_name = cluster_name\r\n        self.cluster_name = cluster_number\r\n        self.purchase = purchase_filtered\r\n        self.usage_filter2 = None\r\n        self.pack_info = pack_info\r\n        self.op = None\r\n        pass\r\n\r\n    def generate_rules(self, segementss, usage):\r\n        segment_list = []\r\n        dftnm_ls = []\r\n        data_dict_query = {}\r\n        \r\n        # print(type(segementss))\r\n        # print(segements)\r\n        for segements in segementss:\r\n            if segements.recommended_product_id is None:\r\n                continue\r\n            usage_filter1 = usage[usage['msisdn'].isin(self.df['msisdn'])]\r\n            # usage_filter1 = usage_filter1.compute()\r\n            conci = int(float(segements.recommended_product_id))\r\n            anti = [int(float(x)) for x in segements.current_product.split(\"|\")]\r\n            anti.append(conci)\r\n            df = form_data(p2=self.purchase, df=usage_filter1, anti_conci=anti)\r\n\r\n            print(\"df columns ==>\",df.columns)\r\n            self.usage_filter2 = usage_filter1.merge(self.df[['msisdn', 'trend']], on='msisdn', how='inner')\r\n            value_counts = df['label'].value_counts()\r\n            if 0 in value_counts.index and 1 in value_counts.index:\r\n                if value_counts.loc[0] >= 50 and value_counts.loc[1] >= 50:\r\n                    print(\"Both values have at least two entries\")\r\n                    # path_temp = os.path.join(cfg.Config.ml_location, self.dag_run_id, \"model\")\r\n                    # Path(path_temp).mkdir(parents=True, exist_ok=True)\r\n                    # df.to_csv(os.path.join(path_temp, segements.segment_name + \".csv\"), header=True, index=False)\r\n                    # ic(\"outputerrrrddddd\")\r\n                else:\r\n                    print(\"At least one value doesn't have two entries\")\r\n                    continue\r\n            else:\r\n                print(\"Both values are not present in the column\")\r\n                continue\r\n\r\n            clf1, features = train_model(df)\r\n           \r\n            print('features -------------------', features)\r\n\r\n            \r\n            # for BTC purpose end\r\n            \r\n            # features = list(features)\r\n            # print('features is ', features)\r\n            # print('type features is ', type(features))\r\n            # features.append('m1_total_revenue')\r\n            # features = list(set(features))\r\n            decision_tree_obj = DecisionTreeConverter(clf1, features, ['differentpack', 'whatsappPack'],\r\n                                                          df[df['label'] == 1])\r\n            treetojson = decision_tree_obj.get_json()\r\n            #print(\"tree fromed\", treetojson)\r\n            prune_tree = pruneTree(root=json.loads(treetojson), columns_for_dummies=[])\r\n            df_temp = df[df['label'] == 1]\r\n            df1 = df_temp.agg(['min', 'max'])\r\n            df_json = json.loads(df1.to_json())\r\n            prune_tree = generate_boundries(features, df_json, prune_tree)\r\n            segement_names = ['trend', 'rfm']\r\n            segement_values = segements.segment_name.split(\"-\")[:-1]\r\n            prune_tree1 = add_clusters_rules(segement_names, segement_values, prune_tree)\r\n\r\n            final = rootToLeafConjugations(prune_tree)\r\n            print('after  rootToLeafConjugations ')\r\n            samples = sum(final.values())\r\n            rule = \" or \".join(list(final.keys()))\r\n\r\n            temp = self.usage_filter2.query(rule)\r\n\r\n            # for BTC purpose\r\n            dftnm = temp.copy()\r\n            msisdns = dftnm.pop(\"msisdn\")\r\n            dftnm = dftnm[features]\r\n            X = dftnm.drop(['msisdn', 'label'], axis=1, errors=\"ignore\")\r\n            # y = dftnm['label']\r\n            predictions = clf1.predict(X)\r\n            prediction_probabilities = clf1.predict_proba(X)\r\n            dftnm['predictions'] = predictions\r\n            dftnm['predict_proba_0'] = prediction_probabilities[:, 0]  # Probability of class 0\r\n            dftnm['predict_proba_1'] = prediction_probabilities[:, 1]  # Probability of class 1\r\n            dftnm['msisdn'] = msisdns\r\n            dftnm[\"segement_id\"] = segements.id\r\n            dftnm[\"recommended_product_id\"] = segements.recommended_product_id\r\n            segement_names = segement_names\r\n            segement_values = segements.segment_name.split(\"-\")[:-1]\r\n            dftnm[segement_names[0]] = segement_values[0]\r\n            dftnm[segement_names[1]] = segement_values[1]\r\n            dftnm[\"segement_name\"] = segements.segment_name\r\n            dftnm = dftnm[\r\n                [\"msisdn\",\"segement_id\",\"segement_name\", segement_names[0], segement_names[1], \"predictions\", \"predict_proba_0\", \"predict_proba_1\",\"recommended_product_id\"]]\r\n            # path_op = os.path.join(cfg.Config.ml_location, self.dag_run_id, \"output_data\")\r\n            # Path(path_op).mkdir(parents=True, exist_ok=True)\r\n            # dftnm.to_csv(os.path.join(path_op, segements.segment_name + \".csv\"), header=True, index=False)\r\n            dftnm_ls.append(dftnm)\r\n            dftnm_final = pd.concat(dftnm_ls)\r\n            self.op = dftnm_final\r\n\r\n\r\n            # segements.rule = json.dumps(prune_tree1)\r\n\r\n            # segements.rule = make_request(prune_tree1)\r\n            segements.rule = \"testing\"\r\n            segements.query = rule\r\n            segements.actual_target_count = len(dftnm)\r\n            \r\n            #print('rule is', rule)\r\n            print('going to query')\r\n            #print('columns of self.usage_filter2 is ', self.usage_filter2.columns)\r\n            \r\n\r\n            filename = os.path.join(ml_location, self.dag_run_id,  'query_count.pickle')\r\n            if os.path.isfile(filename):\r\n                # Read data from file\r\n                with open(filename, 'rb') as handle:\r\n                    data_dict_query = pickle.load(handle)\r\n\r\n                    \r\n            print('len of temp is ', len(temp))\r\n            segment_name_qr = segements.segment_name\r\n            print('segment_name_qr is ',segment_name_qr)\r\n            print('type of segement_values is', type(segment_name_qr))\r\n\r\n            data_dict_query[segment_name_qr] = len(temp)\r\n\r\n            print('len appended')\r\n           \r\n            with open(filename, 'wb') as handle:\r\n                pickle.dump(data_dict_query, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n\r\n            \r\n\r\n            \r\n            \r\n            if temp is None or len(temp) == 0:\r\n                continue\r\n            print('len of temp is ', len(temp))\r\n\r\n            op_path_dir = os.path.join(ml_location, self.dag_run_id,  'segment_data')\r\n            Path(op_path_dir).mkdir(parents=True, exist_ok=True)\r\n\r\n            print(int(float(segements.recommended_product_id)))\r\n            anti_list = [str(x) for x in segements.current_product.split(\"|\")]\r\n            anti_prod = ''.join(str(e) for e in anti_list)\r\n            print('anti_prod is', anti_prod)\r\n            print('segement_values is',segement_values)\r\n            filename = '_'.join(str(e) for e in segement_values)+'_'+str(int(float(segements.recommended_product_id)))+'_'+anti_prod+'.csv'\r\n            print('filename is ',filename)\r\n\r\n            \r\n            df_sub=df.query(rule)\r\n            \r\n            df_sub.to_csv(os.path.join(op_path_dir,filename), header=True, index=False)\r\n\r\n            segements.samples = samples\r\n\r\n            #segements.current_arpu = int(temp['m1_total_revenue'].mean())\r\n            total_revenue = int(temp['m1_total_revenue'].sum())\r\n            segements.total_revenue=total_revenue\r\n\r\n            current_product_id = segements.current_product\r\n            rec_product_id = int(float(segements.recommended_product_id))\r\n\r\n            incremental_revenue = get_prod_diff(current_product_id, rec_product_id, self.pack_info) * samples\r\n            incremental_revenue = (incremental_revenue / 80) * 100\r\n            initial_sum = temp['m1_total_revenue'].sum()\r\n            #uplift = (incremental_revenue / initial_sum) * 100\r\n            \r\n            current_product_price = GetCurrentPackprice(current_product_id, self.pack_info)\r\n            reco_product_price = getpackprice(rec_product_id,self.pack_info)\r\n            segements.uplift_revenue = reco_product_price-current_product_price\r\n\r\n            prodd_diff= reco_product_price-current_product_price\r\n            # uplift=(prodd_diff/current_product_price) *100\r\n            # segements.uplift = round(uplift, 2)\r\n            segements.incremental_revenue = incremental_revenue\r\n\r\n\r\n            segment_length=str(len(self.df))\r\n            print('segment_length is' ,segment_length)\r\n\r\n            segements.segment_length = segment_length\r\n\r\n\r\n\r\n            #segements.predicted_arpu = int(segements.current_arpu + ((segements.current_arpu*uplift)/100))\r\n\r\n            segment_list.append(segements)\r\n#             print('segment_list============>',segment_list)\r\n        return segment_list\r\n\r\n\r\ndef load_picke_file(filename):\r\n    with open(filename, 'rb') as handle:\r\n        data = pickle.load(handle)\r\n    return data\r\n\r\n\r\n\r\ndef filter_data(temp_df):\r\n    cols = [p + \"_\" + s for s in CUSTOMER_NEEDED_COLUMN for p in usage_no_months]\r\n    cols.append(\"msisdn\")\r\n    temp_df = temp_df[cols]\r\n    return temp_df\r\n\r\n\r\ndef otliner_removal(df, per=0.97):\r\n    try:\r\n        # df = df[\"needed_col\"]\r\n        tot_rev = CUSTOMER_TOTAL_REVENUE[0]\r\n        q = df[tot_rev].quantile(per)\r\n        print(\"the length brfore is\", len(df))\r\n        df = df[df[tot_rev] < q]\r\n        print(\"the length after is\", len(df))\r\n        return df\r\n    except Exception as e:\r\n        print(e)\r\n        raise RuntimeError(e)\r\n        \r\n        \r\n\r\ndef rule_generation(dag_run_id, db):\r\n\r\n\r\n    file_name_dict = get_file_names()\r\n    print(\"finding trend ongoing \")\r\n    data = {}\r\n    for month in usage_no_months:\r\n        data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\r\n                                  dtype=CUSTOMER_DTYPES)\r\n    months =usage_no_months\r\n    temp_df = None\r\n\r\n    for month in months:\r\n        df = data.get(month)\r\n        df = df.fillna(0)\r\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\r\n        # df['tot_rev'] = df[total_revenue]\r\n        df = otliner_removal(df.copy())\r\n        df = df.add_prefix(f\"{month}_\")\r\n        df = df.rename(columns={f\"{month}_msisdn\": \"msisdn\"})\r\n        if temp_df is None:\r\n            temp_df = df\r\n        else:\r\n\r\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\r\n            temp_df = temp_df.fillna(0)\r\n    temp_df = temp_df.compute()\r\n    temp_df = filter_data(temp_df)\r\n    result_dict_path = os.path.join(ml_location,dag_run_id,  \"purchased_for_association\", 'dict.pickle')\r\n    data_path = os.path.join(ml_location,dag_run_id,  \"dict.pickle\")\r\n    data_dict = load_picke_file(data_path)\r\n    pack_info = os.path.join(etl_location, 'pack_info.csv')\r\n    pack_info_df = pd.read_csv(pack_info)\r\n    data = load_picke_file(result_dict_path)\r\n\r\n    # for key, value in data_dict.items():\r\n    #     data_dict[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\r\n\r\n    # for key, value in data.items():\r\n    #     if value is None:\r\n    #         continue\r\n    #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\r\n\r\n    filtered_dict = {k: v for k, v in data.items()}\r\n    filtered_data__dict = {k: v for k, v in data_dict.items()}\r\n\r\n    # btc purpose\r\n    output_d_list = []\r\n    # btc purpose end\r\n\r\n\r\n    for item, val in filtered_dict.items():\r\n        # data = pd.read_csv(filtered_data__dict.get(item))\r\n        # if filtered_data__dict.get(item) is None:\r\n        #     continue\r\n        # else:\r\n        data = pd.read_csv(filtered_data__dict.get(item))\r\n            \r\n            \r\n        try:\r\n            purchase = pd.read_csv(val)\r\n        except:\r\n            purchase_none_segements = SegementRepo.findByAutoPilotIdAndSegementNamewithoutcluster(db=db, _id=dag_run_id,\r\n                                                                        segement_name=segements_name)\r\n            for seg in purchase_none_segements:\r\n                print(f\"the segment name{seg.segment_name} and segment id is {seg.id}\")\r\n                ic(\"deleteing segement if they dont have purchase \")\r\n                SegementRepo.deleteById(db=db, _ids=[seg.id])\r\n            continue\r\n\r\n\r\n        #for 1 month purchase \r\n        #purchase = pd.read_csv('/log/btc_autopilot/etl_files/purchase_feb.csv')\r\n\r\n        #for 1 month purchase\r\n\r\n\r\n        for cluster in data['label'].unique():\r\n            segements_name = f\"{item}-{str(cluster)}\"\r\n            print('segements_name============>',segements_name)\r\n            data_temp = data[data['label'] == cluster]\r\n            #purchase_filtered = purchase[purchase[msisdn_name].isin(data_temp[msisdn_name])]\r\n\r\n            purchase_filtered = purchase[purchase[msisdn_name].isin(data_temp[msisdn_name])]\r\n\r\n            print('len of purchase in association_process ', len(purchase_filtered))\r\n            rg = RuleGenerator(df=data_temp, dag_run_id=dag_run_id, cluster_name=item, cluster_number=cluster,\r\n                               purchase_filtered=purchase_filtered, pack_info=pack_info_df)\r\n            segements = SegementRepo.findByAutoPilotIdAndSegementNameAll(db=db, _id=dag_run_id,\r\n                                                                         segement_name=segements_name,\r\n                                                                         cluster_number=int(cluster))\r\n\r\n\r\n\r\n            print(\"len of seg---------\",len(segements))\r\n            if segements is None:\r\n                print('segments is none')\r\n                continue\r\n\r\n            # pd.to_csv(\"seg.csv\")\r\n\r\n            # check service info for which segement to give which serives\r\n            # service_df = pd.read_sql_table('service_info', engine)\r\n            # segements_data = item.split(\"-\")\r\n            # trent = segements_data[0]\r\n            # rfm = segements_data[1]\r\n            # q = f\"trend == '{trent}' and rfm == '{rfm}'\"\r\n            # service_df = service_df.query(q).iloc[:, 3:]\r\n            # if not service_df.empty and service_df.all().all():\r\n            #     print(\"All boolean columns are True.\")\r\n            # else:\r\n            #     print(\"Not all boolean columns are True or no rows matched the query.\")\r\n\r\n            segements_new = rg.generate_rules(segements, temp_df)\r\n            # btc purpose\r\n            if rg.op is  not None :\r\n                output_d_list.append(rg.op)\r\n            # btc purpose end\r\n            for seg in segements_new:\r\n                if seg is not None:\r\n                    if seg.rule is not None:\r\n                        SegementRepo.update(db=db, item_data=seg)\r\n                    else:\r\n                        print(\"deleting segement\")\r\n                        SegementRepo.deleteById(db=db, _ids=[seg.id])\r\n            segements_two = SegementRepo.findByAutoPilotIdAndSegementNameAll(db=db, _id=dag_run_id,\r\n                                                                        segement_name=segements_name,\r\n                                                                        cluster_number=int(cluster))\r\n\r\n\r\n            for seg in segements_two:\r\n                print(f\"the segment name{seg.segment_name} and segment id is {seg.id}\")\r\n                if seg.samples < 10:\r\n                    ic(\"deleteing segement with less samples \")\r\n                    SegementRepo.deleteById(db=db, _ids=[seg.id])\r\n                if seg.rule is  None or len(seg.rule) < 3:\r\n                    ic(\"deleteing segement if rule is none or len is less than 3 \")\r\n                    print(\"seg.id is \",seg.id)\r\n                    SegementRepo.deleteById(db=db, _ids=[seg.id])\r\n                if seg.recommended_product_id is  None or seg.recommended_product_id ==0:\r\n                    ic(\"deleteing segement if recommended_product_id is none \")\r\n                    print(\"seg.id is \",seg.id)\r\n                    SegementRepo.deleteById(db=db, _ids=[seg.id])\r\n                for rfm_seg in not_needed_rfm_segment:\r\n                    if  rfm_seg in seg.segment_name:\r\n                        ic(\"deleteing segements that have no needed rfm segments\")\r\n                        SegementRepo.deleteById(db=db, _ids=[seg.id])\r\n                # recommended_pid = int(seg.recommended_product_id)\r\n                # print('recommended_pid is ',recommended_pid)\r\n                # print('type recommended_pid is ',type(recommended_pid))\r\n\r\n                # print('type next_purchase_date_df is ',next_purchase_date_df.dtypes)\r\n\r\n                # print('len is ', weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id'] == recommended_pid])\r\n                # print(\"weekday is \", weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0])\r\n                # seg.top_purchased_day_1 = weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0]\r\n                # seg.top_purchased_day_2=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_2'].iloc[0]\r\n                # seg.top_purchased_day_3=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_3'].iloc[0]\r\n                # seg.next_purchase_date_range=next_purchase_date_df[next_purchase_date_df['recommended_product_id']==recommended_pid]['range'].iloc[0]\r\n                # SegementRepo.update(db=db, item_data=seg)\r\n\r\n\r\n    # btc purpose\r\n    path_op = os.path.join(ml_location,dag_run_id,  \"output_data\")\r\n    Path(path_op).mkdir(parents=True, exist_ok=True)\r\n    print(output_d_list)        \r\n    dftnm = pd.concat(output_d_list)\r\n    dftnm['dag_run_id']=dag_run_id\r\n\r\n\r\n    dftnm.to_csv(os.path.join(path_op, \"APA_output.csv\"), header=True, index=False)\r\n\r\n    # for df_chunk in pd.read_csv(os.path.join(path_op, \"APA_output.csv\"),\r\n    #                             chunksize=10000):\r\n    #     # insert each chunk into database table\r\n    #     df_chunk.to_sql('APA_output', engine, if_exists='append', index=False)\r\n\r\n\r\n#         btc purpose end\r\n    return dftnm\r\n\r\n\r\n\r\n\r\ndef transform(dataframe):\r\n    db = get_db()\r\n    df = rule_generation(\"manual__2023-07-10T11:06:51\",db)\r\n    return df"
              }
            }, {
              "id": "15bc5fbe-87c5-7e1e-9eaf-dfb357bad98e",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nimport pickle\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\n\r\n\r\nfrom sqlalchemy import create_engine\r\nfrom sqlalchemy.ext.declarative import declarative_base\r\nfrom sqlalchemy.orm import sessionmaker\r\nfrom urllib.parse import quote  \r\n\r\n\r\n\r\nfrom sqlalchemy.orm import Session\r\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\r\nfrom sqlalchemy.orm import relationship\r\nimport datetime\r\nfrom sqlalchemy.dialects.mysql import LONGTEXT\r\n\r\n\r\nfrom typing import List, Optional\r\n\r\nfrom pydantic import BaseModel\r\n\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nfrom sklearn.feature_selection import SelectFromModel\r\n\r\n\r\n\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.metrics import classification_report\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.feature_selection import SelectFromModel\r\nimport sklearn\r\nimport json\r\nimport dask.dataframe as dd\r\nimport os\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\nfrom icecream import ic\r\nimport dask.dataframe as dd\r\nimport os\r\n# import configuration.config as cfg\r\n# import configuration.features as f\r\nimport traceback\r\nimport numpy as np\r\nfrom pathlib import Path\r\nimport pickle\r\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\r\nfrom pathlib import Path\r\nimport requests\r\n# from sql_app.repositories import AssociationRepo\r\n\r\n# config = cfg.Config().to_json()\r\n# features = f.Features().to_json()\r\n\r\n\r\n\r\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\r\n\r\nengine = create_engine(\r\n    SQLALCHEMY_DATABASE_URL,\r\n)\r\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\r\n\r\nBase = declarative_base()\r\n\r\ndef get_db():\r\n    db = SessionLocal()\r\n    try:\r\n        return db\r\n    finally:\r\n        db.close()\r\n        \r\n        \r\n        \r\nclass SegmentInformation(Base):\r\n    __tablename__ = \"APA_segment_information_new\"\r\n    id = Column(Integer, primary_key=True, index=True)\r\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\r\n    end_date = Column(DateTime)\r\n    current_product = Column(String(80), nullable=True, unique=False)\r\n    current_products_names = Column(String(200), nullable=True, unique=False)\r\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\r\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\r\n    predicted_arpu = Column(Integer, nullable=True)\r\n    current_arpu = Column(Integer, nullable=True)\r\n    segment_length = Column(String(80), nullable=True, unique=False)\r\n    rule = Column(LONGTEXT, nullable=True)\r\n    actual_rule = Column(LONGTEXT, nullable=True)\r\n    uplift_percent = Column(Float(precision=2), nullable=True)\r\n    incremental_revenue = Column(Float(precision=2), nullable=True)\r\n    campaign_type = Column(String(80), nullable=True, unique=False)\r\n    campaign_name = Column(String(80), nullable=True, unique=False)\r\n    action_key = Column(String(80), nullable=True, unique=False)\r\n    robox_id = Column(String(80), nullable=True, unique=False)\r\n    dag_run_id = Column(String(80), nullable=True, unique=False)\r\n    samples = Column(Integer, nullable=False)\r\n    segment_name = Column(String(80), nullable=True, unique=False)\r\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\r\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\r\n    customer_status = Column(String(80), nullable=True, unique=False)\r\n    query = Column(LONGTEXT, nullable=True, unique=False)\r\n    cluster_no = Column(Integer, nullable=True)\r\n    confidence = Column(Float(precision=2), nullable=True)\r\n    recommendation_type = Column(String(80), nullable=True, unique=False)\r\n    cluster_description = Column(LONGTEXT, nullable=True)\r\n    actual_target_count = Column(Integer, nullable=True)\r\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\r\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\r\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\r\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\r\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\r\n    total_revenue= Column(Integer, nullable=True)\r\n    uplift_revenue=Column(Integer, nullable=True)\r\n\r\n    def __repr__(self):\r\n        return 'SegmentInformation(name=%s)' % self.name\r\n\r\n\r\n\r\nclass SegementInfo(BaseModel):\r\n    end_date: Optional[str] = None\r\n    dag_run_id: Optional[str] = None\r\n    current_product: Optional[str] = None\r\n    current_products_names: Optional[str] = None\r\n    recommended_product_id: Optional[str] = None\r\n    recommended_product_name: Optional[str] = None\r\n    predicted_arpu: Optional[int] = None\r\n    current_arpu: Optional[int] = None\r\n    segment_length: Optional[str] = None\r\n    rule: Optional[str] = None\r\n    actual_rule: Optional[str] = None\r\n    uplift_percent: Optional[float] = None\r\n    incremental_revenue: Optional[float] = None\r\n    campaign_type: Optional[str] = None\r\n    campaign_name: Optional[str] = None\r\n    action_key: Optional[str] = None\r\n    robox_id: Optional[str] = None\r\n    samples: Optional[int] = None\r\n    segment_name: Optional[str] = None\r\n    current_ARPU_band: Optional[str] = None\r\n    current_revenue_impact: Optional[str] = None\r\n    customer_status: Optional[str] = None\r\n    query: Optional[str] = None\r\n    cluster_no: Optional[int] = None\r\n    confidence: Optional[float] = None\r\n    recommendation_type: Optional[str] = None\r\n    cluster_description: Optional[str] = None\r\n    actual_target_count: Optional[str] = None\r\n    top_purchased_day_1: Optional[str] = None\r\n    top_purchased_day_2: Optional[str] = None\r\n    top_purchased_day_3: Optional[str] = None\r\n    next_purchase_date_range: Optional[str] = None\r\n    campaign_response_percentage: Optional[str] = None\r\n    total_revenue: Optional[int] = None\r\n    uplift_revenue: Optional[int] = None\r\n\r\n\r\n\r\nclass SegementRepo:\r\n    def create(db: Session, segement: SegementInfo):\r\n        db_item = SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\r\n                                            campaign_type=segement.campaign_type,\r\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\r\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\r\n                                            rule=segement.rule, samples=segement.samples,\r\n                                            campaign_name=segement.campaign_name,\r\n                                            recommended_product_id=segement.recommended_product_id,\r\n                                            recommended_product_name=segement.recommended_product_name,\r\n                                            current_product=segement.current_product,\r\n                                            current_products_names=segement.current_products_names,\r\n                                            segment_length=segement.segment_length,\r\n                                            current_ARPU_band=segement.current_ARPU_band,\r\n                                            current_revenue_impact=segement.current_revenue_impact,\r\n                                            customer_status=segement.customer_status,\r\n                                            segment_name=segement.segment_name, query=segement.query,\r\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\r\n                                            recommendation_type=segement.recommendation_type,\r\n                                            cluster_description=segement.cluster_description,\r\n                                            actual_target_count=segement.actual_target_count,\r\n                                            top_purchased_day_1=segement.top_purchased_day_1,\r\n                                            top_purchased_day_2=segement.top_purchased_day_2,\r\n                                            top_purchased_day_3=segement.top_purchased_day_3,\r\n                                            next_purchase_date_range=segement.next_purchase_date_range,\r\n                                            campaign_response_percentage=segement.campaign_response_percentage,\r\n                                            total_revenue=segement.total_revenue,\r\n                                            uplift_revenue=segement.uplift_revenue,\r\n                                            )\r\n        db.add(db_item)\r\n        db.commit()\r\n        db.refresh(db_item)\r\n        return db_item\r\n\r\n    def findByAutoPilotIdAndClusterNo(db: Session, _id, cluster_no):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_no) \\\r\n            .all()\r\n\r\n    def findByAutoPilotIdAndSegementName(db: Session, _id, segement_name, cluster_number):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_number).first()\r\n\r\n    def findByAutoPilotIdAndSegementNameAll(db: Session, _id, segement_name, cluster_number):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_number).all()\r\n    \r\n    def findByAutoPilotIdAndSegementNamewithoutcluster(db: Session, _id, segement_name):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name).all()\r\n            \r\n\r\n\r\n    def findByAutoPilotId(db: Session, _id):\r\n        return db.query(SegmentInformation).filter(SegmentInformation.dag_run_id == _id).all()\r\n\r\n    def findByAutoPilotIdRecommendedId(db: Session, _id,recommended_product_id):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.recommended_product_id == recommended_product_id).all()\r\n\r\n    def deleteById(db: Session, _ids):\r\n        for id in _ids:\r\n            db.query(SegmentInformation).filter(SegmentInformation.id == id).delete()\r\n            db.commit()\r\n\r\n    def update(db: Session, item_data):\r\n        updated_item = db.merge(item_data)\r\n        db.commit()\r\n        return updated_item\r\n\r\n\r\n\r\ndef load_picke_file(filename):\r\n    with open(filename, 'rb') as handle:\r\n        data = pickle.load(handle)\r\n    return data\r\n    \r\n    \r\n    \r\nsegement_names = ['trend', 'rfm']\r\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\r\npurchase_location = '/home/tnmops/seahorse3_bkp/'\r\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\r\netl_location = '/home/tnmops/seahorse3_bkp/'\r\npack_info_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location = '/home/tnmops/seahorse3_bkp/'\r\nlog_file = '/home/tnmops/seahorse3_bkp/'\r\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\r\n\r\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\r\n\r\nusage_no_months = ['m1', 'm2', 'm3']\r\nrecharge_no_months = ['m1', 'm2', 'm3']\r\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\r\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\nthreshold = 0.50\r\nnot_needed_rfm_segment = ['Lost']\r\n\r\npack_cols = ['product_id','product_type']\r\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\r\n\r\ncampaign_df= ['c4','c3']\r\ncampaign_usage= ['m4','m3']\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_location': self.purchase_location,\r\n        'usage_location': self.etl_location,\r\n        'pack_info_location': self.pack_info_location,\r\n        'ml_location': self.ml_location,\r\n        'recharge_location':self.recharge_location\r\n    }\r\n    \r\n    \r\n    \r\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\r\n    'recharge_total_cntm2',\r\n    'recharge_total_cntm3',\r\n    'm1_total_voice_usage',\r\n    'm1_total_data_usage',\r\n    'm2_total_voice_usage',\r\n    'm2_total_data_usage',\r\n    'm3_total_voice_usage',\r\n    'm3_total_data_usage',\r\n    'purchase_total_cntm1',\r\n    'purchase_total_cntm2',\r\n    'purchase_total_cntm3', \r\n    'm3_total_revenue_m2_total_revenue_pct_drop',\r\n    'm3_data_revenue_m2_data_revenue_pct_drop',\r\n    'm2_voice_rev_m1_voice_rev_pct_drop',\r\n    'm2_total_revenue_m1_total_revenue_pct_drop',\r\n    'm2_data_revenue_m1_data_revenue_pct_drop',\r\n    'm3_voice_rev_m2_voice_rev_pct_drop',\r\n    'm1_m2_m3_average_voice',\r\n    'm1_m2_m3_average_data', \r\n    'm1_no_of_days',\r\n    'm2_no_of_days',\r\n    'm3_no_of_days',\r\n    'eng_index',\r\n    'consecutive_inactive_days']\r\n\r\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\r\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\r\nMSISDN_COL_NAME = 'msisdn'\r\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\r\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\r\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\r\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \r\nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\r\n\r\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\r\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\r\nTRANSACTION_PRODUCT_NAME = 'product_id'\r\n\r\npack_name = \"product_name\"\r\nPACK_CONTI_FEATURES = []\r\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\r\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\r\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\r\nPACK_INFO_CATEGORY = 'product_type'\r\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\r\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\r\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\r\nCUSTOMER_CATEG_FEATURES = []\r\nCUSTOMER_CONTI_FEATURES = []\r\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\r\n# 0 index for inbundle 1 index for outbundled  2 index for total\r\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\r\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\r\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\r\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\r\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\r\nCUSTOMER_NEEDED_COLUMN = [\r\n'onnet_revenue',\r\n'onnet_usage_da',\r\n'onnet_usage',\r\n'onnet_voice_count',\r\n'onnet_da_revenue',\r\n'offnet_revenue',\r\n'offnet_usage_da',\r\n'offnet_usage',\r\n'offnet_voice_count',\r\n'offnet_da_revenue',\r\n'idd_revenue',\r\n'idd_usage_da',\r\n'idd_usage',\r\n'idd_voice_count',\r\n'idd_da_revenue',\r\n'voice_rmg_revenue',\r\n'voice_rmg_usage_da',\r\n'voice_rmg_usage',\r\n'voice_rmg_count',\r\n'voice_rmg_da_revenue',\r\n'data_rmg_revenue',\r\n'data_rmg_usage_da',\r\n'data_rmg_usage',\r\n'data_rmg_da_revenue',\r\n'data_revenue',\r\n'data_usage_da',\r\n'data_usage',\r\n'da_data_rev',\r\n'sms_revenue',\r\n'sms_usage_da',\r\n'sms_usage',\r\n'sms_da_revenue',\r\n'sms_idd_revenue',\r\n'sms_idd_usage_da',\r\n'sms_idd_usage',\r\n'sms_idd_da_revenue',\r\n'magik_voice_amount',\r\n'rbt_subscription_rev',\r\n'emergency_credit_rev',\r\n'package_revenue',\r\n'voice_rev',\r\n'sms_rev',\r\n'onn_rev',\r\n'off_rev',\r\n'total_data_rev',\r\n'vas_rev',\r\n'vas_rev_others',\r\n'total_revenue',\r\n'total_voice_count',\r\n'total_voice_duration',\r\n'total_mainaccount_data_usage',\r\n'total_sms_count',\r\n'total_package_count',\r\n'total_other_vas_count',\r\n'total_voice_usage',\r\n'total_data_usage',\r\n'total_sms_usage']\r\n\r\n\r\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\r\n\r\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\r\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\r\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\r\n\r\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\r\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\r\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\r\n\r\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\r\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\r\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\r\n\r\nLABEL1 = 'downtrend'\r\nLABEL2 = 'uptrend'\r\nLABEL3 = 'zigzag'\r\nLABEL4 = 'flat'\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_features': self.TRANSACTION_FEATURES,\r\n        'usage_features': self.CUSTOMER_FEATURES,\r\n        'pack_info_features': self.ALL_PACK_FEATURES,\r\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\r\n        'usage_dtypes': self.CUSTOMER_DTYPES\r\n\r\n    }\r\n    \r\n    \r\nmsisdn_name = MSISDN_COL_NAME\r\nclass AssociationInfo(BaseModel):\r\n    dag_run_id: Optional[str] = None\r\n    current_pack: Optional[str] = None\r\n    current_pack_ids: Optional[str] = None\r\n    number_of_current_packs: Optional[int] = None\r\n    recommended_pack: Optional[str] = None\r\n    recommended_pack_id: Optional[str] = None\r\n    support: Optional[float] = None\r\n    lift: Optional[float] = None\r\n    confidence: Optional[float] = None\r\n    service_type: Optional[str] = None\r\n    type_info: Optional[str] = None\r\n    segement_name: Optional[str] = None\r\n    recommendation_type: Optional[str] = None\r\n    \r\nclass AssociationRepo:\r\n    def create(db: Session, info: AssociationInfo):\r\n        db_item = AssociationInfo(dag_run_id=info.dag_run_id, lift=info.lift,\r\n                                         service_type=info.service_type,\r\n                                         current_pack=info.current_pack, type_info=info.type_info,\r\n                                         confidence=info.confidence,\r\n                                         number_of_current_packs=info.number_of_current_packs,\r\n                                         support=info.support, recommended_pack=info.recommended_pack,\r\n                                         segement_name=info.segement_name,\r\n                                         recommended_pack_id=info.recommended_pack_id,\r\n                                         current_pack_ids=info.current_pack_ids,\r\n                                         recommendation_type=info.recommendation_type\r\n                                         )\r\n        db.add(db_item)\r\n        db.commit()\r\n        db.refresh(db_item)\r\n        return db_item\r\n        \r\nclass AssociationInfo(Base):\r\n    __tablename__ = \"APA_ASSOCIATION_INFO\"\r\n    id = Column(Integer, primary_key=True, index=True)\r\n    dag_run_id = Column(String(80), nullable=False)\r\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\r\n    current_pack_ids=Column(String(80), nullable=True, unique=False)\r\n    current_pack=Column(String(200), nullable=True, unique=False)\r\n    number_of_current_packs = Column(Integer, nullable=True)\r\n    recommended_pack = Column(String(80), nullable=True, unique=False)\r\n    recommended_pack_id=Column(String(80), nullable=True, unique=False)\r\n    support = Column(Float(precision=2), nullable=True)\r\n    lift = Column(Float(precision=2), nullable=True)\r\n    confidence = Column(Float(precision=2), nullable=True)\r\n    service_type = Column(String(80), nullable=True, unique=False)\r\n    type_info = Column(String(80), nullable=True, unique=False)\r\n    segement_name = Column(String(80), nullable=True, unique=False)\r\n    recommendation_type = Column(String(80), nullable=True, unique=False)\r\n    \r\nclass Fpgrowth(object):\r\n    def __init__(self, purchase, dag_run_id, item):\r\n        self.purchase = purchase\r\n        self.frequent_itemsets = None\r\n        self.results = None\r\n        self.results_processed = None\r\n        self.item_set_max_length = 4\r\n        self.item_set_min_length = 1\r\n        self.dag_run_id = dag_run_id\r\n        self.item = item\r\n\r\n        status, msg = self.form_associations()\r\n        print(msg)\r\n\r\n        if not status:\r\n            return None\r\n        self.process_association()\r\n\r\n        self.results_processed.to_csv(os.path.join(ml_location, self.dag_run_id,  \"association.csv\"), header=True,\r\n                                      index=False)\r\n        self.results_processed.to_csv(os.path.join(ml_location, self.dag_run_id,  str(item) + \"_association.csv\"),\r\n                                      header=True,\r\n                                      index=False)\r\n\r\n    def form_associations(self):\r\n        print('len of purchase before fropping na ', len(self.purchase))\r\n        self.purchase.dropna(inplace=True)\r\n        print('len of purchase ', len(self.purchase))\r\n\r\n        basket = (self.purchase.groupby([msisdn_name, TRANSACTION_PRODUCT_NAME])[\r\n                      TRANSACTION_PRODUCT_NAME]\r\n                  .count().unstack().reset_index().fillna(0)\r\n                  .set_index(msisdn_name))\r\n\r\n        print('basket created')\r\n        print('len of basket ', len(basket))\r\n\r\n        basket_sets = basket.applymap(encode_units)\r\n        basket_sets_filter = basket_sets[(basket_sets > 0).sum(axis=1) >= 2]\r\n        frequent_itemsets = fpgrowth(basket_sets_filter, min_support=0.03, use_colnames=True)\r\n        print('frequent_itemsets created')\r\n        print('len of frequent_itemsets ', len(frequent_itemsets))\r\n        if frequent_itemsets is None or len(frequent_itemsets) == 0:\r\n            # retun none so that the next cluster\r\n            return False, \"the result does not have any lenth the lenfth is \" + str(len(frequent_itemsets))\r\n        frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\r\n\r\n        self.frequent_itemsets = frequent_itemsets[(frequent_itemsets['length'] >= self.item_set_min_length) & (\r\n                frequent_itemsets['length'] <= self.item_set_max_length)]\r\n        print('frequent_itemsets filtered')\r\n        self.results = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=2).sort_values('lift',\r\n                                                                                                        ascending=False).reset_index(\r\n            drop=True)\r\n        print('association_rules created')\r\n        print('self.results is ', self.results)\r\n        print(' len self.results is ', len(self.results))\r\n        print(' type self.results is ', type(self.results))\r\n        if len(self.results) > 2:\r\n            return True, \"got the asociaitons\"\r\n        return False, \"the result does not have any lenth the lenfth is \" + str(len(self.results))\r\n\r\n    def process_association(self):\r\n        print(' inside process_association')\r\n        results = self.results\r\n        results['antecedents_length'] = results['antecedents'].apply(lambda x: len(x))\r\n        results['consequents_length'] = results['consequents'].apply(lambda x: len(x))\r\n        # antecedents1 is a set converted from frozen set antecedents\r\n        results['antecedents1'] = results['antecedents'].apply(set)\r\n        results['consequents1'] = results['consequents'].apply(set)\r\n\r\n        results.drop(['antecedents', 'consequents', 'leverage', 'conviction'], inplace=True, axis=1, errors='ignore')\r\n\r\n        self.results_processed = results\r\n\r\n\r\n\r\n\r\ndef encode_units(x):\r\n    if x <= 0:\r\n        return 0\r\n    if x >= 1:\r\n        return 1\r\n        \r\ndef getpackname(product_id, packinfo_df):\r\n    product_name = \"No product name\"\r\n    try:\r\n        if (packinfo_df is not None):\r\n            if (product_id in packinfo_df['product_id'].values):\r\n\r\n                product_name = str(packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"product_name\"])\r\n                print(\"the product name of \", product_id, \" is \", product_name)\r\n            else:\r\n                print(\"product_id is not in dataframe  \")\r\n        else:\r\n            print(\"some problem with the  dataframe  \")\r\n\r\n    except Exception as e:\r\n        print(\"error occurred in getpackname \", e)\r\n\r\n    return product_name\r\n    \r\ndef getpacktype(product_id, packinfo_df):\r\n    product_name = \"No product name\"\r\n    try:\r\n        if (packinfo_df is not None):\r\n            if (product_id in packinfo_df['product_id'].values):\r\n\r\n                product_name = str(packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"type\"])\r\n                print(\"the product name of \", product_id, \" is \", product_name)\r\n            else:\r\n                print(\"product_id is not in dataframe  \")\r\n        else:\r\n            print(\"some problem with the  dataframe  \")\r\n\r\n    except Exception as e:\r\n        print(\"error occurred in getpackname \", e)\r\n\r\n    return product_name\r\n    \r\ndef get_rule_query(segements):\r\n\r\n    return None\r\n        \r\nclass UpsellCrossell(object):\r\n    def __init__(self, consequents_length=1, exclude_types=None,\r\n                 dag_run_id=None, db=None, pack_info=None,\r\n                 result=None, cluster_number=None, segement_name=None):\r\n        self.segement_name_list = None\r\n        if exclude_types is None:\r\n            exclude_types = ['SMS']\r\n        self.dag_run_id = dag_run_id\r\n        self.exclude_types = exclude_types\r\n        self.associations_df = result\r\n        self.db = db\r\n        self.pack_info = pack_info\r\n        self.df_cross_df = None\r\n        self.df_upsell_df = None\r\n        self.consequents_length = consequents_length\r\n\r\n        # self.read_fiels()\r\n        self.check_files()\r\n        # self.determine_service_type()\r\n        # print('df_cross_df is', self.df_cross_df)\r\n        # if self.df_cross_df is not None and len(self.df_cross_df) > 0:\r\n        #     self.find_crossell()\r\n        # print(\"outputed crosssell files \")\r\n        # for number in [1, 2, 3]:\r\n        #     print('going to find upsell for number', number)\r\n        #     self.find_upsell(type_service='upsell', anticendent_number=number,)\r\n        # print(\"done with upsell cross sell\")\r\n        #\r\n        # print(\"outputed crosssell files \")\r\n\r\n    def read_fiels(self):\r\n        try:\r\n            self.pack_info = pd.read_csv(os.path.join(ml_location,self.dag_run_id, 'packinfo.csv'))\r\n            self.associations_df = pd.read_csv(os.path.join(ml_location, self.dag_run_id, 'association.csv'))\r\n        except Exception as e:\r\n            raise ValueError(e)\r\n\r\n    def check_files(self):\r\n        if self.pack_info is None or self.associations_df is None:\r\n            print(f\"the packinfo or association is null\")\r\n            raise ValueError(f\"the packinfo or association is null\")\r\n\r\n    def determine_service_type(self):\r\n        try:\r\n            print('self.associations_df is ', self.associations_df)\r\n            print('self.associations_df.columns is ', self.associations_df.columns)\r\n            if len(self.associations_df) <= 2:\r\n                return\r\n            self.associations_df = self.associations_df[self.associations_df['consequents_length'] == 1]\r\n            df = self.associations_df.apply(self.determine_service, axis=1)\r\n            if len(df) == 0:\r\n                return\r\n            for type_info in self.exclude_types:\r\n                df = df[~df['type_service'].str.contains(type_info)]\r\n\r\n            self.df_upsell_df = df[df['service'] == 'upsell']\r\n            self.df_cross_df = df[df['service'] != 'upsell']\r\n\r\n\r\n        except Exception as e:\r\n            print(\"error occoured in determine service\" + str(e))\r\n            raise ValueError(e)\r\n\r\n    def determine_service(self, x):\r\n        print('x is', x)\r\n\r\n        group_type = PACK_INFO_SUB_CATEGORY\r\n\r\n        def get_pack_type(pack_name):\r\n            type_pack = None\r\n            print(\"isnide get_pack_type\")\r\n            print(\"pack_name is \", pack_name)\r\n            print(\"group_type is \", group_type)\r\n            # print(\"f.Features.PACK_INFO_PACK_COLUMN_NAME is \", f.Features.PACK_INFO_PACK_COLUMN_NAME)\r\n            # print(\"self.pack_info is \", self.pack_info)\r\n            print(\"len self.pack_info is \", len(self.pack_info))\r\n\r\n            data = self.pack_info[self.pack_info[PACK_INFO_PACK_COLUMN_NAME] == pack_name][group_type]\r\n            print(\"data is \", data)\r\n            if len(data) > 0:\r\n                type_pack = data.values[0]\r\n                print(\"type_pack is \", type_pack)\r\n            return type_pack\r\n\r\n        x['antecedents1'] = str(x['antecedents1'])\r\n        x['consequents1'] = str(x['consequents1'])\r\n\r\n        antecedents_list = list(eval(x['antecedents1']))\r\n        print('antecedents_list is ', antecedents_list)\r\n        consequents_list = list(eval(x['consequents1']))\r\n        print('consequents_list is ', consequents_list)\r\n        antecedents_length = len(antecedents_list)\r\n\r\n        i = 0\r\n        service = \"upsell\"\r\n        type_service = None\r\n        temp_set = set()\r\n        try:\r\n            # since for each antecident we need to find eg: antecident length 1 ,2 ,3\r\n            while i < antecedents_length:\r\n                print('i is', i)\r\n                temp_set.add(get_pack_type(antecedents_list[i]))\r\n                print('temp_set is', temp_set)\r\n                i = i + 1\r\n                print('i is', i)\r\n            temp_set.add(get_pack_type(consequents_list[0]))\r\n            print('temp_set 1  is', temp_set)\r\n\r\n            if len(temp_set) > 1:\r\n                service = \"corsssell\"\r\n                type_service = ','.join(temp_set)\r\n            elif len(temp_set) == 1:\r\n                type_service = next(iter(temp_set))\r\n\r\n            x['service'] = service\r\n            x['type_service'] = type_service\r\n            return x\r\n        except Exception as e:\r\n            print(\"error occoured in determine_service\", e)\r\n            raise ValueError(e)\r\n\r\n    def find_crossell(self, segement_name, cluster_number):\r\n        try:\r\n            segement_name = f\"{segement_name}-{str(int(cluster_number))}\"\r\n            # self.segement_name_list = segement_name.split(\"|\")\r\n\r\n            print(\"self.df_cross_df is \", self.df_cross_df)\r\n            df = self.df_cross_df.apply(self.cross_sell_parser, axis=1)\r\n\r\n            if len(df) > 0:\r\n                df = df.sort_values(by=\"confidence\", ascending=False)\r\n                segements = SegementRepo.findByAutoPilotIdAndSegementName(self.db, _id=self.dag_run_id,\r\n                                                                          segement_name=segement_name,\r\n                                                                          cluster_number=int(cluster_number))\r\n                if segements is None:\r\n                    return\r\n                self.insert_segementinfo(segements, 1, df, \"crossell\")\r\n                SegementRepo.deleteById(self.db, [segements.id])\r\n                # confidence = int(df.head(1)['confidence'].values[0] * 100)\r\n                # current_pack = list(eval(df.head(1)['antecedents1'].values[0]))[0]\r\n                # recommended_pack = str(df.head(1)['conci'].values[0])\r\n                # number_of_current_packs = int(df.head(1)['antecedents_length'].values[0])\r\n                # if confidence > .50:\r\n                #     segements = SegementRepo.findByAutoPilotIdAndSegementName(self.db, _id=self.dag_run_id,\r\n                #                                                               segement_name=f\"{segement_name}_{str(int(cluster_number))}\",\r\n                #                                                               cluster_number=int(cluster_number))\r\n                #     segements.campaign_type = \"cross_sell\"\r\n                #     segements.confidence = confidence\r\n                #     segements.recommended_product_name = recommended_pack\r\n                #     segements.current_product = current_pack\r\n                #\r\n                #     SegementRepo.update(self.db, segements)\r\n\r\n                # self.update_info(\"antecedents1\", \"cross_sell\", segement_name, cluster_number)\r\n            # ------------------------adding to db -------------------------------#\r\n            for index, row in df.iterrows():\r\n                print(row['confidence'])\r\n                info = AssociationInfo()\r\n                info.service_type = str(row['type_service'])\r\n                info.type_info = str(row['val'])\r\n                info.dag_run_id = self.dag_run_id\r\n                info.support = round(float(row['support']), 2)\r\n                info.confidence = round(float(row['confidence']), 2)\r\n                info.lift = round(float(row['lift']), 2)\r\n                print(\"row['conci'] is \", row['conci'])\r\n\r\n                info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\r\n                info.recommended_pack_id = str(row['conci'])\r\n\r\n                info.number_of_current_packs = int(row['antecedents_length'])\r\n                info.current_pack = \"|\".join(\r\n                    [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\r\n                info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\r\n\r\n                info.segement_name = segements.segment_name\r\n                current_pack_types = \"|\".join(\r\n                    [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\r\n                info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\r\n                print(\"added crossel info to db\")\r\n                AssociationRepo.create(db=self.db, info=info)\r\n            # ------------------------adding to db -------------------------------#\r\n\r\n\r\n        # ----------------------------to br looked at ------------------------------------------------------#\r\n        # df_2 = df[df['val'] == 'crossell'].sort_values(by='confidence', ascending=False)\r\n        #\r\n        # df_2.drop_duplicates(subset=['consequents1'], inplace=True)\r\n        # df_cross_conci = df_2.drop_duplicates(subset=['conci'])\r\n        # df_cross_anti = df_2.drop_duplicates(subset=['anti'])\r\n        # df_2 = pd.concat([df_cross_conci, df_cross_anti])\r\n        # df_2.drop_duplicates(subset=['anti', 'conci'], keep='first')\r\n        # # df_2.drop(['antecedent support', 'consequent support', 'support', 'lift',\r\n        # #            'antecedents1', 'consequents1'], axis=1, inplace=True, errors='ignore')\r\n        #\r\n        # df_3 = df_2.merge(\r\n        #     self.pack_info[\r\n        #         f.Features.ALL_PACK_FEATURES].add_prefix(\r\n        #         \"ANTECEDENT_\"), left_on='anti', right_on='ANTECEDENT_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\r\n        #     how='left')\r\n        # df_3 = df_3.merge(\r\n        #     self.pack_info[\r\n        #         f.Features.ALL_PACK_FEATURES].add_prefix(\r\n        #         \"CONSEQUENT_\"), left_on='conci', right_on='CONSEQUENT_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\r\n        #     how='left')\r\n        # # print(\"length of df_3 before condition is  \", len(df_3))\r\n        # # df_3 = df_3[df_3['ANTECEDENT_price'] < df_3['CONSEQUENT_price']]\r\n        # # print(\"length of df_3 after condition  is  \", len(df_3))\r\n        #\r\n        # if os.path.exists(os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'crossell_one.csv')):\r\n        #\r\n        #     crossell_one_df = pd.read_csv(\r\n        #         os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'crossell_one.csv'))\r\n        #     df_3 = df_3.drop(['val', 'anti', 'conci'], axis=1)\r\n        #     final_crossell_one_df = pd.concat([crossell_one_df, df_3])\r\n        #     final_crossell_one_df.to_csv(\r\n        #         os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'crossell_one.csv'),\r\n        #         header=True,\r\n        #         index=False)\r\n        # else:\r\n        #\r\n        #     df_3.drop(['val', 'anti', 'conci'], axis=1, errors='ignore').to_csv(\r\n        #         os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'crossell_one.csv'),\r\n        #         header=True, index=False)\r\n\r\n        # ----------------------------to br looked at ------------------------------------------------------#\r\n        except Exception as e:\r\n            traceback.print_exc()\r\n            raise ValueError(e)\r\n\r\n    def cross_sell_parser(self, x):\r\n        cross_sell = False\r\n        try:\r\n            print('x is', x)\r\n            anti = list(eval(x['antecedents1']))\r\n            conci = list(eval(x['consequents1']))\r\n            print('here 2 ')\r\n            print(f\"anti {anti} conci {conci}\")\r\n            if len(anti) == 1:\r\n                anti_catagory = self.pack_info[self.pack_info[pack_name] == anti[0]]['bundle_type'].values\r\n                anti_class = self.pack_info[self.pack_info[pack_name] == anti[0]]['bundle_type'].values\r\n                if len(anti_catagory) > 0:\r\n                    anti_catagory = anti_catagory[0]\r\n                    anti_class = anti_class[0]\r\n                conci_catagory = self.pack_info[self.pack_info[pack_name] == conci[0]]['bundle_type'].values\r\n                conci_class = self.pack_info[self.pack_info[pack_name] == conci[0]]['bundle_type'].values\r\n                if len(conci_catagory) > 0:\r\n                    conci_catagory = conci_catagory[0]\r\n                    conci_class = conci_class[0]\r\n                print(f\"the anti c  is {anti_catagory}  and conci cata is  {conci_catagory}\")\r\n                if ((anti_catagory != conci_catagory) and (anti_class != conci_class)):\r\n                    cross_sell = True\r\n            anti_name = None\r\n            if len(anti) == 2:\r\n                cross_sell = True\r\n                for anti_item in anti:\r\n                    print(\"the anti item \", anti_item)\r\n                    anti_catagory = self.pack_info[self.pack_info[pack_name] == anti_item]['bundle_type'].values\r\n                    conci_catagory = self.pack_info[self.pack_info[pack_name] == conci[0]]['bundle_type'].values\r\n                    print(f\"the anticent name {anti_item} conci name {conci[0]}\")\r\n                    print(f\"the anticent catagory {anti_catagory} conci cata {conci_catagory}\")\r\n                    if (anti_catagory == conci_catagory):\r\n                        cross_sell = False\r\n                        anti_name = None\r\n                    else:\r\n                        anti_name = anti_item\r\n\r\n            if cross_sell:\r\n                x['val'] = \"crossell\"\r\n                if len(anti) == 2:\r\n                    x['anti'] = anti_name\r\n                else:\r\n                    x['anti'] = anti[0]\r\n\r\n                x['conci'] = conci[0]\r\n                return x\r\n            x['val'] = \"not_valid_crossell\"\r\n            x['anti'] = anti[0]\r\n            x['conci'] = conci[0]\r\n            return x\r\n        except Exception as e:\r\n            print('the error occoured in find crosssell', e)\r\n            raise ValueError(e)\r\n\r\n    def find_upsell(self, type_service, anticendent_number, segement_name, cluster_number):\r\n        try:\r\n\r\n            is_upsell = 1\r\n            if type_service != 'upsell':\r\n                is_upsell = 0\r\n\r\n            segements = SegementRepo.findByAutoPilotIdAndSegementName(self.db, _id=self.dag_run_id,\r\n                                                                      segement_name=f\"{segement_name}-{str(int(cluster_number))}\",\r\n                                                                      cluster_number=int(cluster_number))\r\n            if segements is None:\r\n                return\r\n            if anticendent_number == 1:\r\n                print(\"inside antecedent 1 \")\r\n                df = self.df_upsell_df[self.df_upsell_df['antecedents_length'] == 1]\r\n                data = df.sort_values(by='confidence', ascending=False)\r\n                print(len(data.drop_duplicates(subset=['consequents1'])))\r\n                print(\"data.columns inside antecedent 1 is \", data.columns)\r\n                data1 = data.apply(self.check_upsell, anticendent_number=1, axis=1)\r\n                print('check_upsell completed')\r\n                # ------------------- insert db ------------------------------\r\n\r\n                # ------------------------adding to db -------------------------------#\r\n                for index, row in data1.iterrows():\r\n                    print(row['confidence'])\r\n                    info = AssociationInfo()\r\n                    info.service_type = str(row['type_service'])\r\n                    info.type_info = str(row['upsell_case'])\r\n                    info.dag_run_id = self.dag_run_id\r\n                    info.support = round(float(row['support']), 2)\r\n                    info.confidence = round(float(row['confidence']), 2)\r\n                    info.lift = round(float(row['lift']), 2)\r\n                    print(\"row['conci'] is \", row['conci'])\r\n                    info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\r\n                    info.recommended_pack_id = str(row['conci'])\r\n\r\n                    info.number_of_current_packs = int(row['antecedents_length'])\r\n                    info.current_pack = \"|\".join(\r\n                        [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\r\n                    info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\r\n                    info.segement_name = segements.segment_name\r\n                    print('info.number_of_current_packs is', info.number_of_current_packs)\r\n                    print('info.current_pack is', info.current_pack)\r\n                    print('info.recommended_pack is', info.recommended_pack)\r\n\r\n                    print('info is', info)\r\n                    current_pack_types = \"|\".join(\r\n                        [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\r\n                    info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\r\n                    AssociationRepo.create(db=self.db, info=info)\r\n                    print('all  inserted')\r\n\r\n                self.insert_segementinfo(segements, anticendent_number, data1, \"upsell\")\r\n\r\n                # ------------------------adding to db -------------------------------#\r\n                # if data1 is None or len(data1) == 0:\r\n                #     ic(f\"the segement {segement_name} and cluser {cluster_number}  has no upsell 1 data1 \")\r\n                #     return\r\n                # ic(f\"the segement {segement_name} and cluser {cluster_number} \")\r\n                # data2 = data1[data1['is_upsell'] == is_upsell]\r\n                # if data2 is None or len(data2) == 0:\r\n                #     ic(f\"the segement {segement_name} and cluser {cluster_number}  has no upsell 1\")\r\n                #     return\r\n                # data2 = data2.sort_values(by='confidence', ascending=False)\r\n                # data2 = data2.head(5)\r\n                # segments_list = []\r\n                # for index, row in data2.iterrows():\r\n                #     info = schemas.SegementInfo()\r\n                #     info.segment_name = segements.segment_name\r\n                #     info.dag_run_id = self.dag_run_id\r\n                #     info.current_product = str(row['antecedents1'])\r\n                #     info.current_products_names = str(row['antecedents1'])\r\n                #     info.recommended_product_id = str(row['conci'])\r\n                #     info.recommended_product_name = str(row['conci'])\r\n                #     info.predicted_arpu = None\r\n                #     info.current_arpu = None\r\n                #     info.segment_length = segements.segment_length\r\n                #     info.rule = None\r\n                #     info.actual_rule = None\r\n                #     info.uplift = None\r\n                #     info.incremental_revenue = None\r\n                #     info.campaign_type = \"upsell\"\r\n                #     info.campaign_name = None\r\n                #     info.action_key = None\r\n                #     info.robox_id = None\r\n                #     info.samples = segements.samples\r\n                #     info.segment_name = segements.segment_name\r\n                #     info.current_ARPU_band = None\r\n                #     info.current_revenue_impact = None\r\n                #     info.customer_status = segements.customer_status\r\n                #     info.query = segements.query\r\n                #     info.cluster_no = segements.cluster_no\r\n                #     info.confidence = round(float(row['confidence']), 2)\r\n                #     segments_list.append(info)\r\n                #\r\n                # for segment in segments_list:\r\n                #     SegementRepo.create(self.db, segment)\r\n                # ------------------------------------------------------------\r\n\r\n            # ---------------------ipo venda -----------------------------------------#\r\n            # data2 = data1[data1['is_upsell'] == is_upsell]\r\n            # print('here1')\r\n            # data3 = data2.drop_duplicates(subset=['consequents1'])\r\n            # data3['antecedent_one'] = data3['antecedents1'].apply(lambda x: list(eval(x))[0])\r\n            # print('here2')\r\n            # data3['consequents1'] = data3['consequents1'].apply(lambda x: list(eval(x))[0])\r\n            # print('here3')\r\n            # data3 = data3.merge(\r\n            #     self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\r\n            #         \"ANTECEDENT_\"),\r\n            #     left_on='antecedent_one', right_on=\"ANTECEDENT_\" + f.Features.PACK_INFO_PACK_COLUMN_NAME,\r\n            #     how='left')\r\n            # data3 = data3.merge(\r\n            #     self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\r\n            #         \"CONSEQUENT_\"),\r\n            #     left_on='consequents1', right_on='CONSEQUENT_' + f.Features.PACK_INFO_PACK_COLUMN_NAME, how='left')\r\n            # # data3.drop(\r\n            # #     ['antecedent_one', 'consequents1', 'service', 'antecedent support', 'consequent support', 'lift'],\r\n            # #     axis=1, inplace=True)\r\n            # data4 = data3[data3['ANTECEDENT_' + f.Features.PACK_INFO_CATEGORY] == data3[\r\n            #     'CONSEQUENT_' + f.Features.PACK_INFO_CATEGORY]]\r\n            #\r\n            # file_name = os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'upsell_one.csv')\r\n            # if os.path.exists(file_name):\r\n            #     upsell_one_df = pd.read_csv(file_name)\r\n            #     final_upsell_one_df = pd.concat([upsell_one_df, data4])\r\n            #     final_upsell_one_df.to_csv(file_name, header=True,\r\n            #                                index=False)\r\n            #\r\n            # else:\r\n            #\r\n            #     data4.to_csv(os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'upsell_one.csv'),\r\n            #                  header=True,\r\n            #                  index=False)\r\n            # print(\"outputed 1 upsell anticident \")\r\n            # ---------------------ipo venda -----------------------------------------#\r\n\r\n            elif anticendent_number == 2:\r\n                print(\"inside antecedent 2 \")\r\n                df = self.df_upsell_df[self.df_upsell_df['antecedents_length'] == 2]\r\n                if len(df) == 0:\r\n                    ic(f\"the anticendent_number 2 is not present for upsell {segement_name}_{str(int(cluster_number))}\")\r\n                    return None\r\n                data = df.sort_values(by='confidence', ascending=False)\r\n                print(len(data.drop_duplicates(subset=['consequents1'])))\r\n                print(\"data.columns inside antecedent 2  is \", data.columns)\r\n                data1 = data.apply(self.check_upsell, anticendent_number=2, axis=1)\r\n                # ------------------------adding to db -------------------------------#\r\n                for index, row in data1.iterrows():\r\n                    print(row['confidence'])\r\n                    info = AssociationInfo()\r\n                    info.service_type = str(row['type_service'])\r\n                    info.type_info = str(row['upsell_case'])\r\n                    info.dag_run_id = self.dag_run_id\r\n                    info.support = round(float(row['support']), 2)\r\n                    info.confidence = round(float(row['confidence']), 2)\r\n                    info.lift = round(float(row['lift']), 2)\r\n                    print(\"row['conci'] is \", row['conci'])\r\n\r\n                    info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\r\n                    info.recommended_pack_id = str(row['conci'])\r\n\r\n                    info.segement_name = segements.segment_name\r\n                    info.number_of_current_packs = int(row['antecedents_length'])\r\n                    info.current_pack = \"|\".join(\r\n                        [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\r\n                    info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\r\n                    print('info.number_of_current_packs is', info.number_of_current_packs)\r\n                    print('info.current_pack is', info.current_pack)\r\n                    print('info.recommended_pack is', info.recommended_pack)\r\n                    current_pack_types = \"|\".join(\r\n                        [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\r\n                    info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\r\n                    AssociationRepo.create(db=self.db, info=info)\r\n                # ------------------------adding to db -------------------------------#\r\n                self.insert_segementinfo(segements, anticendent_number, data1, \"upsell\")\r\n                # ---------------------ipo venda -----------------------------------------#\r\n                # data2 = data1[data1['is_upsell'] == 1]\r\n                # data3 = data2.drop_duplicates(subset=['consequents1'])\r\n                # data3['antecedents_one'] = data3['antecedents1'].apply(lambda x: list(eval(x))[0])\r\n                # data3['antecedents_two'] = data3['antecedents1'].apply(lambda x: list(eval(x))[1])\r\n                # data3['consequent'] = data3['consequents1'].apply(lambda x: list(eval(x))[0])\r\n                # # data3.drop(['antecedents1', 'consequents1', 'service', 'antecedent support', 'consequent support'],\r\n                # #            axis=1,\r\n                # #            inplace=True)\r\n                # data3 = data3.merge(\r\n                #     self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\r\n                #         \"ANTECEDENT_1_\"),\r\n                #     left_on='antecedents_one', right_on='ANTECEDENT_1_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\r\n                #     how='left')\r\n                # data3 = data3.merge(\r\n                #     self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\r\n                #         \"ANTECEDENT_2_\"),\r\n                #     left_on='antecedents_two', right_on='ANTECEDENT_2_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\r\n                #     how='left')\r\n                # data3 = data3.merge(\r\n                #     self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\r\n                #         \"CONSEQUENT_\"),\r\n                #     left_on='consequent', right_on='CONSEQUENT_' + f.Features.PACK_INFO_PACK_COLUMN_NAME, how='left')\r\n                # file_name = os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'upsell_two.csv');\r\n                # if os.path.exists(file_name):\r\n                #     upsell_two_df = pd.read_csv(file_name)\r\n                #     final_upsell_two_df = pd.concat([upsell_two_df, data3])\r\n                #     final_upsell_two_df.to_csv(file_name, header=True,\r\n                #                                index=False)\r\n                #\r\n                # else:\r\n                #     data3.to_csv(file_name, header=True, index=False)\r\n                # print(\"outputed 2 upsell anticident \")\r\n\r\n                # ---------------------ipo venda -----------------------------------------#\r\n\r\n            elif anticendent_number == 3:\r\n                print(\"inside antecedent 3 \")\r\n                df = self.df_upsell_df[self.df_upsell_df['antecedents_length'] == 3]\r\n                if len(df) == 0:\r\n                    ic(f\"the anticendent_number 3 is not present for upsell {segement_name}_{str(int(cluster_number))}\")\r\n                    return None\r\n\r\n                data = df.sort_values(by='confidence', ascending=False)\r\n                print(len(data.drop_duplicates(subset=['consequents1'])))\r\n                print(\"data.columns inside antecedent 3 is \", data.columns)\r\n                data1 = data.apply(self.check_upsell, anticendent_number=3, axis=1)\r\n                # ------------------------adding to db -------------------------------#\r\n                for index, row in data1.iterrows():\r\n                    print(row['confidence'])\r\n                    info = AssociationInfo()\r\n                    info.service_type = str(row['type_service'])\r\n                    info.type_info = str(row['upsell_case'])\r\n                    info.dag_run_id = self.dag_run_id\r\n                    info.support = round(float(row['support']), 2)\r\n                    info.confidence = round(float(row['confidence']), 2)\r\n                    info.lift = round(float(row['lift']), 2)\r\n                    print(\"row['conci'] is \", row['conci'])\r\n\r\n                    info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\r\n                    info.recommended_pack_id = str(row['conci'])\r\n\r\n                    info.segement_name = segements.segment_name\r\n                    info.number_of_current_packs = int(row['antecedents_length'])\r\n                    info.current_pack = \"|\".join(\r\n                        [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\r\n                    info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\r\n                    print('info.number_of_current_packs is', info.number_of_current_packs)\r\n                    print('info.current_pack is', info.current_pack)\r\n                    print('info.recommended_pack is', info.recommended_pack)\r\n                    current_pack_types = \"|\".join(\r\n                        [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\r\n                    info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\r\n                    AssociationRepo.create(db=self.db, info=info)\r\n                # ------------------------adding to db -------------------------------#\r\n                self.insert_segementinfo(segements, anticendent_number, data1, \"upsell\")\r\n\r\n            SegementRepo.deleteById(self.db, [segements.id])\r\n            # ---------------------ipo venda -----------------------------------------#\r\n            #     data2 = data1[data1['is_upsell'] == 1]\r\n            #     data3 = data2.drop_duplicates(subset=['consequents1'])\r\n            #     data3['antecedents_one'] = data3['antecedents1'].apply(lambda x: list(eval(x))[0])\r\n            #     data3['antecedents_two'] = data3['antecedents1'].apply(lambda x: list(eval(x))[1])\r\n            #     data3['antecedents_three'] = data3['antecedents1'].apply(lambda x: list(eval(x))[2])\r\n            #     data3['consequent'] = data3['consequents1'].apply(lambda x: list(eval(x))[0])\r\n            #     # data3.drop(['antecedents1', 'consequents1', 'service', 'antecedent support', 'consequent support'],\r\n            #     #            axis=1,\r\n            #     #            inplace=True)\r\n            #     data3 = data3.merge(\r\n            #         self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\r\n            #             \"ANTECEDENT_1_\"),\r\n            #         left_on='antecedents_one', right_on='ANTECEDENT_1_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\r\n            #         how='left')\r\n            #     data3 = data3.merge(\r\n            #         self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\r\n            #             \"ANTECEDENT_2_\"),\r\n            #         left_on='antecedents_two', right_on='ANTECEDENT_2_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\r\n            #         how='left')\r\n            #     data3 = data3.merge(\r\n            #         self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\r\n            #             \"ANTECEDENT_3_\"),\r\n            #         left_on='antecedents_three', right_on='ANTECEDENT_3_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\r\n            #         how='left')\r\n            #     data3 = data3.merge(\r\n            #         self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\r\n            #             \"CONSEQUENT_\"),\r\n            #         left_on='consequent', right_on='CONSEQUENT_' + f.Features.PACK_INFO_PACK_COLUMN_NAME, how='left')\r\n            #     file_name = os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'upsell_three.csv');\r\n            #     if os.path.exists(file_name):\r\n            #         upsell_three_df = pd.read_csv(file_name)\r\n            #         final_upsell_three_df = pd.concat([upsell_three_df, data3])\r\n            #         final_upsell_three_df.to_csv(file_name, header=True,\r\n            #                                      index=False)\r\n            #     else:\r\n            #         data3.to_csv(file_name, header=True, index=False)\r\n            #     print(\"outputed 3 upsell anticident \")\r\n            #\r\n            # else:\r\n            #     raise ValueError(\"anticient number more that 3 \")\r\n\r\n            # ---------------------ipo venda -----------------------------------------#\r\n        except Exception as e:\r\n            print(\"error ocoured in output service\", e)\r\n            raise ValueError(e)\r\n\r\n    def check_upsell(self, x, anticendent_number):\r\n        print('Inside check_upsell')\r\n\r\n        col = PACK_INFO_PACK_PRICE_COLUMN_NAME\r\n\r\n        def get_price(pack_name):\r\n            price = None\r\n            data = self.pack_info[self.pack_info[PACK_INFO_PACK_COLUMN_NAME] == pack_name][col]\r\n            if (len(data) > 0):\r\n                price = data.values[0]\r\n            return price\r\n\r\n        anti = None\r\n        conci = None\r\n        print('x is', x)\r\n        print(\"x['consequents1'] is\", x['consequents1'])\r\n        print(\"anticendent_number is\", anticendent_number)\r\n        x['is_upsell'] = 1\r\n        x['upsell_case'] = \"upsell\"\r\n\r\n        try:\r\n            conci = list(eval(x['consequents1']))[0]\r\n            print(\"teh conci is \", conci)\r\n            conci_price = get_price(conci)\r\n            print(f\"the conci price is {conci_price}\")\r\n            x['conci'] = conci\r\n\r\n            if anticendent_number == 1:\r\n                anti = list(eval(x['antecedents1']))[0]\r\n                anti_price = get_price(anti)\r\n\r\n                if conci_price < anti_price:\r\n                    x['is_upsell'] = 0\r\n                    x['upsell_case'] = \"not_a_valid_upsell\"\r\n                    x['conci'] = conci\r\n            else:\r\n                amti_list = list(eval(x['antecedents1']))\r\n                for anti_obj in amti_list:\r\n\r\n                    anti_price = get_price(anti_obj)\r\n                    if conci_price <= anti_price:\r\n                        x['is_upsell'] = 0\r\n                        x['upsell_case'] = \"not_a_valid_upsell\"\r\n                        x['conci'] = conci\r\n\r\n            return x\r\n        except Exception as e:\r\n            print(\" the error occoured in check_upsell \", e)\r\n            raise ValueError(e)\r\n\r\n    def update_info(self, anticident_name, type_of_service, segement_name, cluster_number):\r\n        segements = SegementRepo.findByAutoPilotIdAndSegementName(self.db, segement_name=segement_name,\r\n                                                                  cluster_number=cluster_number)\r\n        pass\r\n\r\n    def insert_segementinfo(self, segements, anticendent_number, data1, type_of_service):\r\n        if data1 is None or len(data1) == 0:\r\n            ic(f\"the segement {segements.segment_name} has no upsell 1 data1 \")\r\n            return\r\n        if type_of_service == \"upsell\":\r\n            data1 = data1[data1['is_upsell'] == 1]\r\n            if data1 is None or len(data1) == 0:\r\n                ic(f\"the segement {segements.segment_name}  has no upsell 1\")\r\n                return\r\n        data2 = data1.sort_values(by='confidence', ascending=False)\r\n        data2 = data2.head(5)\r\n        segments_list = []\r\n        for index, row in data2.iterrows():\r\n            get_rule_query(segements)\r\n            info = SegementInfo()\r\n            info.segment_name = segements.segment_name\r\n            info.cluster_description=segements.cluster_description\r\n            info.dag_run_id = self.dag_run_id\r\n            current_product_id = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\r\n            info.current_product = current_product_id\r\n            info.current_products_names = \"|\".join(\r\n                [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\r\n            # if anticendent_number == 1:\r\n            #     info.current_products_names = str(row['antecedents1'])\r\n            # elif anticendent_number == 2:\r\n            #     info.current_products_names = str(row['antecedents1']) + str(row['antecedents2'])\r\n            # else:\r\n            #     info.current_products_names = str(row['antecedents1']) + str(row['antecedents2']) + str(\r\n            #         row['antecedents3'])\r\n\r\n            rec_product_id = str(row['conci'])\r\n            info.recommended_product_id = rec_product_id\r\n\r\n            info.recommended_product_name = str(getpackname(row['conci'], self.pack_info))\r\n\r\n            info.predicted_arpu = None\r\n            info.current_arpu = None\r\n            info.segment_length = segements.segment_length\r\n            info.rule = None\r\n            info.actual_rule = None\r\n            info.uplift_revenue = None\r\n            info.incremental_revenue = None\r\n            info.campaign_type = type_of_service\r\n            info.campaign_name = None\r\n            info.action_key = None\r\n            info.robox_id = None\r\n            info.samples = segements.samples\r\n            info.segment_name = segements.segment_name\r\n            info.current_ARPU_band = None\r\n            info.current_revenue_impact = None\r\n            info.customer_status = segements.customer_status\r\n            info.query = segements.query\r\n            info.cluster_no = segements.cluster_no\r\n            info.confidence = round(float(row['confidence']), 2)\r\n            current_pack_types = \"|\".join(\r\n                [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\r\n            info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\r\n\r\n            segments_list.append(info)\r\n\r\n        for segment in segments_list:\r\n            SegementRepo.create(self.db, segment)\r\n            \r\n            \r\ndef association_process(dag_run_id, db):\r\n    result_dict_path = os.path.join(ml_location,dag_run_id, \"purchased_for_association\", 'dict.pickle')#data\r\n    data_path = os.path.join(ml_location,dag_run_id,  \"dict.pickle\")#datadict\r\n    data_dict = load_picke_file(data_path)\r\n    pack_info = os.path.join(etl_location, 'pack_info.csv')\r\n    pack_info_df = pd.read_csv(pack_info)\r\n    data = load_picke_file(result_dict_path)\r\n    # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\r\n    # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\r\n    # filtered_data__dict = {k: v for k, v in data_dict.items() if k in needed_segements}\r\n    \r\n    \r\n    # for key, value in data_dict.items():\r\n    #     data_dict[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\r\n        \r\n    # for key, value in data.items():\r\n    #     if value is None:\r\n    #         continue\r\n    #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\r\n        \r\n    filtered_dict = {k: v for k, v in data.items()}\r\n    filtered_data__dict = {k: v for k, v in data_dict.items()}\r\n    \r\n    purchase_list=[]\r\n    for item, val in filtered_dict.items():\r\n        if val is None:\r\n            continue\r\n        \r\n        # if filtered_data__dict.get(item) is None:\r\n        #     continue\r\n        # else:\r\n        data = pd.read_csv(filtered_data__dict.get(item))\r\n        \r\n        purchase = pd.read_csv(val)\r\n        for cluster in data['label'].unique():\r\n            data_temp = data[data['label'] == cluster]\r\n            purchase_filtered = purchase[purchase[msisdn_name].isin(data_temp[msisdn_name])]\r\n            pruchase_grp_df=purchase_filtered.groupby([TRANSACTION_PRODUCT_NAME]).agg({msisdn_name:'nunique'}).reset_index()\r\n            pruchase_grp_df['segment_name'] = f\"{item}-{str(cluster)}\"\r\n            print( ' len of pruchase_grp_df if  ' ,len(pruchase_grp_df))\r\n            purchase_list.append(pruchase_grp_df)\r\n            purchase_filtered['product_id'] = purchase_filtered['product_id'].astype(int)\r\n            \r\n#             print(purchase_filtered['product_id'])\r\n            print('len of purchase in association_process ', len(purchase_filtered))\r\n            fp = Fpgrowth(purchase_filtered, dag_run_id, item=item)\r\n            print('fp.results is ', fp.results)\r\n            if fp.results is None or len(fp.results) == 0:\r\n                print('skipped this iteration')\r\n                continue\r\n            print('Fpgrowth completed')\r\n            uc = UpsellCrossell(dag_run_id=dag_run_id, pack_info=pack_info_df, result=fp.results, db=db)\r\n            uc.determine_service_type()\r\n            if len(uc.associations_df) <= 2:\r\n                continue\r\n            if uc.df_cross_df is not None and len(uc.df_cross_df) > 0:\r\n                uc.find_crossell(segement_name=item, cluster_number=cluster)\r\n            print(\"outputed crosssell files \")\r\n            if uc.df_upsell_df is None or len(uc.df_upsell_df) == 0:\r\n                continue\r\n            for number in [1, 2, 3]:\r\n                print('going to find upsell for number', number)\r\n                uc.find_upsell(type_service='upsell', anticendent_number=number, segement_name=item,\r\n                               cluster_number=cluster)\r\n            print(\"done with upsell cross sell\")\r\n\r\n            print('UpsellCrossell completed')\r\n\r\n\r\n    purchase_list_df = pd.concat(purchase_list)\r\n    purchase_list_df['dag_run_id']=str(dag_run_id)\r\n    purchase_list_df.to_csv(os.path.join(ml_location,dag_run_id,  \"total_purchased_products.csv\"), header=True,\r\n                       index=False)\r\n    # for df_chunk in pd.read_csv(os.path.join(ml_location,dag_run_id,  \"total_purchased_products.csv\"),\r\n    #                             chunksize=10000):\r\n    #     # insert each chunk into database table\r\n    #     df_chunk.to_sql(name='APA_total_purchased_products', con=engine, if_exists='append', index=False)\r\n    return purchase_list_df\r\n    \r\n    \r\n    \r\n\r\n\r\ndef transform(dataframe):\r\n    db = get_db()\r\n    df = association_process(\"manual__2023-07-10T11:06:51\",db)\r\n    \r\n    return df"
              }
            }, {
              "id": "77423f41-512c-4db1-92e0-65020ac7970a",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nfrom icecream import ic\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\nfrom pathlib import Path\r\nimport pickle\r\nimport math\r\nfrom dask.delayed import delayed\r\nimport dask\r\nfrom datetime import datetime\r\nfrom sklearn.feature_selection import SelectFromModel\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nfrom sklearn.feature_selection import SelectKBest, chi2\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.metrics import accuracy_score\r\nfrom imblearn.under_sampling import RandomUnderSampler\r\nfrom sklearn.metrics import classification_report\r\nfrom urllib.parse import quote  \r\n\r\n\r\n\r\nimport pandas as pd\r\nimport pickle\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\n\r\n\r\nfrom sqlalchemy import create_engine\r\nfrom sqlalchemy.ext.declarative import declarative_base\r\nfrom sqlalchemy.orm import sessionmaker\r\nfrom urllib.parse import quote  \r\n\r\n\r\n\r\nfrom sqlalchemy.orm import Session\r\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\r\nfrom sqlalchemy.orm import relationship\r\nimport datetime\r\nfrom sqlalchemy.dialects.mysql import LONGTEXT\r\n\r\n\r\nfrom typing import List, Optional\r\n\r\nfrom pydantic import BaseModel\r\n\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nfrom sklearn.feature_selection import SelectFromModel\r\n\r\n\r\n\r\n\r\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\r\n\r\nengine = create_engine(\r\n    SQLALCHEMY_DATABASE_URL,\r\n)\r\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\r\n\r\nBase = declarative_base()\r\n\r\ndef get_db():\r\n    db = SessionLocal()\r\n    try:\r\n        return db\r\n    finally:\r\n        db.close()\r\n        \r\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\r\nsegement_names = ['trend', 'rfm']\r\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\r\npurchase_location = '/home/tnmops/seahorse3_bkp/'\r\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\r\netl_location = '/home/tnmops/seahorse3_bkp/'\r\npack_info_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location = '/home/tnmops/seahorse3_bkp/'\r\nlog_file = '/home/tnmops/seahorse3_bkp/'\r\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\r\n\r\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\r\n\r\nusage_no_months = ['m1', 'm2', 'm3']\r\nrecharge_no_months = ['m1', 'm2', 'm3']\r\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\r\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\nthreshold = 0.50\r\nnot_needed_rfm_segment = ['Lost']\r\n\r\npack_cols = ['product_id','product_type']\r\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\r\n\r\ncampaign_df= ['c4','c3']\r\ncampaign_usage= ['m4','m3']\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_location': self.purchase_location,\r\n        'usage_location': self.etl_location,\r\n        'pack_info_location': self.pack_info_location,\r\n        'ml_location': self.ml_location,\r\n        'recharge_location':self.recharge_location\r\n    }\r\n    \r\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\r\n    'recharge_total_cntm2',\r\n    'recharge_total_cntm3',\r\n    'm1_total_voice_usage',\r\n    'm1_total_data_usage',\r\n    'm2_total_voice_usage',\r\n    'm2_total_data_usage',\r\n    'm3_total_voice_usage',\r\n    'm3_total_data_usage',\r\n    'purchase_total_cntm1',\r\n    'purchase_total_cntm2',\r\n    'purchase_total_cntm3', \r\n    'm3_total_revenue_m2_total_revenue_pct_drop',\r\n    'm3_data_revenue_m2_data_revenue_pct_drop',\r\n    'm2_voice_rev_m1_voice_rev_pct_drop',\r\n    'm2_total_revenue_m1_total_revenue_pct_drop',\r\n    'm2_data_revenue_m1_data_revenue_pct_drop',\r\n    'm3_voice_rev_m2_voice_rev_pct_drop',\r\n    'm1_m2_m3_average_voice',\r\n    'm1_m2_m3_average_data', \r\n    'm1_no_of_days',\r\n    'm2_no_of_days',\r\n    'm3_no_of_days',\r\n    'eng_index',\r\n    'consecutive_inactive_days']\r\n\r\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\r\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\r\nMSISDN_COL_NAME = 'msisdn'\r\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\r\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\r\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\r\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \r\nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\r\n\r\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\r\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\r\nTRANSACTION_PRODUCT_NAME = 'product_id'\r\n\r\nPACK_CONTI_FEATURES = []\r\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\r\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\r\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\r\nPACK_INFO_CATEGORY = 'product_type'\r\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\r\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\r\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\r\nCUSTOMER_CATEG_FEATURES = []\r\nCUSTOMER_CONTI_FEATURES = []\r\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\r\n# 0 index for inbundle 1 index for outbundled  2 index for total\r\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\r\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\r\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\r\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\r\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\r\nCUSTOMER_NEEDED_COLUMN = [\r\n        'onnet_revenue',  'onnet_usage', 'onnet_voice_count', \r\n        'offnet_revenue', 'offnet_usage', 'offnet_voice_count',  \r\n        'idd_revenue', 'idd_usage', 'idd_voice_count',\r\n        'voice_rmg_revenue', 'voice_rmg_usage', 'voice_rmg_count', \r\n        'data_rmg_revenue',  'data_rmg_usage',  \r\n        'data_revenue', 'data_usage', 'sms_revenue', 'sms_usage',  'sms_idd_revenue', \r\n        'sms_idd_usage', 'magik_voice_amount', 'rbt_subscription_rev', 'emergency_credit_rev', \r\n        'package_revenue', 'voice_rev', 'sms_rev', 'onn_rev', 'off_rev', 'total_data_rev', \r\n        'vas_rev', 'vas_rev_others', 'total_revenue', 'total_voice_count', 'total_voice_duration', \r\n        'total_mainaccount_data_usage', 'total_sms_count', 'total_package_count', 'total_other_vas_count', \r\n        'total_voice_usage', 'total_data_usage', 'total_sms_usage'\r\n        ]\r\n\r\n\r\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\r\n\r\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\r\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\r\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\r\n\r\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\r\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\r\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\r\n\r\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\r\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\r\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\r\n\r\nLABEL1 = 'downtrend'\r\nLABEL2 = 'uptrend'\r\nLABEL3 = 'zigzag'\r\nLABEL4 = 'flat'\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_features': self.TRANSACTION_FEATURES,\r\n        'usage_features': self.CUSTOMER_FEATURES,\r\n        'pack_info_features': self.ALL_PACK_FEATURES,\r\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\r\n        'usage_dtypes': self.CUSTOMER_DTYPES\r\n\r\n    }\r\n    \r\n    \r\n    \r\nimport logging\r\nimport os\r\n\r\ndef get_file_names():\r\n    return {\r\n        \"usage\": {\r\n            \"m1\": \"usage_jan.csv\",\r\n            \"m2\": \"usage_feb.csv\",\r\n            \"m3\": \"usage_march.csv\"},\r\n\r\n        \"recharge\": {\r\n            \"m1\": \"recharge_july_20230411084648.csv\",\r\n            \"m2\": \"recharge_june_20230411084648.csv\",\r\n            \"m3\": \"recharge_may_20230411084648.csv\"\r\n        },\r\n        \"purchase\": {\r\n            \"m1\": \"purchase_july_20230411090513.csv\",\r\n            \"m2\": \"purchase_june_20230411090513.csv\",\r\n            \"m3\": \"purchase_may_20230411090513.csv\"\r\n        },\r\n         \"daily_summerized\": {\r\n            \"m1\": \"daily_summarized_july_20230517122646.csv\",\r\n            \"m2\": \"daily_summarized_june_20230517122646.csv\",\r\n            \"m3\": \"daily_summarized_may_20230517122646.csv\",  \r\n          \r\n        },\r\n        \"weekwise\": \r\n        {\r\n          \r\n           \"m1\": \"weekwise_july_20230517180929.csv\",\r\n           \"m2\": \"weekwise_june_20230517180929.csv\",\r\n           \"m3\": \"weekwise_may_20230517180929.csv\", \r\n          \r\n        },\r\n        \"weekly_daily\": \r\n        {\r\n          \r\n            \"m1\": \"daily_weekly_avg_july_20230517163429.csv\",\r\n            \"m2\": \"daily_weekly_avg_june_20230517163429.csv\",\r\n            \"m3\": \"daily_weekly_avg_may_20230517163429.csv\" ,  \r\n          \r\n        },\r\n        \"rfm_purchase\": \r\n        {\r\n  \r\n            \"p1\": \"purchase_march_20230411090513.csv\",\r\n            \"p2\": \"purchase_april_20230411090513.csv\",\r\n            \"p3\": \"purchase_may_20230411090513.csv\",\r\n            \"p4\": \"purchase_june_20230411090513.csv\",\r\n            \"p5\": \"purchase_july_20230411090513.csv\"\r\n  \r\n        },\r\n         \"campaign_data\": \r\n        {\r\n            \"c3\": \"campaign_detailed_fct_july_20230517092908.csv\",\r\n            \"c4\": \"campaign_detailed_fct_june_20230517092908.csv\" ,            \r\n                      \r\n        },\r\n      \r\n        \"pack\":\r\n        {\r\n            \"pack\":\"pack_info.csv\"\r\n        },\r\n        \"profile\":\"profile_july_20230801055632.csv\"\r\n      \r\n    }\r\n\r\n\r\n\r\n    # source_purchase_and_etl_location = \"/log/magikuser/tnm_autopilot_calender_month\"\r\n    # purchase_location = \"/log/magikuser/autopilot_data/purchase\"\r\n    # etl_location = \"/log/magikuser/autopilot_data/etl/etl_with_130_columns\"\r\n    # pack_info_location = '/log/magikuser/autopilot_data/packinfo'\r\n    # ml_location = '/log/magikuser/autopilot_data/ml'\r\n    #\r\n    \r\n\r\nclass SegmentInformation(Base):\r\n    __tablename__ = \"APA_segment_information_new\"\r\n    id = Column(Integer, primary_key=True, index=True)\r\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\r\n    end_date = Column(DateTime)\r\n    current_product = Column(String(80), nullable=True, unique=False)\r\n    current_products_names = Column(String(200), nullable=True, unique=False)\r\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\r\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\r\n    predicted_arpu = Column(Integer, nullable=True)\r\n    current_arpu = Column(Integer, nullable=True)\r\n    segment_length = Column(String(80), nullable=True, unique=False)\r\n    rule = Column(LONGTEXT, nullable=True)\r\n    actual_rule = Column(LONGTEXT, nullable=True)\r\n    uplift_percent = Column(Float(precision=2), nullable=True)\r\n    incremental_revenue = Column(Float(precision=2), nullable=True)\r\n    campaign_type = Column(String(80), nullable=True, unique=False)\r\n    campaign_name = Column(String(80), nullable=True, unique=False)\r\n    action_key = Column(String(80), nullable=True, unique=False)\r\n    robox_id = Column(String(80), nullable=True, unique=False)\r\n    dag_run_id = Column(String(80), nullable=True, unique=False)\r\n    samples = Column(Integer, nullable=False)\r\n    segment_name = Column(String(80), nullable=True, unique=False)\r\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\r\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\r\n    customer_status = Column(String(80), nullable=True, unique=False)\r\n    query = Column(LONGTEXT, nullable=True, unique=False)\r\n    cluster_no = Column(Integer, nullable=True)\r\n    confidence = Column(Float(precision=2), nullable=True)\r\n    recommendation_type = Column(String(80), nullable=True, unique=False)\r\n    cluster_description = Column(LONGTEXT, nullable=True)\r\n    actual_target_count = Column(Integer, nullable=True)\r\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\r\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\r\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\r\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\r\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\r\n    total_revenue= Column(Integer, nullable=True)\r\n    uplift_revenue=Column(Integer, nullable=True)\r\n\r\n    def __repr__(self):\r\n        return 'SegmentInformation(name=%s)' % self.name\r\n\r\n\r\nclass SegementInfo(BaseModel):\r\n    end_date: Optional[str] = None\r\n    dag_run_id: Optional[str] = None\r\n    current_product: Optional[str] = None\r\n    current_products_names: Optional[str] = None\r\n    recommended_product_id: Optional[str] = None\r\n    recommended_product_name: Optional[str] = None\r\n    predicted_arpu: Optional[int] = None\r\n    current_arpu: Optional[int] = None\r\n    segment_length: Optional[str] = None\r\n    rule: Optional[str] = None\r\n    actual_rule: Optional[str] = None\r\n    uplift_percent: Optional[float] = None\r\n    incremental_revenue: Optional[float] = None\r\n    campaign_type: Optional[str] = None\r\n    campaign_name: Optional[str] = None\r\n    action_key: Optional[str] = None\r\n    robox_id: Optional[str] = None\r\n    samples: Optional[int] = None\r\n    segment_name: Optional[str] = None\r\n    current_ARPU_band: Optional[str] = None\r\n    current_revenue_impact: Optional[str] = None\r\n    customer_status: Optional[str] = None\r\n    query: Optional[str] = None\r\n    cluster_no: Optional[int] = None\r\n    confidence: Optional[float] = None\r\n    recommendation_type: Optional[str] = None\r\n    cluster_description: Optional[str] = None\r\n    actual_target_count: Optional[str] = None\r\n    top_purchased_day_1: Optional[str] = None\r\n    top_purchased_day_2: Optional[str] = None\r\n    top_purchased_day_3: Optional[str] = None\r\n    next_purchase_date_range: Optional[str] = None\r\n    campaign_response_percentage: Optional[str] = None\r\n    total_revenue: Optional[int] = None\r\n    uplift_revenue: Optional[int] = None\r\n\r\n\r\nclass SegementRepo:\r\n    def create(db: Session, segement: SegementInfo):\r\n        db_item = SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\r\n                                            campaign_type=segement.campaign_type,\r\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\r\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\r\n                                            rule=segement.rule, samples=segement.samples,\r\n                                            campaign_name=segement.campaign_name,\r\n                                            recommended_product_id=segement.recommended_product_id,\r\n                                            recommended_product_name=segement.recommended_product_name,\r\n                                            current_product=segement.current_product,\r\n                                            current_products_names=segement.current_products_names,\r\n                                            segment_length=segement.segment_length,\r\n                                            current_ARPU_band=segement.current_ARPU_band,\r\n                                            current_revenue_impact=segement.current_revenue_impact,\r\n                                            customer_status=segement.customer_status,\r\n                                            segment_name=segement.segment_name, query=segement.query,\r\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\r\n                                            recommendation_type=segement.recommendation_type,\r\n                                            cluster_description=segement.cluster_description,\r\n                                            actual_target_count=segement.actual_target_count,\r\n                                            top_purchased_day_1=segement.top_purchased_day_1,\r\n                                            top_purchased_day_2=segement.top_purchased_day_2,\r\n                                            top_purchased_day_3=segement.top_purchased_day_3,\r\n                                            next_purchase_date_range=segement.next_purchase_date_range,\r\n                                            campaign_response_percentage=segement.campaign_response_percentage,\r\n                                            total_revenue=segement.total_revenue,\r\n                                            uplift_revenue=segement.uplift_revenue,\r\n                                            )\r\n        db.add(db_item)\r\n        db.commit()\r\n        db.refresh(db_item)\r\n        return db_item\r\n\r\n    def findByAutoPilotIdAndClusterNo(db: Session, _id, cluster_no):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_no) \\\r\n            .all()\r\n\r\n    def findByAutoPilotIdAndSegementName(db: Session, _id, segement_name, cluster_number):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_number).first()\r\n\r\n    def findByAutoPilotIdAndSegementNameAll(db: Session, _id, segement_name, cluster_number):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_number).all()\r\n    \r\n    def findByAutoPilotIdAndSegementNamewithoutcluster(db: Session, _id, segement_name):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name).all()\r\n            \r\n\r\n\r\n    def findByAutoPilotId(db: Session, _id):\r\n        return db.query(SegmentInformation).filter(SegmentInformation.dag_run_id == _id).all()\r\n\r\n    def findByAutoPilotIdRecommendedId(db: Session, _id,recommended_product_id):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.recommended_product_id == recommended_product_id).all()\r\n\r\n    def deleteById(db: Session, _ids):\r\n        for id in _ids:\r\n            db.query(SegmentInformation).filter(SegmentInformation.id == id).delete()\r\n            db.commit()\r\n\r\n    def update(db: Session, item_data):\r\n        updated_item = db.merge(item_data)\r\n        db.commit()\r\n        return updated_item\r\n\r\n\r\ndef otliner_removal(df, per=0.97):\r\n    try:\r\n\r\n        q = df['total_revenue'].quantile(per)\r\n        print(\"the length brfore is\", len(df))\r\n        df = df[df['total_revenue'] < q]\r\n        print(\"the length after is\", len(df))\r\n        return df\r\n    except Exception as e:\r\n        print(e)\r\n        raise RuntimeError(e)\r\n        \r\ndef filter_data(temp_df):\r\n    cols = [p + \"_\" + s for s in CUSTOMER_NEEDED_COLUMN for p in usage_no_months]\r\n    cols.append(\"msisdn\")\r\n    temp_df = temp_df[cols]\r\n\r\n    return temp_df\r\n    \r\ndef train_model(data, sample=False):\r\n    def randomsample(X, y):\r\n        rus = RandomUnderSampler(random_state=42)\r\n        X_res, y_res = rus.fit_resample(X, y)\r\n        return X_res, y_res\r\n\r\n    def run_decision_tree(X_train, X_test, y_train, y_test):\r\n        clf = DecisionTreeClassifier(max_leaf_nodes=12)\r\n        clf.fit(X_train, y_train)\r\n        y_pred = clf.predict(X_test)\r\n        print('Accuracy run_decision_tree: ', accuracy_score(y_test, y_pred))\r\n        print(\"confusion matrix run_decision_tree\", confusion_matrix(y_test, y_pred))\r\n        print(classification_report(y_test, y_pred))\r\n        return clf\r\n\r\n    X = data.drop(['msisdn', 'label'], axis=1)\r\n    y = data['label']\r\n\r\n    if sample:\r\n        X, y = randomsample(X=X, y=y)\r\n\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n    print(f\"X_train.shape = {X_train.shape}, X_test.shape =  {X_test.shape}\")\r\n    # sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\r\n    sel = SelectFromModel(DecisionTreeClassifier(max_leaf_nodes=12))\r\n    sel.fit(X_train, y_train)\r\n    sel.get_support()\r\n    features = X_train.columns[sel.get_support()]\r\n    print(\"the feature selection features are \", features)\r\n    X_train_rfc = sel.transform(X_train)\r\n    X_test_rfc = sel.transform(X_test)\r\n    clf1 = run_decision_tree(X_train_rfc, X_test_rfc, y_train, y_test)\r\n    features_importance = list(clf1.feature_importances_)\r\n    dic = {features[i]: features_importance[i] for i in range(len(features))}\r\n    # t = None\r\n    # try:\r\n    #     t = pd.read_csv('temp.csv')\r\n    #     tt = t.append(dic, ignore_index=True)\r\n    # except:\r\n    #     t = pd.DataFrame(columns=list(X_train.columns))\r\n    #     tt = t.append(dic, ignore_index=True)\r\n    #\r\n    # print(tt.head(5))\r\n    # tt.to_csv('temp.csv', header=True, index=False)\r\n    return clf1, features\r\n    \r\ndef get_top_product_ids(row):\r\n    return row.nlargest(2).index.tolist()\r\n    \r\ndef getpackname(product_id, packinfo_df):\r\n    product_name = \"No product name\"\r\n    try:\r\n        if (packinfo_df is not None):\r\n            if (product_id in packinfo_df['product_id'].values):\r\n\r\n                product_name = str(packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"product_name\"])\r\n                print(\"the product name of \", product_id, \" is \", product_name)\r\n            else:\r\n                print(\"product_id is not in dataframe  \")\r\n        else:\r\n            print(\"some problem with the  dataframe  \")\r\n\r\n    except Exception as e:\r\n        print(\"error occurred in getpackname \", e)\r\n\r\n    return product_name\r\n    \r\n\r\n\r\ndef getpackprice(product_id, packinfo_df):\r\n    price = 0\r\n    try:\r\n        if (packinfo_df is not None):\r\n            if (product_id in packinfo_df['product_id'].values):\r\n\r\n                price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\r\n                print(\"the price  of \", product_id, \" is \", price)\r\n            else:\r\n                print(\"product_id is not in dataframe  \")\r\n        else:\r\n            print(\"some problem with the  dataframe  \")\r\n\r\n    except Exception as e:\r\n        print(\"error occurred in getpackprice \", e)\r\n\r\n    return price\r\n    \r\n    \r\ndef insert_segement_info_churn(item, dag_run_id, data_filter_churners, recommenting_product,recommenting_product_name,\r\n                                      incremental_revenue,uplift, current_arpu,predicted_arpu ,total_revenue,db):\r\n    info = SegementInfo()\r\n    info.segment_name = item\r\n    info.cluster_description = None\r\n    info.dag_run_id = dag_run_id\r\n    current_product_id = None\r\n    info.current_product = current_product_id\r\n    # info.current_products_names = \"|\".join(\r\n    #     [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\r\n    info.recommended_product_id\r\n    d = str(recommenting_product)\r\n    info.recommended_product_name = str(recommenting_product_name)\r\n    info.predicted_arpu = int(predicted_arpu)\r\n    info.current_arpu = int(current_arpu)\r\n    info.segment_length = len(data_filter_churners)\r\n    info.rule = None\r\n    info.actual_rule = None\r\n    info.uplift_revenue = float(uplift)\r\n    info.incremental_revenue = float(incremental_revenue)\r\n    info.campaign_type = \"churn\"\r\n    info.campaign_name = None\r\n    info.action_key = None\r\n    info.robox_id = None\r\n    info.samples = info.segment_length\r\n    info.segment_name = item\r\n    info.current_ARPU_band = None\r\n    info.current_revenue_impact = None\r\n    info.customer_status = None\r\n    info.query = None\r\n    info.cluster_no = None\r\n    info.confidence = None\r\n    info.total_revenue = total_revenue\r\n    # info.top_purchased_day_1=top_purchased_day_1\r\n    # info.top_purchased_day_2=top_purchased_day_2\r\n    # info.top_purchased_day_3=top_purchased_day_3\r\n    # info.next_purchase_date_range=next_purchase_date_range\r\n\r\n    info.recommendation_type = f\"nbo\"\r\n\r\n    SegementRepo.create(db, info)\r\n    \r\n\r\ndef load_picke_file(filename):\r\n    with open(filename, 'rb') as handle:\r\n        data = pickle.load(handle)\r\n    return data\r\n    \r\ndef churn_process(dag_run_id,db):\r\n    try:\r\n        \r\n        pack_info = os.path.join(etl_location, 'pack_info.csv')\r\n        pack_info_df = pd.read_csv(pack_info)\r\n        file_name_dict = get_file_names()\r\n        print(\"finding trend ongoing \")\r\n        data = {}\r\n        for month in usage_no_months:\r\n            data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\r\n                                      dtype=CUSTOMER_DTYPES)\r\n        months = usage_no_months\r\n        temp_df = None\r\n        for month in months:\r\n            df = data.get(month)\r\n            df = df.fillna(0)\r\n            total_revenue = CUSTOMER_TOTAL_REVENUE[0]\r\n            # df['tot_rev'] = df[total_revenue]\r\n            df = otliner_removal(df.copy())#-------------------------------------------------------------\r\n            df = df.add_prefix(f\"{month}_\")\r\n            df = df.rename(columns={f\"{month}_msisdn\": \"msisdn\"})\r\n            if temp_df is None:\r\n                temp_df = df\r\n            else:\r\n\r\n                temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\r\n                temp_df = temp_df.fillna(0)\r\n        temp_df = temp_df.compute()\r\n        temp_df = filter_data(temp_df)#----------------------------------------------------\r\n        temp_df['label'] = ((temp_df['m3_total_revenue'] > 0) &\r\n                            (temp_df['m2_total_revenue'] > 0) &\r\n                            (temp_df['m1_total_revenue'] == 0)).astype(int)\r\n        \r\n        data_for_uplift = temp_df[['msisdn','m1_total_revenue']]\r\n        temp_df = temp_df.loc[:, ~temp_df.columns.str.startswith('m1')]\r\n        clf1, features = train_model(temp_df, sample=True)#----------------------------------------------\r\n        ic(f\"the features selected are \", features)\r\n        data_for_prediction = temp_df[features]\r\n        \r\n        \r\n        msisdns = temp_df.pop(\"msisdn\")\r\n        probabilities = clf1.predict_proba(data_for_prediction)#------------------------------------------\r\n        data_for_prediction['predictions'] = clf1.predict(data_for_prediction)\r\n        data_for_prediction['predict_proba_0'] = probabilities[:, 0]\r\n        data_for_prediction['predict_proba_1'] = probabilities[:, 1]\r\n        data_for_prediction['msisdn'] = msisdns\r\n       \r\n        #matrix operations\r\n        matrix_path = os.path.join(ml_location,dag_run_id,  \"matrix.csv\")\r\n        matrix = dd.read_csv(matrix_path)\r\n        data_path = os.path.join(ml_location,dag_run_id,  \"dict.pickle\")\r\n        data_dict = load_picke_file(data_path)\r\n        \r\n        \r\n        # for key, value in data_dict.items():\r\n        #     data_dict[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\r\n        \r\n#         for key, value in data.items():\r\n#             if value is None:\r\n#                 continue\r\n#             data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\r\n            \r\n            \r\n        filtered_dict = {k: v for k, v in data_dict.items()}\r\n        final_df_ls=[]\r\n        for item, val in filtered_dict.items():\r\n            data = pd.read_csv(filtered_dict.get(item))\r\n            data_filter = data_for_prediction[data_for_prediction['msisdn'].isin(data['msisdn'])]\r\n            data_filter_churners = data_filter[data_filter['predict_proba_1'] > .50]\r\n            data_for_uplift_temp = data_for_uplift[data_for_uplift['msisdn'].isin(data_filter_churners['msisdn'])]\r\n\r\n            matrix_filter = matrix[matrix['msisdn'].isin(data['msisdn'])]\r\n            if matrix_filter is None or len(matrix_filter)==0:\r\n                continue\r\n            matrix_filter = matrix_filter.compute()\r\n            \r\n            msisdns_list = matrix_filter.pop(\"msisdn\")\r\n            matrix_filter['top_2_products'] = matrix_filter.iloc[:, 1:].apply(get_top_product_ids, axis=1)\r\n            matrix_filter['highest_product'] = matrix_filter['top_2_products'].apply(lambda x: x[0])\r\n            matrix_filter['second_highest_product'] = matrix_filter['top_2_products'].apply(lambda x: x[1])\r\n            print('len(matrix_filter)',len(matrix_filter))\r\n            print('matrix_filter[highest_product].value_counts()', matrix_filter['highest_product'].value_counts())\r\n            recommenting_product = matrix_filter['highest_product'].value_counts().idxmax()\r\n            data_filter_churners['recommended_product_id'] = recommenting_product\r\n            recommenting_product_name = str(getpackname(int(recommenting_product), pack_info_df))\r\n            data_filter_churners['recommenting_product_name'] = recommenting_product_name\r\n            data_filter_churners['segement_id'] = 0000\r\n\r\n            segements_data = item.split(\"-\")\r\n            trend = segements_data[0]\r\n            rfm = segements_data[1]\r\n            data_filter_churners['segement_name'] = item\r\n            data_filter_churners['trend'] = trend\r\n            data_filter_churners['rfm'] = rfm\r\n\r\n            print('recommenting_product is' ,recommenting_product)\r\n            print('recommenting_product_name is' ,recommenting_product_name)\r\n\r\n            current_arpu = data_for_uplift_temp['m1_total_revenue'].mean()\r\n            total_revenue = int(data_for_uplift_temp['m1_total_revenue'].sum())\r\n            if math.isnan(current_arpu):\r\n                current_arpu=0\r\n            recommenting_product_price = getpackprice(int(recommenting_product), pack_info_df)\r\n\r\n\r\n            print('recommenting_product_price is ',recommenting_product_price,'and type is ',type(recommenting_product_price))\r\n            \r\n            print('len(data_for_uplift_temp)',len(data_for_uplift_temp))\r\n            incremental_revenue =  recommenting_product_price * len(data_for_uplift_temp)\r\n\r\n            if incremental_revenue !=0:\r\n                incremental_revenue = (  60 / incremental_revenue) * 100\r\n            print('incremental_revenue',incremental_revenue)\r\n            initial_sum = data_for_uplift_temp['m1_total_revenue'].sum()\r\n\r\n            if initial_sum == 0 or incremental_revenue ==0:\r\n                 print('either initial_sum or incremental_revenue is 0'  )\r\n                 uplift =0\r\n            else:\r\n                print('initial_sum',initial_sum)\r\n                uplift = (incremental_revenue / initial_sum) * 100\r\n\r\n                uplift = round(uplift, 2)\r\n\r\n            print('uplift',uplift)\r\n            print('current_arpu',current_arpu)\r\n\r\n            predicted_arpu = int(current_arpu + ((current_arpu*uplift)/100))\r\n           \r\n           \r\n            print('predicted_arpu',predicted_arpu)\r\n           \r\n\r\n            if len(data_filter_churners) ==0:\r\n                continue\r\n\r\n            final_df_ls.append(data_filter_churners)\r\n\r\n            # recommended_pid = int(recommenting_product)\r\n            # print('recommended_pid is ',recommended_pid)\r\n            # print('type recommended_pid is ',type(recommended_pid))\r\n\r\n            # print('type next_purchase_date_df is ',next_purchase_date_df.dtypes)\r\n\r\n            # print('len is ', len(weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id'] == recommended_pid]))\r\n            # print(\"weekday is \", weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0])\r\n            # top_purchased_day_1 = weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0]\r\n            # top_purchased_day_2=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_2'].iloc[0]\r\n            # top_purchased_day_3=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_3'].iloc[0]\r\n            \r\n            # print('len of next is is ', len(next_purchase_date_df[next_purchase_date_df['recommended_product_id']==recommended_pid]))\r\n            # next_purchase_date_range=next_purchase_date_df[next_purchase_date_df['recommended_product_id']==recommended_pid]['range'].iloc[0]\r\n\r\n        \r\n            insert_segement_info_churn(item, dag_run_id, data_filter_churners, recommenting_product,recommenting_product_name,\r\n                                      incremental_revenue,uplift,current_arpu,predicted_arpu,total_revenue, db)\r\n        \r\n        final_df=pd.concat(final_df_ls)\r\n        final_df['dag_run_id']=str(dag_run_id)\r\n        final_df=final_df[['msisdn', 'segement_id', 'segement_name', 'trend', 'rfm', 'predictions',\r\n       'predict_proba_0', 'predict_proba_1', 'recommended_product_id',\r\n       'dag_run_id']]\r\n        \r\n        \r\n\r\n        APA_output_path = ml_location+'/{}/output_data/APA_output.csv'.format(dag_run_id)\r\n        APA_output=pd.read_csv(APA_output_path)\r\n\r\n\r\n        APA_output=pd.concat([APA_output,final_df])\r\n        APA_output_path = os.path.join(ml_location, dag_run_id, \"output_data\")\r\n\r\n        APA_output.to_csv(os.path.join(APA_output_path, \"APA_output.csv\"), header=True, index=False)\r\n\r\n        final_df.to_csv(os.path.join(ml_location,dag_run_id,  \"churn_propensity.csv\"),\r\n                                   header=True, index=False)\r\n        ic(\"done outputting churn propensity \")\r\n        \r\n        return final_df\r\n    except Exception as e:\r\n        print(e)\r\n        \r\n        \r\n\r\n\r\ndef transform(dataframe):\r\n    db = get_db()\r\n    df = churn_process(\"manual__2023-07-10T11:06:51\",db)\r\n    return df"
              }
            }, {
              "id": "d388c946-68fa-35e9-5563-95a201376626",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from datetime import datetime, timedelta,date\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.metrics import accuracy_score\r\nfrom imblearn.under_sampling import RandomUnderSampler\r\nfrom sklearn.metrics import classification_report\r\nfrom collections import Counter\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.multioutput import MultiOutputClassifier\r\nfrom sklearn.ensemble import GradientBoostingClassifier\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.linear_model import LogisticRegression\r\nimport xgboost as xgb\r\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\r\nimport pandas as pd\r\nimport pickle\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\nfrom icecream import ic\r\nfrom pathlib import Path\r\nimport dask.array as da\r\nfrom dask.dataframe import merge\r\nimport pickle\r\nimport pathlib\r\nimport dask.dataframe as dd\r\n\r\n\r\nimport pandas as pd\r\nimport pickle\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\n\r\n\r\nfrom sqlalchemy import create_engine\r\nfrom sqlalchemy.ext.declarative import declarative_base\r\nfrom sqlalchemy.orm import sessionmaker\r\nfrom urllib.parse import quote  \r\n\r\n\r\n\r\nfrom sqlalchemy.orm import Session\r\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\r\nfrom sqlalchemy.orm import relationship\r\nimport datetime\r\nfrom sqlalchemy.dialects.mysql import LONGTEXT\r\n\r\n\r\nfrom typing import List, Optional\r\n\r\nfrom pydantic import BaseModel\r\n\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nfrom sklearn.feature_selection import SelectFromModel\r\n\r\n\r\n\r\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\r\n\r\nengine = create_engine(\r\n    SQLALCHEMY_DATABASE_URL,\r\n)\r\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\r\n\r\nBase = declarative_base()\r\n\r\ndef get_db():\r\n    db = SessionLocal()\r\n    try:\r\n        return db\r\n    finally:\r\n        db.close()\r\n        \r\n\r\n\r\n\r\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\r\nsegement_names = ['trend', 'rfm']\r\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\r\npurchase_location = '/home/tnmops/seahorse3_bkp/'\r\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\r\netl_location = '/home/tnmops/seahorse3_bkp/'\r\npack_info_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location = '/home/tnmops/seahorse3_bkp/'\r\nlog_file = '/home/tnmops/seahorse3_bkp/'\r\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\r\n\r\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\r\n\r\nusage_no_months = ['m1', 'm2', 'm3']\r\nrecharge_no_months = ['m1', 'm2', 'm3']\r\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\r\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\nthreshold = 0.50\r\nnot_needed_rfm_segment = ['Lost']\r\n\r\npack_cols = ['product_id','product_type']\r\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\r\n\r\ncampaign_df= ['c4','c3']\r\ncampaign_usage= ['m4','m3']\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_location': self.purchase_location,\r\n        'usage_location': self.etl_location,\r\n        'pack_info_location': self.pack_info_location,\r\n        'ml_location': self.ml_location,\r\n        'recharge_location':self.recharge_location\r\n    }\r\n    \r\n    \r\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\r\n    'recharge_total_cntm2',\r\n    'recharge_total_cntm3',\r\n    'm1_total_voice_usage',\r\n    'm1_total_data_usage',\r\n    'm2_total_voice_usage',\r\n    'm2_total_data_usage',\r\n    'm3_total_voice_usage',\r\n    'm3_total_data_usage',\r\n    'purchase_total_cntm1',\r\n    'purchase_total_cntm2',\r\n    'purchase_total_cntm3', \r\n    'm3_total_revenue_m2_total_revenue_pct_drop',\r\n    'm3_data_revenue_m2_data_revenue_pct_drop',\r\n    'm2_voice_rev_m1_voice_rev_pct_drop',\r\n    'm2_total_revenue_m1_total_revenue_pct_drop',\r\n    'm2_data_revenue_m1_data_revenue_pct_drop',\r\n    'm3_voice_rev_m2_voice_rev_pct_drop',\r\n    'm1_m2_m3_average_voice',\r\n    'm1_m2_m3_average_data', \r\n    'm1_no_of_days',\r\n    'm2_no_of_days',\r\n    'm3_no_of_days',\r\n    'eng_index',\r\n    'consecutive_inactive_days']\r\n\r\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\r\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\r\nMSISDN_COL_NAME = 'msisdn'\r\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\r\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\r\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\r\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \r\nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\r\n\r\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\r\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\r\nTRANSACTION_PRODUCT_NAME = 'product_id'\r\ncampaign_df= ['c4','c3']\r\ncampaign_usage= ['m4','m3']\r\n\r\n\r\nPACK_CONTI_FEATURES = []\r\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\r\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\r\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\r\nPACK_INFO_CATEGORY = 'product_type'\r\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\r\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\r\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\r\nCUSTOMER_CATEG_FEATURES = []\r\nCUSTOMER_CONTI_FEATURES = []\r\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\r\n# 0 index for inbundle 1 index for outbundled  2 index for total\r\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\r\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\r\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\r\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\r\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\r\nCUSTOMER_NEEDED_COLUMN = [\r\n        'onnet_revenue',  'onnet_usage', 'onnet_voice_count', \r\n        'offnet_revenue', 'offnet_usage', 'offnet_voice_count',  \r\n        'idd_revenue', 'idd_usage', 'idd_voice_count',\r\n        'voice_rmg_revenue', 'voice_rmg_usage', 'voice_rmg_count', \r\n        'data_rmg_revenue',  'data_rmg_usage',  \r\n        'data_revenue', 'data_usage', 'sms_revenue', 'sms_usage',  'sms_idd_revenue', \r\n        'sms_idd_usage', 'magik_voice_amount', 'rbt_subscription_rev', 'emergency_credit_rev', \r\n        'package_revenue', 'voice_rev', 'sms_rev', 'onn_rev', 'off_rev', 'total_data_rev', \r\n        'vas_rev', 'vas_rev_others', 'total_revenue', 'total_voice_count', 'total_voice_duration', \r\n        'total_mainaccount_data_usage', 'total_sms_count', 'total_package_count', 'total_other_vas_count', \r\n        'total_voice_usage', 'total_data_usage', 'total_sms_usage'\r\n        ]\r\n\r\n\r\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\r\n\r\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\r\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\r\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\r\n\r\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\r\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\r\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\r\n\r\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\r\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\r\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\r\n\r\nLABEL1 = 'downtrend'\r\nLABEL2 = 'uptrend'\r\nLABEL3 = 'zigzag'\r\nLABEL4 = 'flat'\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_features': self.TRANSACTION_FEATURES,\r\n        'usage_features': self.CUSTOMER_FEATURES,\r\n        'pack_info_features': self.ALL_PACK_FEATURES,\r\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\r\n        'usage_dtypes': self.CUSTOMER_DTYPES\r\n\r\n    }\r\n    \r\n    \r\n\r\nimport logging\r\nimport os\r\ndef get_file_names():\r\n    return {\r\n        \"usage\": {\r\n            \"m1\": \"usage_march.csv\",\r\n            \"m3\": \"usage_feb.csv\",\r\n            \"m4\": \"usage_jan.csv\"},\r\n            \r\n        \"campaign_data\": \r\n        {\r\n            \"c3\": \"campaign_detailed_fct_jan.csv\",\r\n            \"c4\": \"campaign_detailed_fct_dec.csv\"            \r\n                        \r\n        }\r\n        }\r\n        \r\nclass SegmentInformation(Base):\r\n    __tablename__ = \"APA_segment_information_new\"\r\n    id = Column(Integer, primary_key=True, index=True)\r\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\r\n    end_date = Column(DateTime)\r\n    current_product = Column(String(80), nullable=True, unique=False)\r\n    current_products_names = Column(String(200), nullable=True, unique=False)\r\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\r\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\r\n    predicted_arpu = Column(Integer, nullable=True)\r\n    current_arpu = Column(Integer, nullable=True)\r\n    segment_length = Column(String(80), nullable=True, unique=False)\r\n    rule = Column(LONGTEXT, nullable=True)\r\n    actual_rule = Column(LONGTEXT, nullable=True)\r\n    uplift_percent = Column(Float(precision=2), nullable=True)\r\n    incremental_revenue = Column(Float(precision=2), nullable=True)\r\n    campaign_type = Column(String(80), nullable=True, unique=False)\r\n    campaign_name = Column(String(80), nullable=True, unique=False)\r\n    action_key = Column(String(80), nullable=True, unique=False)\r\n    robox_id = Column(String(80), nullable=True, unique=False)\r\n    dag_run_id = Column(String(80), nullable=True, unique=False)\r\n    samples = Column(Integer, nullable=False)\r\n    segment_name = Column(String(80), nullable=True, unique=False)\r\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\r\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\r\n    customer_status = Column(String(80), nullable=True, unique=False)\r\n    query = Column(LONGTEXT, nullable=True, unique=False)\r\n    cluster_no = Column(Integer, nullable=True)\r\n    confidence = Column(Float(precision=2), nullable=True)\r\n    recommendation_type = Column(String(80), nullable=True, unique=False)\r\n    cluster_description = Column(LONGTEXT, nullable=True)\r\n    actual_target_count = Column(Integer, nullable=True)\r\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\r\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\r\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\r\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\r\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\r\n    total_revenue= Column(Integer, nullable=True)\r\n    uplift_revenue=Column(Integer, nullable=True)\r\n\r\n    def __repr__(self):\r\n        return 'SegmentInformation(name=%s)' % self.name\r\n        \r\n        \r\nclass SegementInfo(BaseModel):\r\n    end_date: Optional[str] = None\r\n    dag_run_id: Optional[str] = None\r\n    current_product: Optional[str] = None\r\n    current_products_names: Optional[str] = None\r\n    recommended_product_id: Optional[str] = None\r\n    recommended_product_name: Optional[str] = None\r\n    predicted_arpu: Optional[int] = None\r\n    current_arpu: Optional[int] = None\r\n    segment_length: Optional[str] = None\r\n    rule: Optional[str] = None\r\n    actual_rule: Optional[str] = None\r\n    uplift_percent: Optional[float] = None\r\n    incremental_revenue: Optional[float] = None\r\n    campaign_type: Optional[str] = None\r\n    campaign_name: Optional[str] = None\r\n    action_key: Optional[str] = None\r\n    robox_id: Optional[str] = None\r\n    samples: Optional[int] = None\r\n    segment_name: Optional[str] = None\r\n    current_ARPU_band: Optional[str] = None\r\n    current_revenue_impact: Optional[str] = None\r\n    customer_status: Optional[str] = None\r\n    query: Optional[str] = None\r\n    cluster_no: Optional[int] = None\r\n    confidence: Optional[float] = None\r\n    recommendation_type: Optional[str] = None\r\n    cluster_description: Optional[str] = None\r\n    actual_target_count: Optional[str] = None\r\n    top_purchased_day_1: Optional[str] = None\r\n    top_purchased_day_2: Optional[str] = None\r\n    top_purchased_day_3: Optional[str] = None\r\n    next_purchase_date_range: Optional[str] = None\r\n    campaign_response_percentage: Optional[str] = None\r\n    total_revenue: Optional[int] = None\r\n    uplift_revenue: Optional[int] = None\r\n\r\n\r\n\r\nclass SegementRepo:\r\n    def create(db: Session, segement: SegementInfo):\r\n        db_SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\r\n                                            campaign_type=segement.campaign_type,\r\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\r\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\r\n                                            rule=segement.rule, samples=segement.samples,\r\n                                            campaign_name=segement.campaign_name,\r\n                                            recommended_product_id=segement.recommended_product_id,\r\n                                            recommended_product_name=segement.recommended_product_name,\r\n                                            current_product=segement.current_product,\r\n                                            current_products_names=segement.current_products_names,\r\n                                            segment_length=segement.segment_length,\r\n                                            current_ARPU_band=segement.current_ARPU_band,\r\n                                            current_revenue_impact=segement.current_revenue_impact,\r\n                                            customer_status=segement.customer_status,\r\n                                            segment_name=segement.segment_name, query=segement.query,\r\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\r\n                                            recommendation_type=segement.recommendation_type,\r\n                                            cluster_description=segement.cluster_description,\r\n                                            actual_target_count=segement.actual_target_count,\r\n                                            top_purchased_day_1=segement.top_purchased_day_1,\r\n                                            top_purchased_day_2=segement.top_purchased_day_2,\r\n                                            top_purchased_day_3=segement.top_purchased_day_3,\r\n                                            next_purchase_date_range=segement.next_purchase_date_range,\r\n                                            campaign_response_percentage=segement.campaign_response_percentage,\r\n                                            total_revenue=segement.total_revenue,\r\n                                            uplift_revenue=segement.uplift_revenue\r\n                                            )\r\n        db.add(db_item)\r\n        db.commit()\r\n        db.refresh(db_item)\r\n        return db_item\r\n\r\n    def findByAutoPilotIdAndClusterNo(db: Session, _id, cluster_no):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_no) \\\r\n            .all()\r\n\r\n    def findByAutoPilotIdAndSegementName(db: Session, _id, segement_name, cluster_number):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_number).first()\r\n\r\n    def findByAutoPilotIdAndSegementNameAll(db: Session, _id, segement_name, cluster_number):\r\n        return db.query(SegmentInformation) \\\r\n            .filter(SegmentInformation.dag_run_id == _id) \\\r\n            .filter(SegmentInformation.segment_name == segement_name) \\\r\n            .filter(SegmentInformation.cluster_no == cluster_number).all()\r\n    \r\n    def findByAutoPilotIdAndSegementNamewithoutcluster(db: Session, _id, segement_name):\r\n        return db.query(SegmentInformation) \\\r\n            .filter( SegmentInformation.dag_run_id == _id) \\\r\n            .filter( SegmentInformation.segment_name == segement_name).all()\r\n            \r\n\r\n\r\n    def findByAutoPilotId(db: Session, _id):\r\n        return db.query( SegmentInformation).filter( SegmentInformation.dag_run_id == _id).all()\r\n\r\n    def findByAutoPilotIdRecommendedId(db: Session, _id,recommended_product_id):\r\n        return db.query( SegmentInformation) \\\r\n            .filter( SegmentInformation.dag_run_id == _id) \\\r\n            .filter( SegmentInformation.recommended_product_id == recommended_product_id).all()\r\n\r\n    def deleteById(db: Session, _ids):\r\n        for id in _ids:\r\n            db.query( SegmentInformation).filter( SegmentInformation.id == id).delete()\r\n            db.commit()\r\n\r\n    def update(db: Session, item_data):\r\n        updated_item = db.merge(item_data)\r\n        db.commit()\r\n        return updated_item\r\n        \r\n        \r\ndef getpackprice(product_id, packinfo_df):\r\n    price = 0\r\n    try:\r\n        if (packinfo_df is not None):\r\n            if (product_id in packinfo_df['product_id'].values):\r\n\r\n                price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\r\n                print(\"the price  of \", product_id, \" is \", price)\r\n            else:\r\n                print(\"product_id is not in dataframe  \")\r\n        else:\r\n            print(\"some problem with the  dataframe  \")\r\n\r\n    except Exception as e:\r\n        print(\"error occurred in getpackprice \", e)\r\n\r\n    return price\r\n    \r\n    \r\n    \r\ndef GetCurrentPackprice(product_id, packinfo_df):\r\n    price = 0\r\n    \r\n    try:\r\n        print('product_id in GetCurrentPackprice is ',product_id)\r\n        print('type of product_id in GetCurrentPackprice is ',type(product_id))\r\n        if (packinfo_df is not None):\r\n            if isinstance(product_id, str):\r\n                split = product_id.split(\"|\")\r\n                \r\n                print('split in GetCurrentPackprice is ', split)\r\n                split_length = len(split)\r\n                if split_length > 1:\r\n                    highest = 0\r\n                    for i in range(split_length):\r\n                        product = int(float(split[i]))\r\n                        print('product is ', product)\r\n                        price = packinfo_df[packinfo_df['product_id'] == product].iloc[0][\"price\"]\r\n                        print('price is', price)\r\n                        if price > highest:\r\n                            highest = price\r\n                \r\n                    print(\"the price  of \", product_id, \" is \", price)\r\n                \r\n                else:\r\n                    \r\n                    product_id = int(split[0])\r\n                    \r\n                    price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\r\n                    highest = price\r\n            else:\r\n                print(\"product_id is not in dataframe  \")\r\n        else:\r\n            print(\"some problem with the  dataframe  \")\r\n\r\n    except Exception as e:\r\n        print(\"error occurred in getpackprice \", e)\r\n\r\n    return highest\r\n    \r\n\r\n\r\ndef calculate_percentage_of_ones(lst):\r\n\r\n    print('type of  lst is', type(lst))\r\n    total_count = len(lst)\r\n    ones_count = lst.count(1)\r\n    percentage = (ones_count / total_count) * 100\r\n    return percentage\r\n    \r\n    \r\ndef model_predict(df_test,df_control,usage_jan, dag_run_id):\r\n    \r\n    print(\"model_predict...\")\r\n    \r\n    df_under = pd.concat([df_test, df_control], axis=0)\r\n\r\n    df_under.drop(['TG_Response','CG_Response'], inplace = True,axis =1)\r\n\r\n    df_under =df_under[df_under['msisdn'].isin(usage_jan['msisdn'])]\r\n    df_under1 = df_under.sample(n=100000)\r\n    final = pd.merge(usage_jan,df_under,how='inner',on='msisdn')\r\n\r\n\r\n\r\n    cat_col = final.select_dtypes(include =['object','category'] ).columns.to_list()\r\n    msisdn = final.pop(\"msisdn\")\r\n    groups_test = final['action_group']\r\n    final.drop(columns=cat_col, inplace = True)\r\n\r\n    # load model from file\r\n    path = os.path.join(ml_location, dag_run_id, \"campaign_xgmodel.pickle\")\r\n    xgb_model = pickle.load(open(path, \"rb\"))\r\n\r\n    X=final.drop(['response'],axis=1)\r\n    y=final['response']  \r\n    \r\n\r\n    y_pred_full = xgb_model.predict(X)\r\n    print(classification_report(y, y_pred_full))#without random classifier\r\n\r\n    X['y_pred_full']=list(y_pred_full)\r\n    X['msisdn']=list(msisdn)\r\n    X=X[['msisdn','y_pred_full']]\r\n\r\n    X.to_csv(os.path.join(ml_location,dag_run_id,'campaign_response_model_predicted.csv'),header=True,index=False)\r\n    print(\"model_predict is completed...\") \r\n    #percentage_of_ones = calculate_percentage_of_ones(list(y_pred_full))\r\n\r\n    #return percentage_of_ones\r\n    \r\n    \r\ndef uplift_data(X_full):\r\n    # Initialize lists to store results\r\n    products = []\r\n    real_uplifts = []\r\n    predicted_uplifts = []\r\n\r\n    # Get unique products\r\n    unique_products = X_full['Product_Promoted'].unique()\r\n\r\n    # Calculate real and predicted uplift for each product\r\n    for product in unique_products:\r\n        # Get data for this product\r\n        df_product = X_full[X_full['Product_Promoted'] == product]\r\n\r\n        # Calculate real uplift\r\n        real_uplift = df_product[df_product['action_group'] == 'TEST_GROUP']['response'].mean() - df_product[df_product['action_group'] == 'CONTROL_GROUP']['response'].mean()\r\n\r\n        # Calculate predicted uplift\r\n        predicted_uplift = df_product[df_product['action_group'] == 'TEST_GROUP']['predicted_prob'].mean() - df_product[df_product['action_group'] == 'CONTROL_GROUP']['predicted_prob'].mean()\r\n\r\n        # Append results\r\n        products.append(product)\r\n        real_uplifts.append(real_uplift)\r\n        predicted_uplifts.append(predicted_uplift)\r\n\r\n    # Store results in a DataFrame\r\n    uplift_df = pd.DataFrame({\r\n        'Product_Promoted': products,\r\n        'Real_Uplift': real_uplifts,\r\n        'Predicted_Uplift': predicted_uplifts\r\n    })\r\n    \r\n    return uplift_df\r\n    \r\n    \r\ndef xgboost_model(final,dag_run_id):\r\n    \r\n    X=final.drop(['response'],axis=1)\r\n    y=final['response']\r\n    \r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\r\n    \r\n    xgb_model = xgb.XGBClassifier().fit(X_train, y_train)\r\n    \r\n       \r\n    \r\n    path = os.path.join(ml_location,dag_run_id, \"campaign_xgmodel.pickle\") \r\n\r\n    pickle.dump(xgb_model, open(path, 'wb'))\r\n    \r\n    \r\n    print('Accuracy of XGB classifier on training set: {:.2f}'\r\n           .format(xgb_model.score(X_train, y_train)))\r\n    print('Accuracy of XGB classifier on test set: {:.2f}'\r\n           .format(xgb_model.score(X_test[X_train.columns], y_test)))\r\n    \r\n    y_pred = xgb_model.predict(X_test)\r\n    \r\n    print(classification_report(y_test, y_pred))\r\n    \r\n    \r\ndef model_fit(df_test, df_control, usage_jan, dag_run_id): \r\n    \r\n    print(\"model_fit started\")\r\n    \r\n    print(type(df_test['TG_Response'].sum()))\r\n    \r\n    # Perform undersampling within 'TEST_GROUP' only\r\n#     df_test_under = df_test[df_test['TG_Response'] == 0].sample(df_test['TG_Response'].sum(), random_state=42)\r\n    df_test_under = df_test[df_test['TG_Response'] == 0].sample(int(df_test['TG_Response'].sum()), random_state=42)\r\n\r\n\r\n    # Concatenate undersampled class 0 DataFrame with original class 1 DataFrame within 'TEST_GROUP'\r\n    df_test_under = pd.concat([df_test_under, df_test[df_test['TG_Response'] == 1]], axis=0)\r\n\r\n    # Combine the undersampled 'TEST_GROUP' DataFrame with the original 'CONTROL_GROUP' DataFrame\r\n    \r\n    df_under = pd.concat([df_test_under, df_control], axis=0)  \r\n   \r\n    \r\n    df_under.drop(['TG_Response','CG_Response'], inplace = True,axis =1)\r\n    \r\n    df_under_test_group = df_under[df_under[\"action_group\"] == \"TEST_GROUP\"]\r\n    \r\n    final_all_groups = pd.merge(usage_jan,df_under,how='inner',on='msisdn')\r\n    final = pd.merge(usage_jan,df_under_test_group,how='inner',on='msisdn')\r\n    \r\n    final.fillna(0,inplace=True)\r\n    final_all_groups.fillna(0,inplace=True)   \r\n    \r\n    cat_col = final.select_dtypes(include =['object','category'] ).columns.to_list()\r\n    msisdn = final.pop(\"msisdn\")\r\n    groups_test = final['action_group']\r\n    final.drop(columns=cat_col, inplace = True)\r\n    \r\n    msisdn_all = final_all_groups.pop(\"msisdn\")\r\n    groups = final_all_groups['action_group']\r\n    final_all_groups.drop(columns=cat_col, inplace = True)\r\n    \r\n    \r\n    xgboost_model(final,dag_run_id)   \r\n    \r\n    #full data prediction \r\n    X_full=final_all_groups.drop(['response'],axis=1)\r\n    y_full=final_all_groups['response']    \r\n    \r\n    \r\n    path = os.path.join(ml_location,dag_run_id,  \"campaign_xgmodel.pickle\")\r\n    xgb_model = pickle.load(open(path, \"rb\"))\r\n    \r\n    \r\n    y_pred_prob_full = xgb_model.predict_proba(X_full)\r\n    X_full['predicted_prob'] =  y_pred_prob_full[:,1]\r\n    \r\n    \r\n    X_full['action_group']  =  groups\r\n    X_full['msisdn']  =  msisdn_all\r\n    X_full['response'] = y_full\r\n    \r\n    uplift_df = uplift_data(X_full)    \r\n    \r\n    df_product = X_full[X_full['Product_Promoted'] == 5.0]    \r\n    uplift_df['Difference_Percentage'] = ((uplift_df['Predicted_Uplift'] - uplift_df['Real_Uplift']) / uplift_df['Real_Uplift']) * 100\r\n    \r\n    path = os.path.join(ml_location,dag_run_id,  \"campaign_uplift_model.csv\")\r\n    uplift_df.to_csv(path, index=False)\r\n    \r\n    print(\"model_fit completed\")\r\n    \r\n\r\ndef campaign_response_model(dag_run_id, db):\r\n    try:\r\n        c=0\r\n        \r\n        file_name_dict = get_file_names()\r\n\r\n        print('inside campaign_response_model started')\r\n        pack_info = pd.read_csv(os.path.join(pack_info_location, \"pack_info.csv\"))\r\n        for i, j in zip(campaign_df, campaign_usage):\r\n        \r\n            jan = pd.read_csv(os.path.join(etl_location, file_name_dict.get(\"campaign_data\").get(i)))    \r\n            usage_jan= pd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(j)))\r\n            jan_filter = jan[['msisdn','action_group','TG_Response','CG_Response','Product_Promoted']]\r\n            jan_filter.dropna(inplace = True)\r\n\r\n\r\n\r\n            jan_filter.loc[:, 'TG_Response'] = np.where(jan_filter['TG_Response']>0 , 1, jan_filter['TG_Response'])\r\n            jan_filter.loc[:, 'CG_Response'] = np.where(jan_filter['CG_Response']>0 , 1, jan_filter['CG_Response'])\r\n\r\n\r\n            # Separate DataFrame based on 'action_group'\r\n            df_test = jan_filter[jan_filter['action_group'] == 'TEST_GROUP']\r\n            df_control = jan_filter[jan_filter['action_group'] == 'CONTROL_GROUP']\r\n\r\n\r\n            df_test['response'] = df_test['TG_Response'] \r\n            df_control['response'] = df_control['CG_Response'] \r\n            print('adding to  table')\r\n            if c==0:        \r\n                model_fit(df_test,df_control, usage_jan, dag_run_id)        \r\n                c=1\r\n\r\n            else:\r\n                model_predict(df_test,df_control,usage_jan, dag_run_id)\r\n        campaign_response_model_predicted=pd.read_csv(os.path.join(ml_location,dag_run_id,'campaign_response_model_predicted.csv'))\r\n        segements_two = SegementRepo.findByAutoPilotId(db=db, _id=dag_run_id)\r\n        for seg in segements_two:\r\n            item = seg.segment_name\r\n            segements_data = item.split(\"-\")\r\n            trent = segements_data[0]\r\n            rfm = segements_data[1]\r\n            service = segements_data[2]\r\n            filename = trent+\"-\"+rfm+\"-\"+service+\".csv\"\r\n            print('filename is ',filename)\r\n            segment_df_path = os.path.join(ml_location, dag_run_id, filename)\r\n            segment_df = pd.read_csv(segment_df_path)\r\n            campaign_response_model_predicted1=campaign_response_model_predicted[campaign_response_model_predicted['msisdn'].isin(segment_df['msisdn'])]\r\n            print('len(campaign_response_model_predicted1)',len(campaign_response_model_predicted1))\r\n            print(\"len(usage_jan)\",len(usage_jan))\r\n            if(len(campaign_response_model_predicted1) == 0 or campaign_response_model_predicted1 is None ):\r\n                perc=0\r\n                print(f\"for item{item} the campaign_response_percentage is {perc}\")\r\n            else:\r\n                perc = calculate_percentage_of_ones(list(campaign_response_model_predicted1['y_pred_full']))\r\n                print(f\"for item{item} the campaign_response_percentage is {perc}\")\r\n            seg.campaign_response_percentage=perc\r\n            campaign_type = seg.campaign_type\r\n            if campaign_type == \"upsell\" or campaign_type == \"crossell\":\r\n                current_product_id = seg.current_product\r\n                rec_product_id = int(seg.recommended_product_id)\r\n                current_product_price = GetCurrentPackprice(current_product_id, pack_info)\r\n                reco_product_price = getpackprice(rec_product_id,pack_info)\r\n                prodd_diff= reco_product_price-current_product_price\r\n\r\n                actual_target_count = seg.actual_target_count\r\n                total_revenue = seg.total_revenue\r\n                uplift_revenue= seg.uplift_revenue\r\n                current_arpu=  total_revenue/actual_target_count\r\n                seg.current_arpu =current_arpu\r\n                predicted_arpu = current_arpu+uplift_revenue \r\n                seg.predicted_arpu = predicted_arpu\r\n                seg.incremental_revenue =prodd_diff *(actual_target_count*perc/100)\r\n                seg.uplift_percent = ((predicted_arpu-current_arpu)/current_arpu)*100\r\n            \r\n            SegementRepo.update(db=db, item_data=seg)\r\n\r\n                    \r\n        print(\"campaign response compleated\")\r\n           \r\n    except Exception as e:\r\n        print(e)\r\n        \r\n        \r\n\r\n\r\ndef transform(dataframe):\r\n    db = get_db()\r\n    campaign_response_model(\"manual__2023-07-10T11:06:51\",db)\r\n    return dataframe"
              }
            }, {
              "id": "e5e91f6d-c6b6-3cdc-112a-ecc8a1a7bff8",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nimport pickle\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\nfrom urllib.parse import quote  \r\nfrom datetime import datetime\r\nfrom icecream import ic\r\nfrom pathlib import Path\r\nimport dask.array as da\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nfrom sklearn.feature_selection import SelectKBest, chi2\r\nfrom sklearn.feature_selection import SelectKBest, f_regression\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.feature_selection import SelectFromModel\r\nfrom sklearn.tree import DecisionTreeClassifier\r\n\r\n\r\n\r\nsegement_names = ['trend', 'rfm']\r\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\r\npurchase_location = '/home/tnmops/seahorse3_bkp/'\r\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\r\netl_location = '/home/tnmops/seahorse3_bkp/'\r\npack_info_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location = '/home/tnmops/seahorse3_bkp/'\r\nlog_file = '/home/tnmops/seahorse3_bkp/'\r\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\r\n\r\n\r\ndef write_pickle(data, path):\r\n    with open(path, 'wb') as handle:\r\n        print(f'opened {path} ')\r\n        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n        print(f'data dumped  {data}')\r\n        \r\n\r\n\r\n\r\ndef get_features(df):\r\n    numerical_cols = df.select_dtypes(include=[np.number]).columns\r\n    df=df[numerical_cols]\r\n    X = df.drop(['msisdn', 'label'], axis=1)\r\n    y = df['label']\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n    print(f\"X_train.shape = {X_train.shape}, X_test.shape =  {X_test.shape}\")\r\n    # sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\r\n    sel = SelectFromModel(DecisionTreeClassifier(max_leaf_nodes=12))\r\n    sel.fit(X_train, y_train)\r\n    sel.get_support()\r\n    features = X_train.columns[sel.get_support()]\r\n    print(\"the feature selection features are \", features)\r\n    selected_features = [feature for feature in features if feature != 'label']\r\n    \r\n    return selected_features\r\n    \r\n    \r\n\r\n\r\ndef feature_selection(dag_run_id):\r\n    try:\r\n        path_d = os.path.join(ml_location, dag_run_id, \"dict.pickle\")\r\n        features_path = os.path.join(ml_location, dag_run_id, \"features.pickle\")\r\n        with open(path_d, 'rb') as handle:\r\n            data = pickle.load(handle)\r\n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\r\n        # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\r\n        \r\n        \r\n        # for key, value in data.items():\r\n        #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\r\n            \r\n        filtered_dict = {k: v for k, v in data.items()}\r\n        result_features = {}\r\n        for item, val in filtered_dict.items():\r\n            # val is the path item is the segment name\r\n            df = pd.read_csv(val)\r\n            if len(df)<50:\r\n                continue\r\n            features = get_features(df)\r\n            result_features[item] = features\r\n            print(f\"the features are {features} for the segement {item}\")\r\n\r\n        write_pickle(data=result_features, path=features_path)\r\n        return df\r\n    except Exception as e:\r\n        print(e)\r\n    \r\ndef transform(dataframe):\r\n    df = feature_selection(\"manual__2023-07-10T11:06:51\" )\r\n    return df"
              }
            }, {
              "id": "9a819e7e-4d3c-8efe-16ef-50ffdf3c5516",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from sklearn.metrics import accuracy_score\r\nfrom sklearn.metrics import classification_report\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.feature_selection import SelectFromModel\r\nimport sklearn\r\nimport json\r\nimport dask.dataframe as dd\r\nimport os\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\nfrom icecream import ic\r\nimport dask.dataframe as dd\r\nimport os\r\nfrom kmodes.kmodes import KModes\r\n# import configuration.config as cfg\r\n# import configuration.features as f\r\nimport traceback\r\nimport numpy as np\r\n# from fastapi import Depends, FastAPI, HTTPException\r\nfrom pathlib import Path\r\nimport pickle\r\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\r\nfrom pathlib import Path\r\nimport requests\r\n# from sql_app.repositories import AssociationRepo\r\nimport pandas as pd\r\nimport pickle\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\n\r\n\r\nfrom urllib.parse import quote  \r\n\r\n\r\n\r\n\r\n\r\nfrom typing import List, Optional\r\n\r\nfrom pydantic import BaseModel\r\n\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nfrom sklearn.feature_selection import SelectFromModel\r\n\r\n\r\nsegement_names = ['trend', 'rfm']\r\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\r\npurchase_location = '/home/tnmops/seahorse3_bkp/'\r\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\r\netl_location = '/home/tnmops/seahorse3_bkp/'\r\npack_info_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location = '/home/tnmops/seahorse3_bkp/'\r\nlog_file = '/home/tnmops/seahorse3_bkp/'\r\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\r\n\r\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\r\n\r\nusage_no_months = ['m1', 'm2', 'm3']\r\nrecharge_no_months = ['m1', 'm2', 'm3']\r\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\r\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\nthreshold = 0.50\r\nnot_needed_rfm_segment = ['Lost']\r\n\r\npack_cols = ['product_id','product_type']\r\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\r\n\r\ncampaign_df= ['c4','c3']\r\ncampaign_usage= ['m4','m3']\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_location': self.purchase_location,\r\n        'usage_location': self.etl_location,\r\n        'pack_info_location': self.pack_info_location,\r\n        'ml_location': self.ml_location,\r\n        'recharge_location':self.recharge_location\r\n    }\r\n    \r\n    \r\n\r\n\r\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\r\n    'recharge_total_cntm2',\r\n    'recharge_total_cntm3',\r\n    'm1_total_voice_usage',\r\n    'm1_total_data_usage',\r\n    'm2_total_voice_usage',\r\n    'm2_total_data_usage',\r\n    'm3_total_voice_usage',\r\n    'm3_total_data_usage',\r\n    'purchase_total_cntm1',\r\n    'purchase_total_cntm2',\r\n    'purchase_total_cntm3', \r\n    'm3_total_revenue_m2_total_revenue_pct_drop',\r\n    'm3_data_revenue_m2_data_revenue_pct_drop',\r\n    'm2_voice_rev_m1_voice_rev_pct_drop',\r\n    'm2_total_revenue_m1_total_revenue_pct_drop',\r\n    'm2_data_revenue_m1_data_revenue_pct_drop',\r\n    'm3_voice_rev_m2_voice_rev_pct_drop',\r\n    'm1_m2_m3_average_voice',\r\n    'm1_m2_m3_average_data', \r\n    'm1_no_of_days',\r\n    'm2_no_of_days',\r\n    'm3_no_of_days',\r\n    'eng_index',\r\n    'consecutive_inactive_days']\r\n\r\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\r\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\r\nMSISDN_COL_NAME = 'msisdn'\r\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\r\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\r\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\r\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \r\nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\r\n\r\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\r\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\r\nTRANSACTION_PRODUCT_NAME = 'product_id'\r\n\r\npack_name = \"product_name\"\r\nPACK_CONTI_FEATURES = []\r\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\r\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\r\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\r\nPACK_INFO_CATEGORY = 'product_type'\r\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\r\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\r\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\r\nCUSTOMER_CATEG_FEATURES = []\r\nCUSTOMER_CONTI_FEATURES = []\r\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\r\n# 0 index for inbundle 1 index for outbundled  2 index for total\r\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\r\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\r\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\r\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\r\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\r\nCUSTOMER_NEEDED_COLUMN = [\r\n'onnet_revenue',\r\n'onnet_usage_da',\r\n'onnet_usage',\r\n'onnet_voice_count',\r\n'onnet_da_revenue',\r\n'offnet_revenue',\r\n'offnet_usage_da',\r\n'offnet_usage',\r\n'offnet_voice_count',\r\n'offnet_da_revenue',\r\n'idd_revenue',\r\n'idd_usage_da',\r\n'idd_usage',\r\n'idd_voice_count',\r\n'idd_da_revenue',\r\n'voice_rmg_revenue',\r\n'voice_rmg_usage_da',\r\n'voice_rmg_usage',\r\n'voice_rmg_count',\r\n'voice_rmg_da_revenue',\r\n'data_rmg_revenue',\r\n'data_rmg_usage_da',\r\n'data_rmg_usage',\r\n'data_rmg_da_revenue',\r\n'data_revenue',\r\n'data_usage_da',\r\n'data_usage',\r\n'da_data_rev',\r\n'sms_revenue',\r\n'sms_usage_da',\r\n'sms_usage',\r\n'sms_da_revenue',\r\n'sms_idd_revenue',\r\n'sms_idd_usage_da',\r\n'sms_idd_usage',\r\n'sms_idd_da_revenue',\r\n'magik_voice_amount',\r\n'rbt_subscription_rev',\r\n'emergency_credit_rev',\r\n'package_revenue',\r\n'voice_rev',\r\n'sms_rev',\r\n'onn_rev',\r\n'off_rev',\r\n'total_data_rev',\r\n'vas_rev',\r\n'vas_rev_others',\r\n'total_revenue',\r\n'total_voice_count',\r\n'total_voice_duration',\r\n'total_mainaccount_data_usage',\r\n'total_sms_count',\r\n'total_package_count',\r\n'total_other_vas_count',\r\n'total_voice_usage',\r\n'total_data_usage',\r\n'total_sms_usage']\r\n\r\n\r\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\r\n\r\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\r\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\r\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\r\n\r\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\r\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\r\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\r\n\r\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\r\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\r\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\r\n\r\nLABEL1 = 'downtrend'\r\nLABEL2 = 'uptrend'\r\nLABEL3 = 'zigzag'\r\nLABEL4 = 'flat'\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_features': self.TRANSACTION_FEATURES,\r\n        'usage_features': self.CUSTOMER_FEATURES,\r\n        'pack_info_features': self.ALL_PACK_FEATURES,\r\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\r\n        'usage_dtypes': self.CUSTOMER_DTYPES\r\n\r\n    }\r\n    \r\n    \r\n\r\n\r\ndef perform_k_modes(df, path, dag_run_id):\r\n    try:\r\n        # K_modes_location = os.path.join(ml_location, dag_run_id, \"kmodes\")\r\n        # Path(K_modes_location).mkdir(parents=True, exist_ok=True)\r\n        objList = df.select_dtypes(include=\"object\").columns\r\n        X = df[objList]\r\n        X = pd.get_dummies(X)\r\n\r\n        # elbow = find_k_kmodes(X=X)\r\n\r\n        km = KModes(n_clusters=2)\r\n        km.fit(X)\r\n        X['label'] = km.labels_\r\n        df['label'] = km.labels_\r\n        if df['label'].unique() is None or df['label'][0] is None :\r\n             df['label'] =0\r\n\r\n        print(df['label'].value_counts())\r\n        df.to_csv(path, header=True, index=False)\r\n        print(f\"outputed to path {path}\")\r\n        # for label in df['label'].unique():\r\n        #     df_temp = df[df['label'] == label]\r\n        #     name_path = f\"{df_name}_cluster_{label}.csv\"\r\n        #     output_path = os.path.join(K_modes_location, dag_run_id, name_path)\r\n        #     name_list[name_path] = output_path\r\n        #     print(f\"the output path is \", output_path)\r\n        #     df_temp.to_csv(output_path, header=True, index=False)\r\n        #     print(f\"done exporting \")\r\n\r\n        return df\r\n    except Exception as e:\r\n        print(e)\r\n        \r\n\r\n\r\n\r\n\r\ndef k_modes(dag_run_id):\r\n    try:\r\n        path_d = os.path.join(ml_location,dag_run_id, \"dict.pickle\")\r\n        path_dd = os.path.join(ml_location,dag_run_id, \"cluster_analysis.csv\")\r\n        with open(path_d, 'rb') as handle:\r\n            data = pickle.load(handle)\r\n        \r\n        \r\n#         for key, value in data_dict.items():\r\n#             data_dict[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\r\n\r\n        # for key, value in data.items():\r\n        #     if value is None:\r\n        #         continue\r\n        #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\r\n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\r\n        # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\r\n        filtered_dict = {k: v for k, v in data.items()}\r\n        data_list = []\r\n\r\n        for item, val in filtered_dict.items():\r\n            # val is the path item is the segment name\r\n            df = pd.read_csv(val)\r\n            print(f\"the cluster is {item}\")\r\n            val = perform_k_modes(df, val, dag_run_id)\r\n            val['segement_name'] = item\r\n            data_list.append(val)\r\n\r\n        df_final = pd.concat(data_list)\r\n        rfm_path = os.path.join(ml_location,dag_run_id, \"rfm\")\r\n        rfm_dd = dd.read_parquet(rfm_path)\r\n        rfm_df=rfm_dd.compute()\r\n\r\n        df_final1=df_final[['msisdn','trend','Segment','label']]\r\n        df_final1=pd.merge(df_final1,rfm_df[['msisdn','Recency'\t,'Revenue'\t,'Frequency',\t'R_Score',\t'F_Score',\t'M_Score',\t'RFM_Segment']],\r\n                           on = 'msisdn',how = 'left')\r\n                           \r\n\r\n        df_final1['dag_run_id']=str(dag_run_id)\r\n        df_final1.to_csv(os.path.join(ml_location,dag_run_id, \"rfm_segement_mode.csv\"), index=False, header=True)\r\n#         for df_chunk in pd.read_csv(os.path.join(ml_location,dag_run_id, \"rfm_segement_mode.csv\"),\r\n#                                     chunksize=10000):\r\n#             # insert each chunk into database table\r\n#             df_chunk.to_sql('APA_rfm_segement_mode', engine, if_exists='append', index=False)\r\n\r\n        df_final = df_final.groupby(['segement_name', 'label']).agg({\"msisdn\": \"count\"}).reset_index()\r\n        df_final['dag_run_id']=str(dag_run_id)\r\n        \r\n        df_final.to_csv(path_dd,index=False,header = True)\r\n#         for df_chunk in pd.read_csv(os.path.join(ml_location,dag_run_id, \"cluster_analysis.csv\"),chunksize=10000):\r\n#             # insert each chunk into database table\r\n#             df_chunk.to_sql('APA_rfm_segement_mode_groupby', engine, if_exists='append', index=False)\r\n        \r\n        return df_final\r\n\r\n\r\n    except Exception as e:\r\n        print(e)\r\n        \r\n        \r\n\r\n\r\n\r\n\r\ndef transform(dataframe):\r\n    df = k_modes(\"manual__2023-07-10T11:06:51\")\r\n    return df"
              }
            }, {
              "id": "23865ebd-df86-3eb2-157b-15f3351e784d",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nimport sklearn\nimport json\nimport dask.dataframe as dd\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\n# import configuration.config as cfg\n# import configuration.features as f\nimport traceback\nimport numpy as np\nfrom fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom pathlib import Path\nimport requests\n# from sql_app.repositories import AssociationRepo\n\n# config = cfg.Config().to_json()\n# features = f.Features().to_json()\n\n\nimport pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom urllib.parse import quote  \n\n\n\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\nfrom sqlalchemy.orm import relationship\nimport datetime\nfrom sqlalchemy.dialects.mysql import LONGTEXT\n\n\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\n\n\n\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\npurchase_location = '/home/tnmops/seahorse3_bkp/'\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\netl_location = '/home/tnmops/seahorse3_bkp/'\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nlog_file = '/home/tnmops/seahorse3_bkp/'\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\n\n\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\n\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URL,\n)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        return db\n    finally:\n        db.close()\n\n\n\nclass SegmentInformation(Base):\n    __tablename__ = \"APA_segment_information_new\"\n    id = Column(Integer, primary_key=True, index=True)\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\n    end_date = Column(DateTime)\n    current_product = Column(String(80), nullable=True, unique=False)\n    current_products_names = Column(String(200), nullable=True, unique=False)\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\n    predicted_arpu = Column(Integer, nullable=True)\n    current_arpu = Column(Integer, nullable=True)\n    segment_length = Column(String(80), nullable=True, unique=False)\n    rule = Column(LONGTEXT, nullable=True)\n    actual_rule = Column(LONGTEXT, nullable=True)\n    uplift_percent = Column(Float(precision=2), nullable=True)\n    incremental_revenue = Column(Float(precision=2), nullable=True)\n    campaign_type = Column(String(80), nullable=True, unique=False)\n    campaign_name = Column(String(80), nullable=True, unique=False)\n    action_key = Column(String(80), nullable=True, unique=False)\n    robox_id = Column(String(80), nullable=True, unique=False)\n    dag_run_id = Column(String(80), nullable=True, unique=False)\n    samples = Column(Integer, nullable=False)\n    segment_name = Column(String(80), nullable=True, unique=False)\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\n    customer_status = Column(String(80), nullable=True, unique=False)\n    query = Column(LONGTEXT, nullable=True, unique=False)\n    cluster_no = Column(Integer, nullable=True)\n    confidence = Column(Float(precision=2), nullable=True)\n    recommendation_type = Column(String(80), nullable=True, unique=False)\n    cluster_description = Column(LONGTEXT, nullable=True)\n    actual_target_count = Column(Integer, nullable=True)\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\n    total_revenue= Column(Integer, nullable=True)\n    uplift_revenue=Column(Integer, nullable=True)\n    uplift=Column(Integer, nullable=True)\n\n    def __repr__(self):\n        return 'SegmentInformation(name=%s)' % self.name\n\n\n\n\nclass SegementInfo(BaseModel):\n    end_date: Optional[str] = None\n    dag_run_id: Optional[str] = None\n    current_product: Optional[str] = None\n    current_products_names: Optional[str] = None\n    recommended_product_id: Optional[str] = None\n    recommended_product_name: Optional[str] = None\n    predicted_arpu: Optional[int] = None\n    current_arpu: Optional[int] = None\n    segment_length: Optional[str] = None\n    rule: Optional[str] = None\n    actual_rule: Optional[str] = None\n    uplift_percent: Optional[float] = None\n    incremental_revenue: Optional[float] = None\n    campaign_type: Optional[str] = None\n    campaign_name: Optional[str] = None\n    action_key: Optional[str] = None\n    robox_id: Optional[str] = None\n    samples: Optional[int] = None\n    segment_name: Optional[str] = None\n    current_ARPU_band: Optional[str] = None\n    current_revenue_impact: Optional[str] = None\n    customer_status: Optional[str] = None\n    query: Optional[str] = None\n    cluster_no: Optional[int] = None\n    confidence: Optional[float] = None\n    recommendation_type: Optional[str] = None\n    cluster_description: Optional[str] = None\n    actual_target_count: Optional[str] = None\n    top_purchased_day_1: Optional[str] = None\n    top_purchased_day_2: Optional[str] = None\n    top_purchased_day_3: Optional[str] = None\n    next_purchase_date_range: Optional[str] = None\n    campaign_response_percentage: Optional[str] = None\n    total_revenue: Optional[int] = None\n    uplift_revenue: Optional[int] = None\n    uplift: Optional[float] = None\n    \n    \n\n\n\nclass SegementRepo:\n    def create(db: Session, segement: SegementInfo):\n        db_item = SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\n                                            campaign_type=segement.campaign_type,\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\n                                            rule=segement.rule, samples=segement.samples,\n                                            campaign_name=segement.campaign_name,\n                                            recommended_product_id=segement.recommended_product_id,\n                                            recommended_product_name=segement.recommended_product_name,\n                                            current_product=segement.current_product,\n                                            current_products_names=segement.current_products_names,\n                                            segment_length=segement.segment_length,\n                                            current_ARPU_band=segement.current_ARPU_band,\n                                            current_revenue_impact=segement.current_revenue_impact,\n                                            customer_status=segement.customer_status,\n                                            segment_name=segement.segment_name, query=segement.query,\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\n                                            recommendation_type=segement.recommendation_type,\n                                            cluster_description=segement.cluster_description,\n                                            actual_target_count=segement.actual_target_count,\n                                            top_purchased_day_1=segement.top_purchased_day_1,\n                                            top_purchased_day_2=segement.top_purchased_day_2,\n                                            top_purchased_day_3=segement.top_purchased_day_3,\n                                            next_purchase_date_range=segement.next_purchase_date_range,\n                                            campaign_response_percentage=segement.campaign_response_percentage,\n                                            total_revenue=segement.total_revenue,\n                                            uplift_revenue=segement.uplift_revenue,\n                                            uplift=segement.uplift\n                                            )\n        db.add(db_item)\n        db.commit()\n        db.refresh(db_item)\n        return db_item\n\n    def findByAutoPilotIdAndClusterNo(db: Session, _id, cluster_no):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.cluster_no == cluster_no) \\\n            .all()\n\n    def findByAutoPilotIdAndSegementName(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).first()\n\n    def findByAutoPilotIdAndSegementNameAll(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).all()\n    \n    def findByAutoPilotIdAndSegementNamewithoutcluster(db: Session, _id, segement_name):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name).all()\n            \n\n\n    def findByAutoPilotId(db: Session, _id):\n        return db.query(SegmentInformation).filter(SegmentInformation.dag_run_id == _id).all()\n\n    def findByAutoPilotIdRecommendedId(db: Session, _id,recommended_product_id):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.recommended_product_id == recommended_product_id).all()\n\n    def deleteById(db: Session, _ids):\n        for id in _ids:\n            db.query(SegmentInformation).filter(SegmentInformation.id == id).delete()\n            db.commit()\n\n    def update(db: Session, item_data):\n        updated_item = db.merge(item_data)\n        db.commit()\n        return updated_item\n\n\n\n\n\nclass RuleExtreaction(object):\n    def __init__(self, dag_run_id=None, features_path=None, path_d=None, db=None):\n        self.path_d = path_d\n        self.dag_run_id = dag_run_id\n        self.features_path = features_path\n        self.db = db\n\n        # going to load this varibles\n        self.data = None\n        self.filtered_dict = None\n        self.data_feature = None\n        # loading\n\n        self.load_pickle()\n        self.filter_dict()\n\n    def load_pickle(self):\n        self.data = load_picke_file(self.path_d)\n        self.data_feature = load_picke_file(self.features_path)\n        \n            \n            \n    def filter_dict(self):\n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\n        # self.filtered_dict = {k: v for k, v in self.data.items() if k in needed_segements}\n        self.filtered_dict = {k: v for k, v in self.data.items()}\n\n    def execute(self):\n        for item, val in self.filtered_dict.items():\n            # val is the path item is the segment name\n            df = pd.read_csv(val)\n            if(len(df) < 50):\n                continue\n            print('len====',len(df))\n            print(f\"the cluster is {item}\")\n            cluster_conditions = extract_rules(df, self.data_feature[item])\n            \n            print('cluster_conditions is ',cluster_conditions)\n            if cluster_conditions is None:\n                print(f\" the segment {item} is none ,  the path is {val}\")\n            for cluster, rule in cluster_conditions.items():\n                try:\n\n                    info = SegementInfo()\n                    # info.actual_rule\n                    info.dag_run_id = self.dag_run_id\n                    info.segment_name = f\"{item}-{str(cluster)}\"\n                    info.segment_length = str(len(df))\n                    info.customer_status = \"active\"\n                    info.cluster_description = rule\n                    info.cluster_no = int(cluster)\n                    if len(rule) > 1:\n                        #info.samples = len(df.query(rule))\n                        info.samples = 15\n                    else:\n                        info.samples = 0\n                    SegementRepo.create(db=self.db, segement=info)\n                except Exception as e:\n                    print(\" error occorured in rule insertion \")\n                    print(e)\n                    raise ValueError(e)\n\n\n\n\n\n\ndef load_picke_file(filename):\n    with open(filename, 'rb') as handle:\n        data = pickle.load(handle)\n    return data\n\n\n\ndef extract_rules(df, features):\n    \"\"\"\n\n    @rtype: dictonatu of cluster and ther rule \n    \"\"\"\n    try:\n        \n        features.append('label')    \n        r1 =df[features]\n        \n        for c in r1['label'].unique(): \n            cluster_conditions={}\n            op_li=[]\n            cluster_conditions[c] = [] \n            r = r1[r1['label'] == c]        \n            for i in r.columns:\n                if i!='label':                \n                    if r[i].min() == r[i].max():\n                        str1 =  i + '==' + str(int(r[i].min()))\n                    else:\n                        str1 = i + '>=' + str(int(r[i].min()) )+ ' & ' + i + '<=' + str(int(r[i].max()) )\n                \n                op_li.append(str1)       \n                condition = \" & \".join(op_li)            \n                cluster_conditions[c] = condition\n            return cluster_conditions\n            # cluster_conditions = {}\n            # df1 = df[features + [\"label\"]]\n            # cluster_modes = {}\n            # for cluster in df1['label'].unique():\n            #     temp = df1[df1['label'] == cluster]\n            #     cluster_modes[cluster]='test'\n            # for cluster, modes in cluster_modes.items():\n            #     cluster_conditions[int(cluster)] = f\"test\"\n        \n    # cluster_conditions = {}\n    # \n    #     df1 = df[features + [\"label\"]]\n    #     cluster_modes = {}\n    #     for cluster in df1['label'].unique():\n    #         temp = df1[df1['label'] == cluster]\n    #         cluster_df = pd.get_dummies(temp[temp.columns[temp.dtypes == 'object']], prefix_sep=\":\",\n    #                                     columns=temp.columns[temp.dtypes == 'object'])\n    #         cluster_modes[cluster] = cluster_df.mode().loc[cluster_df.mode().sum(axis=1).idxmax()].to_dict()\n\n    #     # Step 3: Form the conditions for each cluster\n\n    #     for cluster, modes in cluster_modes.items():\n    #         modes = {k: v for k, v in modes.items() if v == 1}\n    #         op_li = []\n    #         for key in modes.keys():\n    #             k = key.split(\":\")[0]\n    #             v = key.split(\":\")[1]\n    #             op = f\"{k} == '{v}'\"\n    #             op_li.append(op)\n    #         condition = \" & \".join(op_li)\n    #         # condition = ' & '.join([f\"{col} == {mode}\" for col, mode in modes.items() if col != 'label'])\n    #         cluster_conditions[int(cluster)] = f\"{condition}\"\n    except Exception as e:\n        print(\"extract_rules :  error occoured \" + str(e))\n        return cluster_conditions\n    return cluster_conditions\n    \n\n\n\ndef rule_extraction(dag_run_id, db):\n    try:\n        path_d = os.path.join(ml_location, dag_run_id, \"dict.pickle\")\n        features_path = os.path.join(ml_location, dag_run_id, \"features.pickle\")\n        data = load_picke_file(path_d)\n        data_feature = load_picke_file(features_path)\n          \n\n\n\n          \n\n        re = RuleExtreaction(dag_run_id=dag_run_id, db=db, features_path=features_path, path_d=path_d)\n        re.execute()\n\n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\n        # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\n        # for item, val in filtered_dict.items():\n        #     # val is the path item is the segment name\n        #     df = pd.read_csv(val)\n        #     print(f\"the cluster is {item}\")\n        #     cluster_conditions = extract_rules(df, data_feature[item])\n        #     if cluster_conditions is None:\n        #         print(f\" the segment {item} is none ,  the path is {val}\")\n        #     for cluster, rule in cluster_conditions:\n        #         try:\n        #\n        #             info = schemas.SegementInfo\n        #             info.dag_run_id = dag_run_id\n        #             info.segment_name = f\"{item}_{str(cluster)}\"\n        #             info.segment_length = str(len(df))\n        #             info.customer_status = \"active\"\n        #             info.query = rule\n        #             info.samples = len(df.query(rule))\n        #             SegementRepo.create(db=db, segement=info)\n        #         except Exception as e:\n        #             print(\" error occorured in rule insertion \")\n        #             print(e)\n        #             raise e\n        #\n\n    except Exception as e:\n        print(e)\n\n\n\n\n\n\n\ndef transform(dataframe):\n    db = get_db()\n    rule_extraction(\"manual__2023-07-10T11:06:51\", db)\n    return dataframe"
              }
            }, {
              "id": "01ddd301-006c-1f7d-b1e2-e2a3a1927cae",
              "operation": {
                "id": "f94b04d7-ec34-42f7-8100-93fe235c89f8",
                "name": "Source"
              },
              "parameters": {

              }
            }, {
              "id": "ce4763d6-678a-72cd-e155-5223760a5a94",
              "operation": {
                "id": "e652238f-7415-4da6-95c6-ee33808561b2",
                "name": "Sink"
              },
              "parameters": {

              }
            }],
            "connections": [{
              "from": {
                "nodeId": "d388c946-68fa-35e9-5563-95a201376626",
                "portIndex": 0
              },
              "to": {
                "nodeId": "ce4763d6-678a-72cd-e155-5223760a5a94",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "f8c1f429-01a6-c2eb-b884-23a343381446",
                "portIndex": 0
              },
              "to": {
                "nodeId": "77423f41-512c-4db1-92e0-65020ac7970a",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "15bc5fbe-87c5-7e1e-9eaf-dfb357bad98e",
                "portIndex": 0
              },
              "to": {
                "nodeId": "f8c1f429-01a6-c2eb-b884-23a343381446",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "b9ecbc38-079e-64bd-976f-b60677c77622",
                "portIndex": 0
              },
              "to": {
                "nodeId": "d388c946-68fa-35e9-5563-95a201376626",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "77423f41-512c-4db1-92e0-65020ac7970a",
                "portIndex": 0
              },
              "to": {
                "nodeId": "b9ecbc38-079e-64bd-976f-b60677c77622",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "01ddd301-006c-1f7d-b1e2-e2a3a1927cae",
                "portIndex": 0
              },
              "to": {
                "nodeId": "9a819e7e-4d3c-8efe-16ef-50ffdf3c5516",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "23865ebd-df86-3eb2-157b-15f3351e784d",
                "portIndex": 0
              },
              "to": {
                "nodeId": "91ffda5a-084a-685b-88f4-f9241d240e72",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "91ffda5a-084a-685b-88f4-f9241d240e72",
                "portIndex": 0
              },
              "to": {
                "nodeId": "15bc5fbe-87c5-7e1e-9eaf-dfb357bad98e",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "9a819e7e-4d3c-8efe-16ef-50ffdf3c5516",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e5e91f6d-c6b6-3cdc-112a-ecc8a1a7bff8",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "e5e91f6d-c6b6-3cdc-112a-ecc8a1a7bff8",
                "portIndex": 0
              },
              "to": {
                "nodeId": "23865ebd-df86-3eb2-157b-15f3351e784d",
                "portIndex": 0
              }
            }]
          },
          "thirdPartyData": {
            "gui": {
              "name": "Inner workflow of custom transformer",
              "nodes": {
                "9a819e7e-4d3c-8efe-16ef-50ffdf3c5516": {
                  "uiName": "k_modes",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4951,
                    "y": 5040
                  }
                },
                "77423f41-512c-4db1-92e0-65020ac7970a": {
                  "uiName": "churn_prediction",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5563,
                    "y": 5626
                  }
                },
                "b9ecbc38-079e-64bd-976f-b60677c77622": {
                  "uiName": "next_purchase_day",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5659,
                    "y": 5726
                  }
                },
                "d388c946-68fa-35e9-5563-95a201376626": {
                  "uiName": "campaign_response_model",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5747,
                    "y": 5825
                  }
                },
                "ce4763d6-678a-72cd-e155-5223760a5a94": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5923,
                    "y": 5926
                  }
                },
                "23865ebd-df86-3eb2-157b-15f3351e784d": {
                  "uiName": "rule_extraction",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5167,
                    "y": 5238
                  }
                },
                "15bc5fbe-87c5-7e1e-9eaf-dfb357bad98e": {
                  "uiName": "association_process",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5373,
                    "y": 5431
                  }
                },
                "01ddd301-006c-1f7d-b1e2-e2a3a1927cae": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 4937,
                    "y": 4934
                  }
                },
                "e5e91f6d-c6b6-3cdc-112a-ecc8a1a7bff8": {
                  "uiName": "feature_selection",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5063,
                    "y": 5140
                  }
                },
                "f8c1f429-01a6-c2eb-b884-23a343381446": {
                  "uiName": "rule_generation",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5474,
                    "y": 5531
                  }
                },
                "91ffda5a-084a-685b-88f4-f9241d240e72": {
                  "uiName": "matrix_filter",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5277,
                    "y": 5340
                  }
                }
              }
            }
          },
          "publicParams": []
        }
      }
    }, {
      "id": "3f039986-70e1-a727-fa53-5c0e495f0207",
      "operation": {
        "id": "65240399-2987-41bd-ba7e-2944d60a3404",
        "name": "Create Custom Transformer"
      },
      "parameters": {
        "inner workflow": {
          "workflow": {
            "nodes": [{
              "id": "71de1c96-9330-a4a5-e124-16b64858641a",
              "operation": {
                "id": "f94b04d7-ec34-42f7-8100-93fe235c89f8",
                "name": "Source"
              },
              "parameters": {

              }
            }, {
              "id": "85bffd58-1328-ff18-5491-5d8f570fd3d2",
              "operation": {
                "id": "e652238f-7415-4da6-95c6-ee33808561b2",
                "name": "Sink"
              },
              "parameters": {

              }
            }, {
              "id": "0e87b8ea-567a-da10-1de3-bcef104c30b1",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import os.path\r\nfrom pathlib import Path\r\nfrom fastapi import Depends, FastAPI, HTTPException\r\nimport dask.dataframe as dd\r\nfrom icecream import ic\r\nimport pandas as pd\r\nfrom pathlib import Path\r\nimport traceback\r\n\r\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\netl_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location = '/home/tnmops/seahorse3_bkp/'\r\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                           'total_amt': 'float32'} \r\nderived_pack_info_location= '/home/tnmops/seahorse3_bkp/'\r\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\r\n# dag_run_id = \"manual__2023-07-10T11:06:51\"    \r\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\r\nfeature_unit_list= ['unit_in_mb', 'price']\r\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\r\n\r\ndef get_file_names():\r\n    return {\r\n        \"purchase\": {\r\n            \"m1\": \"purchase_dec_full_df.csv\",\r\n            \"m2\": \"purchase_jan_full_df.csv\",\r\n            \"m3\": \"purchase_feb_full_df.csv\",\r\n        }\r\n    }\r\n    \r\n\r\n\r\nclass PreProcessedData:\r\n    def __init__(self, purchase=None, usage=None):\r\n        self.purchase = purchase\r\n        self.usage = usage\r\n        # combined data of purchase and pack info\r\n        self.purchase_pack_info = None\r\n        \r\n\r\n\r\nclass TNMData:\r\n    def __init__(self,\r\n                 purchase_df=None,\r\n                 usage_df=None,\r\n                 pack_df=None):\r\n        self.purchase_df = purchase_df\r\n        self.usage_df = usage_df\r\n        self.pack_df = pack_df\r\n        \r\n\r\ndef validity_banding(df):\r\n    validity_column = PACK_INFO_VALIDITY_NAME\r\n    df.loc[df[validity_column] < 1, 'band'] = 'Hourly'\r\n    df.loc[(df[validity_column] >= 1) & (df[validity_column] < 5), 'band'] = 'Daily'\r\n    df.loc[(df[validity_column] >= 5) & (df[validity_column] < 15), 'band'] = 'Weekly'\r\n    df.loc[(df[validity_column] >= 15) & (df[validity_column] <= 31), 'band'] = 'Monthly'\r\n    df.loc[df[validity_column] > 31, 'band'] = 'Unlimited'\r\n\r\n    return df\r\n    \r\n\r\ndef form_bands(pur_pack_df, packinfo_df):\r\n    data_li = []\r\n    tot = 0\r\n    if type(packinfo_df) == dd.core.DataFrame:\r\n        packinfo_df = packinfo_df.compute()\r\n    if type(pur_pack_df) == dd.core.DataFrame:\r\n        pur_pack_df = pur_pack_df.compute()\r\n\r\n    # pur_pack_df.to_csv('/data/autopilot/ml/pur_pack_df.csv',header=True,index=False)\r\n    # for service in pur_pack_df['Product_Type'].unique():\r\n\r\n    for service in ['DATA']:\r\n\r\n        # for service in cfg.feature_mapping.keys():\r\n        ic(f\"for the serive {service}\")\r\n        pur_pack_df_service = pur_pack_df[pur_pack_df['product_type'] == service]\r\n        # for unit_name in cfg.feature_mapping.get(service):\r\n        for unit_name in feature_unit_list:\r\n            ic(f\"for the unit name {unit_name}\")\r\n            dft = pur_pack_df_service.groupby(unit_name).agg({'msisdn': 'count'}).rename(\r\n                columns={\"msisdn\": \"m_count\"}).reset_index()\r\n            # remove if any categorical values are present\r\n            dft = dft.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\r\n            dft = dft.round(0)\r\n            dft = dft.astype(int)\r\n            print('type(dft) is ', type(dft))\r\n            unit_count = dft.sort_values(by=unit_name)\r\n            unit_count = unit_count.reset_index(drop=True)\r\n            print('len of unit_count is ', len(unit_count))\r\n            print('unit_count is ', unit_count)\r\n            # function that return the band accourding to mininmum distribution percentage\r\n            b = get_bands(unit_count, unit_name)\r\n            # removing dup\r\n            b = list(set(b))\r\n            b.sort()\r\n            # for cuts method is like  (0,5] means the range not including 0 to including 5 ie: x:  x>0  and x= 5\r\n            b[0] = b[0] - 0.01\r\n            # forming labels accourding to the bands\r\n\r\n            cut_labels = []\r\n            for i in range(len(b) - 1):\r\n                cut_labels.append(f'{b[i]}-{b[i + 1]} {service} {unit_name} band')\r\n            tot = tot + len(cut_labels)\r\n            ic(f\" the bands are {b} and the cut labels are {cut_labels}\")\r\n            # pur_pack_df_service[unit_name] = pur_pack_df_service[unit_name].apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\r\n\r\n            packinfo_df['band'] = pd.cut(packinfo_df[unit_name], bins=b, labels=cut_labels)\r\n            ic(\"the band value_counts\", packinfo_df['band'].value_counts())\r\n            data_li.append(packinfo_df.copy())\r\n            ic(\"len is \", len(data_li))\r\n    final_df = pd.concat(data_li)\r\n    return final_df\r\n    \r\n\r\ndef get_bands(dft, col, per_val=15):\r\n    total = dft['m_count'].sum()\r\n    li = []\r\n    last_pointer = 0\r\n    for i in range(0, len(dft)):\r\n        # print(\"the row\",i)\r\n        # print(dft.iloc[i])\r\n        per = (sum(dft.iloc[last_pointer:i + 1].m_count) / total) * 100\r\n        # print(\"the percentage\" , per)\r\n        if per > per_val:\r\n            band = set()\r\n            band.add(min(dft.iloc[last_pointer:i + 1][col]))\r\n            band.add(max(dft.iloc[last_pointer:i + 1][col]))\r\n            # band = (min(dft.iloc[last_pointer:i+1].b_validity) , max(dft.iloc[last_pointer:i+1].b_validity))\r\n\r\n            li.append(min(dft.iloc[last_pointer:i + 1][col]))\r\n            last_pointer = i + 1\r\n\r\n    band_last = (min(dft.iloc[last_pointer:][col]), max(dft.iloc[last_pointer:][col]))\r\n    band_last = set()\r\n    band_last.add(min(dft.iloc[last_pointer:][col]))\r\n    band_last.add(max(dft.iloc[last_pointer:][col]))\r\n    # print(\"the last band\",band_last)\r\n    li.append(min(dft.iloc[last_pointer:][col]))\r\n    li.append(max(dft.iloc[last_pointer:][col]))\r\n    return li\r\n    \r\n    \r\nclass PreprocessData:\r\n    def __init__(self, tnm_data=None, dag_run_id=None):\r\n        self.tnm_data = tnm_data\r\n        self.dag_run_id = dag_run_id\r\n        self.pre_data_obj = None\r\n        self.pack_features_encoded = None\r\n\r\n    def pre_process_purchase(self):\r\n        # self.tnm_data.purchase_df['total_cnt'] = self.tnm_data.purchase_df['total_cnt'].str.lower()\r\n        # self.tnm_data.purchase_df['total_cnt'] = self.tnm_data.purchase_df['total_cnt'].str.strip()\r\n\r\n        final_df = self.tnm_data.purchase_df.merge(\r\n            self.tnm_data.pack_df, left_on=\"product_id\", right_on=\"product_id\", how='inner')[\r\n            ['msisdn', 'cdr_date', 'total_cnt', 'product_type', 'validity_in_days', 'unit_in_mb', 'price','pack_types','pack_popularity',\r\n             'product_id']]\r\n        final_df = final_df.dropna()\r\n\r\n        print('len of final_df', len(final_df))\r\n        path = os.path.join(ml_location, self.dag_run_id, \"purchase_filtered\")\r\n        print('path is ', path)\r\n        Path(path).mkdir(parents=True, exist_ok=True)\r\n        final_df.to_parquet(path)\r\n\r\n        # save the file to path\r\n\r\n        self.pre_data_obj = PreProcessedData(final_df, None)\r\n\r\n    def get_pre_processed_data(self):\r\n        ic(\" getting pre processed data \")\r\n        if self.pre_data_obj is None:\r\n            ic(\"error - the data is not preprocessed \")\r\n            raise Exception(\"not procedssed\")\r\n\r\n        return self.pre_data_obj\r\n\r\n    def preprocess_pack_features(self):\r\n        #         test_df = self.pre_data_obj.purchase\r\n        #         test_df.to_csv('/data/autopilot/ml/pre_data_obj_purchase.csv',header=True,index=False)\r\n\r\n        #         print('len of self.pre_data_obj.purchase  ',len(self.pre_data_obj.purchase))\r\n\r\n        df = form_bands(self.pre_data_obj.purchase, self.tnm_data.pack_df.copy())\r\n        print('df.columns is ', df.columns)\r\n        df1 = validity_banding(self.tnm_data.pack_df.compute())\r\n        \r\n        df2 = self.tnm_data.pack_df.compute()\r\n        df2['band'] = df2['pack_types']\r\n        df3 = self.tnm_data.pack_df.compute()\r\n        df3['band'] = df3['pack_popularity']\r\n        \r\n        print('df1.columns is ', df1.columns)\r\n        final_df1 = pd.concat([df, df1,df2,df3])\r\n        print('final_df1.columns is ', final_df1.columns)\r\n        df = pd.crosstab(final_df1[PACK_INFO_PACK_COLUMN_NAME], final_df1[\"band\"])\r\n        df.columns.name = None\r\n        df.reset_index(inplace=True)\r\n\r\n        self.pack_features_encoded = df\r\n\r\n    def save_pack_features_df(self, format=None):\r\n        self.pack_features_encoded.to_csv(\r\n            os.path.join(ml_location,self.dag_run_id,  'pack_features_encoded.csv'), header=True, index=False)\r\n            \r\n            \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef pre_process(dag_run_id):\r\n        # loaded the data\r\n\r\n    file_name_dict = get_file_names()\r\n    print(\"purchase preprocess  ongoing \")\r\n    data = {}\r\n    for month in purchase_no_months_seg:\r\n        data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"purchase\").get(month)),\r\n                                  dtype=RECHARGE_TRANSACTION_DTYPES)\r\n\r\n    pack_info = dd.read_csv(os.path.join(derived_pack_info_location, 'pack_info_der.csv'),\r\n                            dtype=PACK_INFO_DTYPES)\r\n    tnm_data = TNMData(purchase_df=dd.concat(list(data.values())), pack_df=pack_info)\r\n    pre_instance = PreprocessData(tnm_data, dag_run_id)\r\n    # # merging purchase and pack\r\n\r\n    pre_instance.pre_process_purchase()\r\n    p_data = pre_instance.get_pre_processed_data()\r\n    # forming user purchase count matrix\r\n    product_name = PACK_INFO_PACK_COLUMN_NAME\r\n    result = (\r\n        p_data.purchase.groupby([\"msisdn\", product_name])\r\n        .agg({product_name: \"count\"})\r\n        .rename(columns={product_name: \"bundle_counts\"})\r\n        .reset_index()\r\n    )\r\n    result = result.compute()\r\n    result.dropna(subset=[product_name], inplace=True)\r\n    df = (\r\n        pd.crosstab(\r\n            result.msisdn,\r\n            result[product_name],\r\n            values=result.bundle_counts,\r\n            aggfunc=sum,\r\n\r\n        )\r\n    )\r\n    ic(\"crosstab of purchase done \")\r\n    df1 = df.fillna(0)\r\n    df1 = df1.reset_index()\r\n    op_path_dir = os.path.join(ml_location)\r\n    Path(op_path_dir).mkdir(parents=True, exist_ok=True)\r\n    df1.to_csv(os.path.join(ml_location,dag_run_id,'user_pack_matrix.csv'), header=True, index=False)\r\n    ic(\"outputed user pack matrix\")\r\n    # forming pack feature matrix\r\n    pre_instance.preprocess_pack_features()\r\n    pre_instance.save_pack_features_df()\r\n\r\n    ic(\"outputed  pack feature matrix \")\r\n    \r\n    \r\n    return df1\r\n    \r\n    \r\n    \r\n\r\ndef transform(dataframe):\r\n    df = pre_process(\"manual__2023-07-10T11:06:51\")\r\n    return df"
              }
            }, {
              "id": "73187f58-f7ee-0380-ffbc-e059d76b3af8",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nfrom icecream import ic\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\n\r\n\r\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\netl_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location = '/home/tnmops/seahorse3_bkp/'\r\n# dag_run_id = \"manual__2023-07-10T11:06:51\"    \r\nMSISDN_COL_NAME = 'msisdn'\r\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\r\n\r\ndef matrix_operations(dag_run_id):\r\n    matrix_pack_features = pd.read_csv(\r\n        os.path.join(ml_location, dag_run_id,\"pack_features_encoded.csv\"))\r\n    matrix_user_pack = pd.read_csv(os.path.join(ml_location,dag_run_id, \"user_pack_matrix.csv\"))\r\n    msisdn_list = matrix_user_pack.pop(MSISDN_COL_NAME)\r\n    product_ids = matrix_user_pack.columns\r\n\r\n#     product_ids = [int(x) for x in product_ids]\r\n    product_ids = [float(x) for x in product_ids]\r\n    matrix_pack_features = matrix_pack_features[matrix_pack_features[PACK_INFO_PACK_COLUMN_NAME].isin(product_ids)]\r\n    product_id_ls = matrix_pack_features.pop(PACK_INFO_PACK_COLUMN_NAME)\r\n    pack_features_cols = matrix_pack_features.columns\r\n    ic(\"performing a x b\")\r\n    final_matrix = np.matmul(matrix_user_pack, matrix_pack_features)\r\n    final_matrix.columns = pack_features_cols\r\n    final_matrix.index = msisdn_list\r\n    # normilizing the matrix\r\n    ic(\"going to normalize \")\r\n    final_matrix = final_matrix.div(final_matrix.sum(axis=1), axis=0)\r\n    matrix_pack_features_t = matrix_pack_features.T\r\n    final_matrix_1 = np.matmul(final_matrix.values, matrix_pack_features_t.values)\r\n    final_matrix_1 = pd.DataFrame(final_matrix_1, index=msisdn_list, columns=product_id_ls)\r\n    ic(\"got the final matrix \")\r\n\r\n    final_matrix_1.columns = final_matrix_1.columns.astype(int)\r\n    print(final_matrix_1.columns)\r\n    final_matrix_1.to_csv(os.path.join(ml_location,dag_run_id, \"matrix.csv\"), header=True,\r\n                          index=True)\r\n\r\n    return final_matrix_1\r\n        \r\n        \r\n        \r\n\r\n\r\ndef transform(dataframe):\r\n    df  = matrix_operations(\"manual__2023-07-10T11:06:51\")\r\n    return df"
              }
            }],
            "connections": [{
              "from": {
                "nodeId": "71de1c96-9330-a4a5-e124-16b64858641a",
                "portIndex": 0
              },
              "to": {
                "nodeId": "0e87b8ea-567a-da10-1de3-bcef104c30b1",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "0e87b8ea-567a-da10-1de3-bcef104c30b1",
                "portIndex": 0
              },
              "to": {
                "nodeId": "73187f58-f7ee-0380-ffbc-e059d76b3af8",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "73187f58-f7ee-0380-ffbc-e059d76b3af8",
                "portIndex": 0
              },
              "to": {
                "nodeId": "85bffd58-1328-ff18-5491-5d8f570fd3d2",
                "portIndex": 0
              }
            }]
          },
          "thirdPartyData": {
            "gui": {
              "name": "Inner workflow of custom transformer",
              "nodes": {
                "71de1c96-9330-a4a5-e124-16b64858641a": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5080,
                    "y": 4959
                  }
                },
                "85bffd58-1328-ff18-5491-5d8f570fd3d2": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5390,
                    "y": 5262
                  }
                },
                "0e87b8ea-567a-da10-1de3-bcef104c30b1": {
                  "uiName": "read_and_preprocess",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5104,
                    "y": 5062
                  }
                },
                "73187f58-f7ee-0380-ffbc-e059d76b3af8": {
                  "uiName": "matrix_multiplication",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5221,
                    "y": 5160
                  }
                }
              }
            }
          },
          "publicParams": []
        }
      }
    }, {
      "id": "b28c4177-6e23-1634-9ad1-a214dbf4fdb5",
      "operation": {
        "id": "643d8706-24db-4674-b5b4-10b5129251fc",
        "name": "Transform"
      },
      "parameters": {
        "Parameters of input Transformer": {

        }
      }
    }, {
      "id": "fa1cfa25-5194-6d74-b04f-eddbf196a72e",
      "operation": {
        "id": "643d8706-24db-4674-b5b4-10b5129251fc",
        "name": "Transform"
      },
      "parameters": {
        "Parameters of input Transformer": {

        }
      }
    }],
    "connections": [{
      "from": {
        "nodeId": "165d23d1-e101-ea4a-b0fe-bb99fc03adcc",
        "portIndex": 0
      },
      "to": {
        "nodeId": "dfb08402-f48a-6a2e-7954-a923c156d847",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "ab20c7e6-2539-4b4d-5928-86a70c06654d",
        "portIndex": 0
      },
      "to": {
        "nodeId": "fa1cfa25-5194-6d74-b04f-eddbf196a72e",
        "portIndex": 1
      }
    }, {
      "from": {
        "nodeId": "b28c4177-6e23-1634-9ad1-a214dbf4fdb5",
        "portIndex": 0
      },
      "to": {
        "nodeId": "dfb08402-f48a-6a2e-7954-a923c156d847",
        "portIndex": 1
      }
    }, {
      "from": {
        "nodeId": "3f039986-70e1-a727-fa53-5c0e495f0207",
        "portIndex": 0
      },
      "to": {
        "nodeId": "b28c4177-6e23-1634-9ad1-a214dbf4fdb5",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "67b64610-52ef-3289-7b1d-a12339a7d4cb",
        "portIndex": 0
      },
      "to": {
        "nodeId": "fa1cfa25-5194-6d74-b04f-eddbf196a72e",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "fa1cfa25-5194-6d74-b04f-eddbf196a72e",
        "portIndex": 0
      },
      "to": {
        "nodeId": "b28c4177-6e23-1634-9ad1-a214dbf4fdb5",
        "portIndex": 1
      }
    }]
  },
  "thirdPartyData": {
    "gui": {
      "name": "final_autopilot_flow",
      "description": "",
      "nodes": {
        "ab20c7e6-2539-4b4d-5928-86a70c06654d": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 9022,
            "y": 9114
          }
        },
        "fa1cfa25-5194-6d74-b04f-eddbf196a72e": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 9037,
            "y": 9258
          }
        },
        "165d23d1-e101-ea4a-b0fe-bb99fc03adcc": {
          "uiName": "ml_operation",
          "color": "#00B1EB",
          "coordinates": {
            "x": 8774,
            "y": 9497
          }
        },
        "b28c4177-6e23-1634-9ad1-a214dbf4fdb5": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 9041,
            "y": 9414
          }
        },
        "67b64610-52ef-3289-7b1d-a12339a7d4cb": {
          "uiName": "preprocess",
          "color": "#00B1EB",
          "coordinates": {
            "x": 8772,
            "y": 9164
          }
        },
        "dfb08402-f48a-6a2e-7954-a923c156d847": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 9039,
            "y": 9586
          }
        },
        "3f039986-70e1-a727-fa53-5c0e495f0207": {
          "uiName": "matrix_operations",
          "color": "#00B1EB",
          "coordinates": {
            "x": 8772,
            "y": 9329
          }
        }
      }
    },
    "notebooks": {

    },
    "datasources": [{
      "accessLevel": "writeRead",
      "params": {
        "name": "purchase_nov",
        "libraryFileParams": {
          "libraryPath": "library://purchase_nov.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-20T08:06:20.219Z",
      "id": "6f14fa48-3b95-2752-8703-5dd52ab66848",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }]
  },
  "variables": {

  },
  "id": "8b69f761-e398-4eb9-8e42-4d5ee4e5f4b3",
  "metadata": {
    "type": "batch",
    "apiVersion": "1.4.3"
  }
}