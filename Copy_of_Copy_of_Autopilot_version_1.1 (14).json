{
  "workflow": {
    "nodes": [{
      "id": "e0b0c6c6-54d9-c40e-d98f-364b6e84656e",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n    \n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n    \n    \ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_march_20230809132937.csv\",\n            \"m2\": \"recharge_feb_20230809132937.csv\",\n            \"m3\": \"recharge_jan_20230809132937.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n\n\nclass RechargeBanding(object):\n    def __init__(self, data):\n        self.data = data\n        # self.categorize()\n\n    def count_category(self, x, m):\n\n        if x == 1:\n            resp = 'b)1'\n        elif 1 < x <= 4:\n            resp = 'c)1-4'\n        elif 4 < x <= 10:\n            resp = 'd)4-10'\n        elif 10 < x <= 30:\n            resp = 'e)10-30'\n        elif x > 30:\n            resp = 'f)30 above'\n        else:\n            resp = 'a)zero'\n        return m + \"_\" + resp\n\n    def categorize(self):\n\n        months = recharge_no_months\n        recharge_count_col_name = RECHARGE_COUNT_COL_NAME\n        msisdn_name = MSISDN_COL_NAME\n        temp_df = None\n        for month in months:\n            needed_col = RECHARGE_NEEDED_COL\n            dataset = self.data.get(month)[needed_col]\n            dataset = dataset.groupby([msisdn_name]).agg({recharge_count_col_name: 'sum'})\n            dataset['count_pattern'] = dataset[recharge_count_col_name].apply(self.count_category, args=(month,))\n            dataset['recharge_count_pattern_' + month] = dataset['count_pattern']\n            dataset['recharge_' + recharge_count_col_name + month] = dataset[recharge_count_col_name]\n            dataset = dataset.drop(recharge_count_col_name, axis=1)\n            dataset = dataset.drop('count_pattern', axis=1)\n            if temp_df is None:\n                temp_df = dataset\n            else:\n                temp_df = temp_df.merge(dataset, on=msisdn_name, how=\"left\")\n                temp_df[\"recharge_count_pattern_\" + month] = temp_df[\"recharge_count_pattern_\" + month].fillna(month+\"_a)zero\")\n                temp_df['recharge_' +recharge_count_col_name + month]=temp_df['recharge_' +recharge_count_col_name + month].fillna(0)\n\n        return temp_df.reset_index()\n        \n        \n\n\n\ndef recharge_process(dag_run_id):\n    try:\n        file_name_dict = get_file_names()\n        print(\"rechare preprocess  ongoing \")\n        data = {}\n        dtype_recharge = RECHARGE_DTYPES\n        for month in recharge_no_months:\n            data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"recharge\").get(month)),\n                                      dtype= RECHARGE_DTYPES)\n\n        # ic(\"the length of m1 in recharge \", len(data['m1']))\n        # ic(\"the length of m2 in recharge \", len(data['m2']))\n        # ic(\"the length of m3 in recharge \", len(data['m3']))\n        #\n        # ic(\"the length of unique msisdn m3 in recharge \", data['m1']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in recharge \", data['m2']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in recharge \", data['m3']['msisdn'].nunique().compute())\n        df_data = dd.concat(list(data.values()))\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        path_recharge = os.path.join(ml_location,  \"recharge_all_months\")\n        Path(path_recharge).mkdir(parents=True, exist_ok=True)\n        df_data.to_parquet(path_recharge)\n        rb = RechargeBanding(data)\n        print(\"recharge categorizing ongoing \")\n        df = rb.categorize()\n        print(\"recharge categorizing done \")\n        path = os.path.join(ml_location, \"recharge_band\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(\"recharge categorizing  to file op ongoing \")\n        df.to_parquet(path)\n        print(\"recharge categorizing  to file op done  \")\n        return df.compute()\n    except Exception as e:\n        print(e)\n        \n        \n\n\ndef transform(dataframe):\n    df = recharge_process(\"manual__2023-07-10T11:06:51\")\n    return df"
      }
    }, {
      "id": "5e6b9e27-3486-806c-2578-216fe8adc948",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import os.path\nfrom pathlib import Path\nfrom fastapi import Depends, FastAPI, HTTPException\nimport dask.dataframe as dd\nfrom icecream import ic\nimport pandas as pd\nfrom pathlib import Path\nimport traceback\n\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\netl_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                           'total_amt': 'float32'} \nderived_pack_info_location= '/home/tnmops/seahorse3_bkp/'\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\n# dag_run_id = \"manual__2023-07-10T11:06:51\"    \nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nfeature_unit_list= ['unit_in_mb', 'price']\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\n\ndef get_file_names():\n    return {\n        \"purchase\": {\n            \"m1\": \"purchase_dec_full_df.csv\",\n            \"m2\": \"purchase_jan_full_df.csv\",\n            \"m3\": \"purchase_feb_full_df.csv\",\n        }\n    }\n    \n\n\nclass PreProcessedData:\n    def __init__(self, purchase=None, usage=None):\n        self.purchase = purchase\n        self.usage = usage\n        # combined data of purchase and pack info\n        self.purchase_pack_info = None\n        \n\n\nclass TNMData:\n    def __init__(self,\n                 purchase_df=None,\n                 usage_df=None,\n                 pack_df=None):\n        self.purchase_df = purchase_df\n        self.usage_df = usage_df\n        self.pack_df = pack_df\n        \n\ndef validity_banding(df):\n    validity_column = PACK_INFO_VALIDITY_NAME\n    df.loc[df[validity_column] < 1, 'band'] = 'Hourly'\n    df.loc[(df[validity_column] >= 1) & (df[validity_column] < 5), 'band'] = 'Daily'\n    df.loc[(df[validity_column] >= 5) & (df[validity_column] < 15), 'band'] = 'Weekly'\n    df.loc[(df[validity_column] >= 15) & (df[validity_column] <= 31), 'band'] = 'Monthly'\n    df.loc[df[validity_column] > 31, 'band'] = 'Unlimited'\n\n    return df\n    \n\ndef form_bands(pur_pack_df, packinfo_df):\n    data_li = []\n    tot = 0\n    if type(packinfo_df) == dd.core.DataFrame:\n        packinfo_df = packinfo_df.compute()\n    if type(pur_pack_df) == dd.core.DataFrame:\n        pur_pack_df = pur_pack_df.compute()\n\n    # pur_pack_df.to_csv('/data/autopilot/ml/pur_pack_df.csv',header=True,index=False)\n    # for service in pur_pack_df['Product_Type'].unique():\n\n    for service in ['DATA']:\n\n        # for service in cfg.feature_mapping.keys():\n        ic(f\"for the serive {service}\")\n        pur_pack_df_service = pur_pack_df[pur_pack_df['product_type'] == service]\n        # for unit_name in cfg.feature_mapping.get(service):\n        for unit_name in feature_unit_list:\n            ic(f\"for the unit name {unit_name}\")\n            dft = pur_pack_df_service.groupby(unit_name).agg({'msisdn': 'count'}).rename(\n                columns={\"msisdn\": \"m_count\"}).reset_index()\n            # remove if any categorical values are present\n            dft = dft.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n            dft = dft.round(0)\n            dft = dft.astype(int)\n            print('type(dft) is ', type(dft))\n            unit_count = dft.sort_values(by=unit_name)\n            unit_count = unit_count.reset_index(drop=True)\n            print('len of unit_count is ', len(unit_count))\n            print('unit_count is ', unit_count)\n            # function that return the band accourding to mininmum distribution percentage\n            b = get_bands(unit_count, unit_name)\n            # removing dup\n            b = list(set(b))\n            b.sort()\n            # for cuts method is like  (0,5] means the range not including 0 to including 5 ie: x:  x>0  and x= 5\n            b[0] = b[0] - 0.01\n            # forming labels accourding to the bands\n\n            cut_labels = []\n            for i in range(len(b) - 1):\n                cut_labels.append(f'{b[i]}-{b[i + 1]} {service} {unit_name} band')\n            tot = tot + len(cut_labels)\n            ic(f\" the bands are {b} and the cut labels are {cut_labels}\")\n            # pur_pack_df_service[unit_name] = pur_pack_df_service[unit_name].apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n\n            packinfo_df['band'] = pd.cut(packinfo_df[unit_name], bins=b, labels=cut_labels)\n            ic(\"the band value_counts\", packinfo_df['band'].value_counts())\n            data_li.append(packinfo_df.copy())\n            ic(\"len is \", len(data_li))\n    final_df = pd.concat(data_li)\n    return final_df\n    \n\ndef get_bands(dft, col, per_val=15):\n    total = dft['m_count'].sum()\n    li = []\n    last_pointer = 0\n    for i in range(0, len(dft)):\n        # print(\"the row\",i)\n        # print(dft.iloc[i])\n        per = (sum(dft.iloc[last_pointer:i + 1].m_count) / total) * 100\n        # print(\"the percentage\" , per)\n        if per > per_val:\n            band = set()\n            band.add(min(dft.iloc[last_pointer:i + 1][col]))\n            band.add(max(dft.iloc[last_pointer:i + 1][col]))\n            # band = (min(dft.iloc[last_pointer:i+1].b_validity) , max(dft.iloc[last_pointer:i+1].b_validity))\n\n            li.append(min(dft.iloc[last_pointer:i + 1][col]))\n            last_pointer = i + 1\n\n    band_last = (min(dft.iloc[last_pointer:][col]), max(dft.iloc[last_pointer:][col]))\n    band_last = set()\n    band_last.add(min(dft.iloc[last_pointer:][col]))\n    band_last.add(max(dft.iloc[last_pointer:][col]))\n    # print(\"the last band\",band_last)\n    li.append(min(dft.iloc[last_pointer:][col]))\n    li.append(max(dft.iloc[last_pointer:][col]))\n    return li\n    \n    \nclass PreprocessData:\n    def __init__(self, tnm_data=None, dag_run_id=None):\n        self.tnm_data = tnm_data\n        self.dag_run_id = dag_run_id\n        self.pre_data_obj = None\n        self.pack_features_encoded = None\n\n    def pre_process_purchase(self):\n        # self.tnm_data.purchase_df['total_cnt'] = self.tnm_data.purchase_df['total_cnt'].str.lower()\n        # self.tnm_data.purchase_df['total_cnt'] = self.tnm_data.purchase_df['total_cnt'].str.strip()\n\n        final_df = self.tnm_data.purchase_df.merge(\n            self.tnm_data.pack_df, left_on=\"product_id\", right_on=\"product_id\", how='inner')[\n            ['msisdn', 'cdr_date', 'total_cnt', 'product_type', 'validity_in_days', 'unit_in_mb', 'price','pack_types','pack_popularity',\n             'product_id']]\n        final_df = final_df.dropna()\n\n        print('len of final_df', len(final_df))\n        path = os.path.join(ml_location,  \"purchase_filtered\")\n        print('path is ', path)\n        Path(path).mkdir(parents=True, exist_ok=True)\n        final_df.to_parquet(path)\n\n        # save the file to path\n\n        self.pre_data_obj = PreProcessedData(final_df, None)\n\n    def get_pre_processed_data(self):\n        ic(\" getting pre processed data \")\n        if self.pre_data_obj is None:\n            ic(\"error - the data is not preprocessed \")\n            raise Exception(\"not procedssed\")\n\n        return self.pre_data_obj\n\n    def preprocess_pack_features(self):\n        #         test_df = self.pre_data_obj.purchase\n        #         test_df.to_csv('/data/autopilot/ml/pre_data_obj_purchase.csv',header=True,index=False)\n\n        #         print('len of self.pre_data_obj.purchase  ',len(self.pre_data_obj.purchase))\n\n        df = form_bands(self.pre_data_obj.purchase, self.tnm_data.pack_df.copy())\n        print('df.columns is ', df.columns)\n        df1 = validity_banding(self.tnm_data.pack_df.compute())\n        \n        df2 = self.tnm_data.pack_df.compute()\n        df2['band'] = df2['pack_types']\n        df3 = self.tnm_data.pack_df.compute()\n        df3['band'] = df3['pack_popularity']\n        \n        print('df1.columns is ', df1.columns)\n        final_df1 = pd.concat([df, df1,df2,df3])\n        print('final_df1.columns is ', final_df1.columns)\n        df = pd.crosstab(final_df1[PACK_INFO_PACK_COLUMN_NAME], final_df1[\"band\"])\n        df.columns.name = None\n        df.reset_index(inplace=True)\n\n        self.pack_features_encoded = df\n\n    def save_pack_features_df(self, format=None):\n        self.pack_features_encoded.to_csv(\n            os.path.join(ml_location,  'pack_features_encoded.csv'), header=True, index=False)\n            \n            \n            \n\n\n\n\ndef pre_process(dag_run_id):\n        # loaded the data\n\n    file_name_dict = get_file_names()\n    print(\"purchase preprocess  ongoing \")\n    data = {}\n    for month in purchase_no_months_seg:\n        data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"purchase\").get(month)),\n                                  dtype=RECHARGE_TRANSACTION_DTYPES)\n\n    pack_info = dd.read_csv(os.path.join(derived_pack_info_location, 'pack_info_der.csv'),\n                            dtype=PACK_INFO_DTYPES)\n    tnm_data = TNMData(purchase_df=dd.concat(list(data.values())), pack_df=pack_info)\n    pre_instance = PreprocessData(tnm_data, dag_run_id)\n    # # merging purchase and pack\n\n    pre_instance.pre_process_purchase()\n    p_data = pre_instance.get_pre_processed_data()\n    # forming user purchase count matrix\n    product_name = PACK_INFO_PACK_COLUMN_NAME\n    result = (\n        p_data.purchase.groupby([\"msisdn\", product_name])\n        .agg({product_name: \"count\"})\n        .rename(columns={product_name: \"bundle_counts\"})\n        .reset_index()\n    )\n    result = result.compute()\n    result.dropna(subset=[product_name], inplace=True)\n    df = (\n        pd.crosstab(\n            result.msisdn,\n            result[product_name],\n            values=result.bundle_counts,\n            aggfunc=sum,\n\n        )\n    )\n    ic(\"crosstab of purchase done \")\n    df1 = df.fillna(0)\n    df1 = df1.reset_index()\n    op_path_dir = os.path.join(ml_location)\n    Path(op_path_dir).mkdir(parents=True, exist_ok=True)\n    df1.to_csv(os.path.join(ml_location,'user_pack_matrix.csv'), header=True, index=False)\n    ic(\"outputed user pack matrix\")\n    # forming pack feature matrix\n    pre_instance.preprocess_pack_features()\n    pre_instance.save_pack_features_df()\n\n    ic(\"outputed  pack feature matrix \")\n    \n    \n    return df1\n    \n    \n    \n\ndef transform(dataframe):\n    df = pre_process(\"manual__2023-07-10T11:06:51\")\n    return df"
      }
    }, {
      "id": "a075c8d5-77f3-cc04-594e-c81571900515",
      "operation": {
        "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
        "name": "Read DataFrame"
      },
      "parameters": {
        "data source": "013f9012-1d89-3589-271b-f424e59d96e1"
      }
    }, {
      "id": "f75dc863-e698-0fbf-5269-078bcac87233",
      "operation": {
        "id": "65240399-2987-41bd-ba7e-2944d60a3404",
        "name": "Create Custom Transformer"
      },
      "parameters": {
        "inner workflow": {
          "workflow": {
            "nodes": [{
              "id": "dab9bf40-7151-49d4-663f-32458984fc7e",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, udf, when\nfrom pyspark.sql.types import StringType, ArrayType, TimestampType\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format\n\n\n\ndef transform(dataframe):\n    spark = SparkSession.builder.getOrCreate()\n\n    dataframe = dataframe.fillna(0)\n    dataframe = dataframe.withColumnRenamed('m1_msisdn', 'msisdn')\n\n\n    # Assuming you have a DataFrame named 'dataframe' with columns starting with 'm1', 'm2', 'm3'\n\n    # Get the column names from the DataFrame\n    column_names = dataframe.columns\n\n    # Define the prefixes to separate\n    prefixes = ['m1', 'm2', 'm3']\n\n    # Create a dictionary to store the separate DataFrames\n    separated_dfs = {}\n\n    # Iterate over the prefixes and select the corresponding columns\n    for prefix in prefixes:\n        # Select the columns with the current prefix\n        selected_columns = [col for col in column_names if col.startswith(prefix)]\n\n        # Include the 'msisdn' column along with the selected columns\n        selected_columns.insert(0, 'msisdn')\n\n        # Select the specified columns from the DataFrame\n        separated_df = dataframe.select(*selected_columns)\n\n        # Store the separated DataFrame in the dictionary\n        separated_dfs[prefix] = separated_df\n\n    # Access the separate DataFrames using the prefixes as keys\n    m1_df = separated_dfs['m1']\n    m2_df = separated_dfs['m2']\n    m3_df = separated_dfs['m3']\n\n    # Rename the columns in m1_df, m2_df, m3_df by removing the prefix\n    m1_df = m1_df.select([col(col_name).alias(col_name.replace('m1_', '')) for col_name in m1_df.columns])\n    m2_df = m2_df.select([col(col_name).alias(col_name.replace('m2_', '')) for col_name in m2_df.columns])\n    m3_df = m3_df.select([col(col_name).alias(col_name.replace('m3_', '')) for col_name in m3_df.columns])\n\n    lis_df = [m1_df, m2_df, m3_df]\n    purchase_base_df = m1_df.union(m2_df).union(m3_df)\n\n\n    weekday = []\n\n    for df in lis_df:\n        df = df.fillna(0)\n        df = df.withColumn(\"cdr_date\", col(\"cdr_date\").cast(TimestampType()))\n        df = df.groupBy('msisdn').agg(F.first(\"cdr_date\").alias(\"cdr_date\"))\n        df = df.withColumn(\"weekdays\", date_format('cdr_date', 'EEEE'))\n        weekday.append(df)\n\n    result = []\n\n    for i, df in enumerate(lis_df, start=1):\n        new_column = f\"m{i}_no_of_days\"\n        df = df.withColumn(\"cdr_date\", col(\"cdr_date\").cast(TimestampType()))\n        resultss = df.groupby('msisdn').agg(F.countDistinct('cdr_date').alias(new_column))\n        result.append(resultss)\n    \n    weekdays = []\n    for i, df in enumerate(weekday, start=1):\n        # Rename the columns in the DataFrame\n        renamed_columns = [col(col_name).alias(f\"m{i}_{col_name}\" if col_name != 'msisdn' else col_name) for col_name in df.columns]\n        renamed_df = df.select(renamed_columns)\n        weekdays.append(renamed_df)\n \n    \n    df = purchase_base_df.join(result[0], on='msisdn', how='left') \\\n            .join(result[1], on='msisdn', how='left') \\\n            .join(result[2], on='msisdn', how='left') \\\n            .join(weekdays[0].withColumnRenamed('cdr_date', 'weekday_cdr_date'), on='msisdn', how='left') \\\n            .join(weekdays[1].withColumnRenamed('cdr_date', 'weekday_cdr_date_1'), on='msisdn', how='left') \\\n            .join(weekdays[2].withColumnRenamed('cdr_date', 'weekday_cdr_date_2'), on='msisdn', how='left')\n\n\n    # List of columns to process\n    columns = ['m1_weekdays', 'm2_weekdays', 'm3_weekdays']\n    \n    # Define the UDF to join the elements of a list into a string\n    join_list_udf = udf(lambda x: ','.join(str(i) for i in x) if isinstance(x, list) else str(x), StringType())\n    \n    # Apply the UDF to the specified columns\n    for col_name in columns:\n        df = df.withColumn(col_name, join_list_udf(df[col_name]))\n    \n    # Replace 'nan' values with empty strings\n    df = df.replace('nan', '', subset=columns)\n    df = df.fillna(0)\n    \n    # List of columns to process\n    columns = ['m1_no_of_days', 'm2_no_of_days', 'm3_no_of_days']\n    \n    # assign_label_udf = udf(assign_label, StringType())\n    \n    # for column in columns:\n    #     new_column = column + '_banded'\n    #     df = df.withColumn(new_column, assign_label_udf(df[column]))\n        \n    drop_cols = ['cdr_date', 'product_id', 'total_amt', 'total_cnt',  'm1_cdr_date', 'm1_weekdays', 'm2_cdr_date', 'm2_weekdays', 'm3_cdr_date', 'm3_weekdays']\n    df = df.drop(*drop_cols)\n\n    \n    return df\n    \n\n\n# def assign_label(value):\n#     if value == 0:\n#         return 'A_[0]'\n#     elif 0 < value <= 5:\n#         return 'B_[0_5]'\n#     elif 5 < value <= 10:\n#         return 'C_[5_10]'\n#     elif 10 < value <= 15:\n#         return 'D_[10_15]'\n#     elif 15 < value <= 20:\n#         return 'E_[15_20]'\n#     elif 20 < value <= 25:\n#         return 'F_[20_25]'\n#     else:\n#         return 'G_[25 +]'\n\n\n\n\n\n\n"
              }
            }, {
              "id": "e747b823-4d23-0a1d-20e0-4eaae91b83d2",
              "operation": {
                "id": "90fed07b-d0a9-49fd-ae23-dd7000a1d8ad",
                "name": "Union"
              },
              "parameters": {

              }
            }, {
              "id": "ee701ea8-8f59-6241-0fbd-09004b5db542",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql import functions as F\r\nfrom pyspark.sql.functions import col\r\nfrom pyspark.sql.functions import when\r\nfrom pyspark.ml.feature import Bucketizer\r\nfrom pyspark.sql.functions import lit\r\nfrom pyspark.sql.functions import max, to_date, month, datediff\r\n\r\n\r\ndef transform(dataframe):\r\n    df = dataframe.select('*')\r\n    \r\n    date_sum = cal_sum(df)\r\n    \r\n    df_final = dataframe.filter(dataframe['total_revenue'] > 0)\r\n    result = df_final.groupby('msisdn').agg(F.count('fct_dt').alias('fct_days'))\r\n    result = result.withColumn('eng_index', result['fct_days'] / date_sum * 100)\r\n    \r\n    df1 = dataframe.filter(dataframe['total_revenue'] == 0)\r\n    df1 = df1.join(df_final.select('msisdn'), on='msisdn', how='left_anti')\r\n        \r\n    # Convert the unique values to a Spark DataFrame\r\n    df1_unique = df1.select('msisdn').distinct()\r\n    \r\n    # Add the 'eng_index' column with a default value of 0.0\r\n    df1_unique = df1_unique.withColumn('eng_index', lit(0.0))\r\n\r\n    \r\n    result = result.drop('fct_days')\r\n    f_result = result.union(df1_unique)\r\n    \r\n#   # Define the bin boundaries and labels\r\n#     bins = [-1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 101]\r\n#     labels = ['a)0_10', 'b)10_20', 'c)20_30', 'd)30_40', 'e)40_50',\r\n#               'f)50_60', 'g)60_70', 'h)70_80', 'i)80_90', 'j)90_100']\r\n    \r\n#     # Apply bucketing to create the 'eng_index_band' column\r\n#     f_result = f_result.withColumn('eng_index_band', when(\r\n#         col('eng_index').between(bins[0], bins[1]), labels[0]\r\n#     ).when(\r\n#         col('eng_index').between(bins[1], bins[2]), labels[1]\r\n#     ).when(\r\n#         col('eng_index').between(bins[2], bins[3]), labels[2]\r\n#     ).when(\r\n#         col('eng_index').between(bins[3], bins[4]), labels[3]\r\n#     ).when(\r\n#         col('eng_index').between(bins[4], bins[5]), labels[4]\r\n#     ).when(\r\n#         col('eng_index').between(bins[5], bins[6]), labels[5]\r\n#     ).when(\r\n#         col('eng_index').between(bins[6], bins[7]), labels[6]\r\n#     ).when(\r\n#         col('eng_index').between(bins[7], bins[8]), labels[7]\r\n#     ).when(\r\n#         col('eng_index').between(bins[8], bins[9]), labels[8]\r\n#     ).otherwise(labels[9]))\r\n    \r\n#     # Convert 'eng_index_band' column to string type\r\n#     f_result = f_result.withColumn('eng_index_band', col('eng_index_band').cast('string'))\r\n    \r\n    return f_result\r\n\r\ndef cal_sum(df):\r\n\r\n    # Convert 'Date' column to DateType\r\n    df = df.withColumn('Date', F.to_date(df['fct_dt']))\r\n\r\n    # Extract month from 'Date' column\r\n    df = df.withColumn('Month', F.month(df['fct_dt']))\r\n\r\n    # Calculate maximum date for each month\r\n    max_dates = df.groupBy('Month').agg(F.max('Date').alias('MaxDate'))\r\n\r\n    m1_max_day = max_dates.filter(max_dates['Month'] == 1).select('MaxDate').first()[0]\r\n    m2_max_day = max_dates.filter(max_dates['Month'] == 2).select('MaxDate').first()[0]\r\n    m3_max_day = max_dates.filter(max_dates['Month'] == 3).select('MaxDate').first()[0]\r\n\r\n    # Calculate the difference in days\r\n    date_sum = (m2_max_day - m1_max_day).days + (m3_max_day - m1_max_day).days\r\n    \r\n    return date_sum"
              }
            }, {
              "id": "8312c9b6-9569-f07e-34a6-186c6c81124e",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "9d8e63c0-0548-42a9-47ce-3f6850e65fe3"
              }
            }, {
              "id": "dfa2d682-18fd-4fc0-1903-447bc28f4bdf",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "f2e8fd17-ac58-a93b-b542-830c5d1cd101"
              }
            }, {
              "id": "7ceadbff-1b1b-8b9e-cb6c-42cebd42fdbf",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql import functions as F\r\nimport numpy as np\r\nfrom pyspark.sql.functions import col\r\nfrom pyspark.sql.functions import udf, col\r\nfrom pyspark.sql.types import StringType\r\n\r\n# def assign_label(value):\r\n#     if value == 0:\r\n#         return 'd)0'\r\n#     elif 0 < value <= 50:\r\n#         return 'e)0_50'\r\n#     elif 50 < value <= 100:\r\n#         return 'f)50_100'\r\n#     elif value > 100:\r\n#         return 'g)>100'\r\n#     elif -50 <= value < 0:\r\n#         return 'c)<0_-50'\r\n#     elif -100 <= value < -50:\r\n#         return 'b)-50_-100'\r\n#     else:\r\n#         return 'a)>-100'\r\n\r\n# assign_label_udf = udf(assign_label, StringType())\r\n\r\ndef transform(dataframe):\r\n    dataframe = dataframe.withColumnRenamed('m1_msisdn', 'msisdn')\r\n    dataframe = dataframe.fillna(0)\r\n\r\n    # Assuming you have a DataFrame named 'dataframe' with columns starting with 'm1', 'm2', 'm3'\r\n\r\n    # Get the column names from the DataFrame\r\n    column_names = dataframe.columns\r\n\r\n    # Define the prefixes to separate\r\n    prefixes = ['m1', 'm2', 'm3']\r\n\r\n    # Create a dictionary to store the separate DataFrames\r\n    separated_dfs = {}\r\n\r\n    # Iterate over the prefixes and select the corresponding columns\r\n    for prefix in prefixes:\r\n        # Select the columns with the current prefix\r\n        selected_columns = [col for col in column_names if col.startswith(prefix)]\r\n\r\n        # Include the 'msisdn' column along with the selected columns\r\n        selected_columns.insert(0, 'msisdn')\r\n\r\n        # Select the specified columns from the DataFrame\r\n        separated_df = dataframe.select(*selected_columns)\r\n\r\n        # Store the separated DataFrame in the dictionary\r\n        separated_dfs[prefix] = separated_df\r\n\r\n    # Access the separate DataFrames using the prefixes as keys\r\n    m1_df = separated_dfs['m1']\r\n    m2_df = separated_dfs['m2']\r\n    m3_df = separated_dfs['m3']\r\n\r\n    # Rename the columns in m1_df, m2_df, m3_df by removing the prefix\r\n    m1_df = m1_df.select([col(col_name).alias(col_name.replace('m1_', '')) for col_name in m1_df.columns])\r\n    m2_df = m2_df.select([col(col_name).alias(col_name.replace('m2_', '')) for col_name in m2_df.columns])\r\n    m3_df = m3_df.select([col(col_name).alias(col_name.replace('m3_', '')) for col_name in m3_df.columns])\r\n\r\n    usage_base_df = m1_df.union(m2_df).union(m3_df)\r\n\r\n\r\n    voice_usage_columns = [col for col in m1_df.columns if 'total_voice_usage' in col]\r\n    data_usage_columns = [col for col in m1_df.columns if 'data_usage' in col]\r\n    total_revenue_columns = [col for col in m1_df.columns if 'total_revenue' in col]\r\n    \r\n    for j in range(1, len(voice_usage_columns)):\r\n        if j != 2:\r\n            prev_col = voice_usage_columns[j-1]\r\n            curr_col = voice_usage_columns[j]\r\n            pct_drop_col = f'{prev_col}_{curr_col}_pct_drop_m1'\r\n            m1_df = m1_df.withColumn(pct_drop_col, F.when(col(prev_col) != 0, ((col(prev_col) - col(curr_col)) / col(prev_col)) * 100).otherwise(0))\r\n            m1_df = m1_df.withColumn(pct_drop_col, F.when(F.isnan(m1_df[pct_drop_col]), 0).otherwise(m1_df[pct_drop_col]))\r\n            m1_df = m1_df.withColumn(pct_drop_col, m1_df[pct_drop_col].cast('float'))\r\n    \r\n            prev_data_col = data_usage_columns[j-1]\r\n            curr_data_col = data_usage_columns[j]\r\n            data_pct_drop_col = f'{prev_data_col}_{curr_data_col}_pct_drop_m1'\r\n            m1_df = m1_df.withColumn(data_pct_drop_col, F.when(col(prev_data_col) != 0, ((col(prev_data_col) - col(curr_data_col)) / col(prev_data_col)) * 100).otherwise(0))\r\n            m1_df = m1_df.withColumn(data_pct_drop_col, F.when(F.isnan(m1_df[data_pct_drop_col]), 0).otherwise(m1_df[data_pct_drop_col]))\r\n            m1_df = m1_df.withColumn(data_pct_drop_col, m1_df[data_pct_drop_col].cast('float'))\r\n    \r\n    m1_df = m1_df.replace([np.inf, -np.inf], np.nan)\r\n    m1_df = m1_df.fillna(0)\r\n    \r\n    m1_df = m1_df.drop(*voice_usage_columns)\r\n    m1_df = m1_df.drop(*data_usage_columns)\r\n    m1_df = m1_df.drop(*total_revenue_columns)\r\n    \r\n    voice_usage_columns = [col for col in m2_df.columns if 'total_voice_usage' in col]\r\n    data_usage_columns = [col for col in m2_df.columns if 'data_usage' in col]\r\n    total_revenue_columns = [col for col in m2_df.columns if 'total_revenue' in col]\r\n    \r\n    for j in range(1, len(voice_usage_columns)):\r\n        if j != 2:\r\n            prev_col = voice_usage_columns[j-1]\r\n            curr_col = voice_usage_columns[j]\r\n            pct_drop_col = f'{prev_col}_{curr_col}_pct_drop_m2'\r\n            m2_df = m2_df.withColumn(pct_drop_col, F.when(col(prev_col) != 0, ((col(prev_col) - col(curr_col)) / col(prev_col)) * 100).otherwise(0))\r\n            m2_df = m2_df.withColumn(pct_drop_col, F.when(F.isnan(m2_df[pct_drop_col]), 0).otherwise(m2_df[pct_drop_col]))\r\n            m2_df = m2_df.withColumn(pct_drop_col, m2_df[pct_drop_col].cast('float'))\r\n    \r\n            prev_data_col = data_usage_columns[j-1]\r\n            curr_data_col = data_usage_columns[j]\r\n            data_pct_drop_col = f'{prev_data_col}_{curr_data_col}_pct_drop_m2'\r\n            m2_df = m2_df.withColumn(data_pct_drop_col, F.when(col(prev_data_col) != 0, ((col(prev_data_col) - col(curr_data_col)) / col(prev_data_col)) * 100).otherwise(0))\r\n            m2_df = m2_df.withColumn(data_pct_drop_col, F.when(F.isnan(m2_df[data_pct_drop_col]), 0).otherwise(m2_df[data_pct_drop_col]))\r\n            m2_df = m2_df.withColumn(data_pct_drop_col, m2_df[data_pct_drop_col].cast('float'))\r\n    \r\n    m2_df = m2_df.replace([np.inf, -np.inf], np.nan)\r\n    m2_df = m2_df.fillna(0)\r\n    \r\n    m2_df = m2_df.drop(*voice_usage_columns)\r\n    m2_df = m2_df.drop(*data_usage_columns)\r\n    m2_df = m2_df.drop(*total_revenue_columns)\r\n\r\n\r\n    voice_usage_columns = [col for col in m3_df.columns if 'total_voice_usage' in col]\r\n    data_usage_columns = [col for col in m3_df.columns if 'data_usage' in col]\r\n    total_revenue_columns = [col for col in m3_df.columns if 'total_revenue' in col]\r\n    \r\n    for j in range(1, len(voice_usage_columns)):\r\n        if j != 2:\r\n            prev_col = voice_usage_columns[j-1]\r\n            curr_col = voice_usage_columns[j]\r\n            pct_drop_col = f'{prev_col}_{curr_col}_pct_drop_m3'\r\n            m3_df = m3_df.withColumn(pct_drop_col, F.when(col(prev_col) != 0, ((col(prev_col) - col(curr_col)) / col(prev_col)) * 100).otherwise(0))\r\n            m3_df = m3_df.withColumn(pct_drop_col, F.when(F.isnan(m3_df[pct_drop_col]), 0).otherwise(m3_df[pct_drop_col]))\r\n            m3_df = m3_df.withColumn(pct_drop_col, m3_df[pct_drop_col].cast('float'))\r\n    \r\n            prev_data_col = data_usage_columns[j-1]\r\n            curr_data_col = data_usage_columns[j]\r\n            data_pct_drop_col = f'{prev_data_col}_{curr_data_col}_pct_drop_m3'\r\n            m3_df = m3_df.withColumn(data_pct_drop_col, F.when(col(prev_data_col) != 0, ((col(prev_data_col) - col(curr_data_col)) / col(prev_data_col)) * 100).otherwise(0))\r\n            m3_df = m3_df.withColumn(data_pct_drop_col, F.when(F.isnan(m3_df[data_pct_drop_col]), 0).otherwise(m3_df[data_pct_drop_col]))\r\n            m3_df = m3_df.withColumn(data_pct_drop_col, m3_df[data_pct_drop_col].cast('float'))\r\n    \r\n    m3_df = m3_df.replace([np.inf, -np.inf], np.nan)\r\n    m3_df = m3_df.fillna(0)\r\n    \r\n    m3_df = m3_df.drop(*voice_usage_columns)\r\n    m3_df = m3_df.drop(*data_usage_columns)\r\n    m3_df = m3_df.drop(*total_revenue_columns)\r\n\r\n    merged_df = m1_df.join(m2_df, on='msisdn', how='left_outer')\r\n    merged_df = merged_df.join(m3_df, on='msisdn', how='left_outer')\r\n    merged_df = merged_df.fillna(0)\r\n    \r\n    usage_base_df = usage_base_df.select('msisdn')\r\n    merged_df = usage_base_df.join(merged_df, on='msisdn', how='left')\r\n    merged_df = merged_df.fillna(0)\r\n\r\n    new_columns = [\r\n        'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1',\r\n        'w1_data_usage_w2_data_usage_pct_drop_m1',\r\n        'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1',\r\n        'w3_data_usage_w4_data_usage_pct_drop_m1',\r\n        'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2',\r\n        'w1_data_usage_w2_data_usage_pct_drop_m2',\r\n        'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2',\r\n        'w3_data_usage_w4_data_usage_pct_drop_m2',\r\n        'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3',\r\n        'w1_data_usage_w2_data_usage_pct_drop_m3',\r\n        'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3',\r\n        'w3_data_usage_w4_data_usage_pct_drop_m3'\r\n    ]\r\n\r\n    for col_name in new_columns:\r\n        merged_df = merged_df.withColumn(col_name, merged_df[col_name].cast('float'))\r\n\r\n    for column in merged_df.columns:\r\n        if 'msisdn' not in column:\r\n            merged_df = merged_df.withColumn(column, F.when(F.isnan(merged_df[column]), 0).otherwise(merged_df[column]))\r\n\r\n    for column in merged_df.columns:\r\n        if 'msisdn' not in column:\r\n            merged_df = merged_df.withColumn(column, merged_df[column].cast('float'))\r\n    \r\n    # for column in merged_df.columns:\r\n    #     if 'msisdn' not in column:\r\n    #         merged_df = merged_df.withColumn(column, assign_label_udf(merged_df[column]))\r\n\r\n    return merged_df\r\n"
              }
            }, {
              "id": "6536c9fb-6ed1-fb30-1ed0-9a3b2192862d",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "54bc9c01-1208-efeb-c15a-a3c542b1796c"
              }
            }, {
              "id": "f3364c3b-36ce-677c-2cec-9fdabfe69df3",
              "operation": {
                "id": "f94b04d7-ec34-42f7-8100-93fe235c89f8",
                "name": "Source"
              },
              "parameters": {

              }
            }, {
              "id": "746497e8-f0b9-a317-ac47-8305dae32885",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nimport pickle\r\nimport numpy as np\r\nimport os\r\nimport traceback\r\n\r\n\r\ndef transform(dataframe):\r\n    dataframe = dataframe.toPandas()\r\n    file_path = '/home/tnmops/seahorse3_bkp/'\r\n    dag_run_id = \"manual__2023-07-10T11:06:51\"\r\n    # dataframe = dataframe.astype(float)\r\n    dataframe = dataframe.fillna(0)\r\n    segment_data(dataframe)\r\n    return dataframe\r\n\r\n\r\ndef segment_data(recharge_trend_usage_rfm):\r\n    path_dict = {}\r\n    file_path = '/home/tnmops/seahorse3_bkp/'\r\n\r\n    try:\r\n\r\n        print('inside segment_data')\r\n        #recharge_trend_usage_rfm['Segment'] = recharge_trend_usage_rfm['Segment'].apply(lambda x: str(x), vectorize=True)\r\n        for trend in recharge_trend_usage_rfm['trend'].unique():\r\n            print('inside for loop1')\r\n\r\n            for segement in recharge_trend_usage_rfm['segments'].unique():\r\n                # if segement in cfg.Config.not_needed_rfm_segment:\r\n                #     continue\r\n                print('inside for loop2')\r\n                print('recharge_trend_usage_rfm.columns',recharge_trend_usage_rfm.columns)\r\n                for service_band in recharge_trend_usage_rfm['service_band'].unique():\r\n                    temp = recharge_trend_usage_rfm[\r\n                        (recharge_trend_usage_rfm['trend'] == trend) & (recharge_trend_usage_rfm['segments'] == segement)& (recharge_trend_usage_rfm['service_band'] == service_band)]\r\n\r\n                    name = f\"{trend}-{segement}-{service_band}\"\r\n                    name = r\"\" + name\r\n                    file_name = f\"{name}.csv\"\r\n                    print(' file_name is ', file_name)\r\n\r\n                    length = len(temp)\r\n                    if length > 20:\r\n                        path = os.path.join(file_path, file_name)\r\n                        print(f\"the length is suff {length} file name {name} \")\r\n                        # print('path_dict before is' ,path_dict)\r\n\r\n                        path_dict[str(name)] = str(path)\r\n                        # print('path_dict after  is' ,path_dict)\r\n\r\n   \r\n                        temp.to_csv(r\"\" + path,index=False)\r\n                        print('file_exported')\r\n\r\n                    else:\r\n                        print(f\"the length is  insuff {length} file name {name} \")\r\n\r\n        path_d = os.path.join(file_path, \"dict.pickle\")\r\n        print('path_d is', path_d)\r\n        print('path_dict is', path_dict)\r\n        with open(path_d, 'wb') as handle:\r\n            print('opened')\r\n            pickle.dump(path_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n            print('path_dict dumped ')\r\n\r\n    except Exception as e:\r\n        print(\"error occurred in segment_data\")\r\n        traceback.print_exc()\r\n        raise Exception(e)\r\n"
              }
            }, {
              "id": "92cc573f-c38a-9b56-4714-8a93f2f62ae4",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "4028c8c1-bc95-6c53-ea81-04e817edcd26",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "d01b979c-6a9f-b6c3-f6fd-ce2d4831ea59"
              }
            }, {
              "id": "4f828b50-e7fe-d036-9f41-ce7ba340d340",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "6aa96681-19eb-9c6e-2b38-7cdd30eebcb2"
              }
            }, {
              "id": "1ba82490-2948-fe2c-226e-60a063030ee6",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql import functions as F\r\nfrom pyspark.sql.functions import col\r\n\r\ndef transform(dataframe):\r\n    df = dataframe.select('*')\r\n    df = df.withColumn('fct_dt', F.to_date(df['fct_dt']))\r\n    \r\n    # Compute current_date\r\n    current_date = df.select(F.max(col(\"fct_dt\"))).first()[0]\r\n    \r\n    # Compute last_active_date\r\n    last_active_date = df.groupBy(\"msisdn\").agg(F.max(col(\"fct_dt\")).alias(\"last_active_date\"))\r\n    \r\n     # Join the DataFrame with last_active_date\r\n    df = df.join(last_active_date, on=\"msisdn\", how=\"left\")\r\n    \r\n    # Compute consecutive_inactive_days\r\n    consecutive_inactive_days = (F.datediff(F.lit(current_date), col(\"last_active_date\"))).alias(\"consecutive_inactive_days\")\r\n    \r\n    df = df.withColumn(\"consecutive_inactive_days\", consecutive_inactive_days)\r\n    \r\n    df = df.select('msisdn', 'consecutive_inactive_days')\r\n    \r\n    # Filter rows with consecutive_inactive_days > 0\r\n    df = df.filter(df[\"consecutive_inactive_days\"] > 0)\r\n    \r\n    \r\n        # Define the bins and labels\r\n    # bins = [-1, 5, 10, 15, 30, 90, float('inf')]\r\n    # labels = ['a)0-5', 'b)5-10', 'c)10-15', 'd)15-30', 'e)30-90', 'f)90+']\r\n    \r\n    # # Create a new column 'inactive_days_band' based on 'consecutive_inactive_days'\r\n    # inactive_days_df = df.withColumn('inactive_days_band',\r\n    #                                               F.when(F.col('consecutive_inactive_days') <= 5, 'a)0-5')\r\n    #                                               .when((F.col('consecutive_inactive_days') > 5) & (F.col('consecutive_inactive_days') <= 10), 'b)5-10')\r\n    #                                               .when((F.col('consecutive_inactive_days') > 10) & (F.col('consecutive_inactive_days') <= 15), 'c)10-15')\r\n    #                                               .when((F.col('consecutive_inactive_days') > 15) & (F.col('consecutive_inactive_days') <= 30), 'd)15-30')\r\n    #                                               .when((F.col('consecutive_inactive_days') > 30) & (F.col('consecutive_inactive_days') <= 90), 'e)30-90')\r\n    #                                               .when(F.col('consecutive_inactive_days') > 90, 'f)90+')\r\n    #                                               .otherwise(None))\r\n        \r\n    # # Convert the 'inactive_days_band' column to string type\r\n    # inactive_days_df = inactive_days_df.withColumn('inactive_days_band', F.col('inactive_days_band').cast('string'))\r\n    \r\n    # inactive_days_df = inactive_days_df.select('msisdn', 'consecutive_inactive_days')\r\n    return df\r\n"
              }
            }, {
              "id": "872012aa-af6b-56f1-9e64-2a7c99e29c63",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "left prefix": "m1_",
                "right prefix": "m2_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "fbe835ea-a32c-1ae5-b5aa-db330d9be6e1",
              "operation": {
                "id": "90fed07b-d0a9-49fd-ae23-dd7000a1d8ad",
                "name": "Union"
              },
              "parameters": {

              }
            }, {
              "id": "50fec26d-a6c5-1f83-56a4-35e21de22b3a",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "left prefix": "m1_",
                "right prefix": "m2_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "f3c2f533-d515-a091-ea49-a3e42ea87e2c",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "21b9451d-8670-f075-08fd-7d296cefa0b8"
              }
            }, {
              "id": "bc13f166-f189-75d6-cc20-7ecaf9ecfffc",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "right prefix": "m3_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "m1_msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "13ab1464-848b-9613-c67b-f79a665668f2",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "45b42b9d-d78b-3529-d216-ae23da9f4074",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "763ca7df-33cf-7732-769e-9e16285e856c",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "99d0d438-78fd-33d0-ee5c-cd8dcc77cdc0"
              }
            }, {
              "id": "fad5eeca-e2ce-974b-8445-caa9104883d7",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "5fa1f504-ff85-225e-296d-4aae662140ae"
              }
            }, {
              "id": "b8662a57-ca09-d5d3-8605-a933a4798f28",
              "operation": {
                "id": "d5f4e717-429f-4a28-a0d3-eebba036363a",
                "name": "Handle Missing Values"
              },
              "parameters": {
                "columns": {
                  "selections": [{
                    "type": "columnList",
                    "values": ["RFM_Segment"]
                  }],
                  "excluding": false
                },
                "strategy": {
                  "replace with custom value": {
                    "value": "000"
                  }
                },
                "user-defined missing values": [{

                }]
              }
            }, {
              "id": "e2ee0d0c-25e4-4837-058e-430f9c981ab4",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "20e965cd-4397-02b5-6ef1-d9da2fa14189",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "e9fc88d5-3cff-7e48-7f59-c4f0721215f0"
              }
            }, {
              "id": "8587033c-17ab-3e98-e2bd-482061cde76d",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "a6a0b872-eeba-819a-941b-d196150fe68c"
              }
            }, {
              "id": "ed5bb83e-3907-de7d-9e3c-5306e016548a",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "aa02e49c-1c0f-b9aa-aa7e-3507fee7d3f9",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "bf2eaebe-4cae-c159-8f33-e315608a1420"
              }
            }, {
              "id": "10669504-cff4-60bc-cf7a-66672c688116",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nimport pandas as pd\n    \ndef transform(dataframe):\n\n    spark = SparkSession.builder.getOrCreate()\n\n    dataframe = dataframe.withColumnRenamed('m1_msisdn', 'msisdn')\n    dataframe = dataframe.fillna(0)\n    \n    dataframe = dataframe.select('msisdn', 'm1_weekly_avg_voice_usage', 'm1_weekly_avg_data_usage','m1_daily_avg_voice_usage','m1_daily_avg_data_usage',\n                         'm2_weekly_avg_voice_usage', 'm2_weekly_avg_data_usage','m2_daily_avg_voice_usage','m2_daily_avg_data_usage',\n                         'm3_weekly_avg_voice_usage', 'm3_weekly_avg_data_usage','m3_daily_avg_voice_usage','m3_daily_avg_data_usage')\n    \n    df = dataframe.select('*')\n    \n    revenue_types = ['weekly_avg_voice_usage', 'weekly_avg_data_usage', 'daily_avg_voice_usage', 'daily_avg_data_usage']\n    \n    for i in range(1, 3):\n        for rt in revenue_types:\n            col1 = F.col(f'm{i+1}_{rt}')\n            col2 = F.col(f'm{i}_{rt}')\n            pct_drop_col = f'{col1}_{col2}_pct_drop'\n            \n            df = df.withColumn(pct_drop_col, (col1 - col2) / col1 * 100)\n            df = df.withColumn(pct_drop_col, F.when((F.isnan(df[pct_drop_col]) | (F.col(pct_drop_col) == float('inf')) | (F.col(pct_drop_col) == float('-inf'))), 0).otherwise(df[pct_drop_col]))\n        \n        # Convert the column to 'string' type\n        df = df.withColumn(pct_drop_col, df[pct_drop_col].cast('string'))\n    columns = [\"Column<b'm2_weekly_avg_voice_usage'>_Column<b'm1_weekly_avg_voice_usage'>_pct_drop\",\n                \"Column<b'm2_weekly_avg_data_usage'>_Column<b'm1_weekly_avg_data_usage'>_pct_drop\",\n          \"Column<b'm2_daily_avg_voice_usage'>_Column<b'm1_daily_avg_voice_usage'>_pct_drop\",\n          \"Column<b'm2_daily_avg_data_usage'>_Column<b'm1_daily_avg_data_usage'>_pct_drop\",\n          \"Column<b'm3_weekly_avg_voice_usage'>_Column<b'm2_weekly_avg_voice_usage'>_pct_drop\",\n          \"Column<b'm3_weekly_avg_data_usage'>_Column<b'm2_weekly_avg_data_usage'>_pct_drop\",\n            \"Column<b'm3_daily_avg_voice_usage'>_Column<b'm2_daily_avg_voice_usage'>_pct_drop\",\n              \"Column<b'm3_daily_avg_data_usage'>_Column<b'm2_daily_avg_data_usage'>_pct_drop\"]\n\n    new_columns = ['m2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pct_drop', \n                       'm2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pct_drop',\n                      'm2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pct_drop',\n                      'm2_daily_avg_data_usage_m1_daily_avg_data_usage_pct_drop',\n                      'm3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pct_drop', \n                      'm3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pct_drop',\n                      'm3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pct_drop',\n                      'm3_daily_avg_data_usage_m2_daily_avg_data_usage_pct_drop']\n          \n    for i, column in enumerate(columns):\n        df = df.withColumnRenamed(column, new_columns[i])\n        \n    df = df.withColumn(\"m2_daily_avg_data_usage_m1_daily_avg_data_usage_pct_drop\", col(\"m2_daily_avg_data_usage_m1_daily_avg_data_usage_pct_drop\").cast(\"double\"))\n    df = df.withColumn(\"m3_daily_avg_data_usage_m2_daily_avg_data_usage_pct_drop\", col(\"m3_daily_avg_data_usage_m2_daily_avg_data_usage_pct_drop\").cast(\"double\"))\n    \n    df = df.fillna(0)\n        \n        \n    # assign_label_udf = udf(assign_label, StringType())\n    \n    # for column in new_columns:\n    #     new_column = column + '_banded'\n    #     df = df.withColumn(new_column, assign_label_udf(df[column]))\n    \n    \n    \n    # selected_col = ('msisdn',\n    #     'm2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pct_drop_banded', \n    #     'm2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pct_drop_banded',\n    #     'm2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pct_drop_banded',\n    #     'm2_daily_avg_data_usage_m1_daily_avg_data_usage_pct_drop_banded',\n    #     'm3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pct_drop_banded', \n    #     'm3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pct_drop_banded',\n    #     'm3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pct_drop_banded',\n    #     'm3_daily_avg_data_usage_m2_daily_avg_data_usage_pct_drop_banded')\n          \n          \n    # df1 = df.selectExpr(*selected_col)\n    df = df.toPandas()\n    \n    df1 = df[['msisdn','m2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pct_drop', \n                       'm2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pct_drop',\n                      'm2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pct_drop',\n                      'm2_daily_avg_data_usage_m1_daily_avg_data_usage_pct_drop',\n                      'm3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pct_drop', \n                      'm3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pct_drop',\n                      'm3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pct_drop',\n                      'm3_daily_avg_data_usage_m2_daily_avg_data_usage_pct_drop']]\n    \n    \n    return df1\n    \n# def assign_label(value):\n#     if value == 0:\n#         return 'd)0'\n#     elif 0 < value <= 50:\n#         return 'e)0_50'\n#     elif 50 < value <= 100:\n#         return 'f)50_100'\n#     elif value > 100:\n#         return 'g)>100'\n#     elif -50 <= value < 0:\n#         return 'c)<0_-50'\n#     elif -100 <= value < -50:\n#         return 'b)-50_-100'\n#     else:\n#         return 'a)>-100'"
              }
            }, {
              "id": "878d91fc-474e-3a3b-53aa-3b9c1e209bf9",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "9d1079c4-b1f0-c724-b05c-a64d65aeaead"
              }
            }, {
              "id": "3fa4f4c6-a7d8-dc2b-5029-6023d02e235c",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "left prefix": "m1_",
                "right prefix": "m2_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "bb269f1d-ff10-dc5c-0099-d7a6e775a06a",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql import functions as F\r\nfrom pyspark.sql.types import TimestampType\r\nfrom pyspark.sql.functions import col, when\r\nimport pandas as pd\r\n\r\ndef perform_rfm(dataframe):\r\n    # Assuming 'dataframe' is the input DataFrame\r\n    dataframe = dataframe.fillna(0)\r\n\r\n\r\n    # Convert 'purchase_date' column to timestamp\r\n    dataframe = dataframe.withColumn('cdr_date', F.from_unixtime(F.unix_timestamp('cdr_date', 'yyyy-MM-dd')).cast(TimestampType()))\r\n    \r\n    recency_df = dataframe.groupBy('msisdn').agg(F.max('cdr_date').alias('MaxPurchaseDate')).select('msisdn', 'MaxPurchaseDate')\r\n    \r\n    # Calculate recency in days\r\n    recency_df = recency_df.withColumn('Recency', F.datediff(F.lit(recency_df.select(F.max('MaxPurchaseDate')).collect()[0][0]), 'MaxPurchaseDate'))\r\n    \r\n    # Drop the 'MaxPurchaseDate' column\r\n    recency_df = recency_df.drop('MaxPurchaseDate')\r\n    \r\n    print(\"done with recency\")\r\n    \r\n\r\n    frequency_df = dataframe.groupBy('msisdn').agg(F.sum('total_cnt').alias('frequency'))\r\n    \r\n    print(\"done with frequency\")\r\n    \r\n    \r\n    monitory_df = dataframe.groupBy('msisdn').agg(F.sum('total_amt').alias('monitery'))\r\n\r\n    print(\"done with monitory\")\r\n    \r\n    rfm_data_base = recency_df.join(frequency_df, on='msisdn').join(monitory_df, on='msisdn')\r\n\r\n    return rfm_data_base\r\n    \r\ndef form_segements(dataframe):\r\n    from pyspark.sql.functions import col, udf\r\n    from pyspark.sql.types import FloatType\r\n    from pyspark.sql import SparkSession\r\n    from pyspark.ml.feature import Bucketizer\r\n    from pyspark.sql.types import IntegerType\r\n    \r\n    \r\n    # calculating R_Score---------------------------------\r\n    # Create a SparkSession if not already created\r\n    spark = SparkSession.builder.getOrCreate()\r\n    \r\n    # Compute the percentiles using Spark SQL's approx_percentile function\r\n    percentiles = dataframe.stat.approxQuantile(\"Recency\", [0.2, 0.4, 0.6, 0.8], 0.01)\r\n    bins_recency = [-1] + percentiles + [dataframe.agg({\"Recency\": \"max\"}).collect()[0][0]]\r\n    \r\n    # Sort and process duplicates in bins_recency\r\n    bins_recency = sorted(list(set(bins_recency)))\r\n    \r\n    # Define the bucketizer UDF\r\n    bucketizer_udf = udf(lambda x: float(len(bins_recency) - sum(x > i for i in bins_recency)), FloatType())\r\n    \r\n    # Apply the UDF to bucketize the recency values\r\n    dataframe = dataframe.withColumn(\"R_Score\", bucketizer_udf(col(\"Recency\").cast(FloatType())))\r\n    \r\n    # Cast the R_Score column to integer\r\n    dataframe = dataframe.withColumn(\"R_Score\", col(\"R_Score\").cast(\"integer\"))\r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    # calculating F_score-------------------------------------------------------------\r\n    # Define the number of bins\r\n    num_bins = 5\r\n    \r\n    # Calculate the range of frequencies\r\n    frequency_range = dataframe.agg({\"frequency\": \"min\"}).collect()[0][0], dataframe.agg({\"frequency\": \"max\"}).collect()[0][0]\r\n    \r\n    # Create evenly spaced splits based on the frequency range and the number of bins\r\n    splits_frequency = [frequency_range[0] + i * (frequency_range[1] - frequency_range[0]) / num_bins for i in range(num_bins+1)]\r\n    \r\n    # Create the Bucketizer transformer\r\n    bucketizer = Bucketizer(splits=splits_frequency, inputCol=\"frequency\", outputCol=\"F_Score\")\r\n    \r\n    # Apply the Bucketizer transformer to create the \"F_Score\" column\r\n    dataframe = bucketizer.transform(dataframe)\r\n    \r\n    # Convert the \"F_Score\" column to integer type\r\n    dataframe = dataframe.withColumn(\"F_Score\", (col(\"F_Score\") + 1).cast(\"integer\"))\r\n    \r\n    \r\n    #calculating M_Score-----------------------------------------------------------------\r\n    # Define the percentile values\r\n    percentiles = [0, 20, 40, 60, 80]\r\n    \r\n    # Compute the percentile values for the \"Frequency\" column\r\n    percentile_values = dataframe.approxQuantile(\"monitery\", [p/100 for p in percentiles], 0.01)\r\n    \r\n    # Define the splits for bucketizing\r\n    splits_frequency = [-float(\"inf\")] + percentile_values + [float(\"inf\")]\r\n    \r\n    # Create the Bucketizer transformer\r\n    bucketizer = Bucketizer(splits=splits_frequency, inputCol=\"monitery\", outputCol=\"M_Score\")\r\n    \r\n    # Apply the Bucketizer transformer to create the \"M_Score\" column\r\n    dataframe = bucketizer.transform(dataframe)\r\n    \r\n    # Convert the \"F_Score\" column to integer type\r\n    dataframe = dataframe.withColumn(\"M_Score\", col(\"M_Score\").cast(IntegerType()))\r\n    dataframe = segmentaion_fun1(dataframe)\r\n\r\n    \r\n    return dataframe\r\n    \r\n    \r\ndef segmentaion_fun1(dataframe):\r\n    transformed_df = dataframe.withColumn(\"RFM_Segment\", col(\"R_Score\") * 100 + col(\"F_Score\") * 10 + col(\"M_Score\"))\r\n    \r\n    transformed_df = transformed_df.withColumn(\"segments\",\r\n        when(col(\"RFM_Segment\").isin([555, 554, 544, 545, 454, 455, 445]), \"Champions\")\r\n        .when(col(\"RFM_Segment\").isin([543, 444, 435, 355, 354, 345, 344, 335]), \"Loyal_Customers\")\r\n        .when(col(\"RFM_Segment\").isin([553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323]), \"Potential_Loyalist\")\r\n        .when(col(\"RFM_Segment\").isin([512, 511, 422, 421, 412, 411, 311]), \"Recent_Customers\")\r\n        .when(col(\"RFM_Segment\").isin([525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313]), \"Promising_Customers\")\r\n        .when(col(\"RFM_Segment\").isin([535, 534, 443, 434, 343, 334, 325, 324]), \"Customers_needing_Attention\")\r\n        .when(col(\"RFM_Segment\").isin([331, 321, 312, 221, 213]), \"About_to_Sleep\")\r\n        .when(col(\"RFM_Segment\").isin([255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124]), \"At_Risk\")\r\n        .when(col(\"RFM_Segment\").isin([155, 154, 144, 214, 215, 115, 114, 113]), \"Cant_Loose_them\")\r\n        .when(col(\"RFM_Segment\").isin([332, 322, 231, 241, 251, 233, 232, 223, 222, 132, 123, 122, 212, 211]), \"Hibernating\")\r\n        .otherwise(\"Lost\")\r\n    )\r\n    \r\n    return transformed_df\r\n    \r\n    \r\ndef transform(dataframe):\r\n    dataframe = perform_rfm(dataframe)\r\n    dataframe = form_segements(dataframe)\r\n    \r\n    dataframe = dataframe.toPandas()\r\n    \r\n    dataframe = dataframe[['msisdn', 'RFM_Segment']]\r\n    \r\n    return dataframe\r\n"
              }
            }, {
              "id": "ef42b373-9e83-c96f-bbc4-574fae0925ca",
              "operation": {
                "id": "e652238f-7415-4da6-95c6-ee33808561b2",
                "name": "Sink"
              },
              "parameters": {

              }
            }, {
              "id": "e89b3570-5e82-bb93-c155-0bf0e009fb08",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "f5ea927d-16f9-4c18-87e3-e23f1933f5e5"
              }
            }, {
              "id": "6fc3ca8b-d533-fddc-5856-7722ba2ddeb2",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "right prefix": "m3_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "m1_msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "067f2613-1695-d96d-561c-838e8899fd79",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "right prefix": "m3_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "m1_msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }],
            "connections": [{
              "from": {
                "nodeId": "b8662a57-ca09-d5d3-8605-a933a4798f28",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e2ee0d0c-25e4-4837-058e-430f9c981ab4",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "e747b823-4d23-0a1d-20e0-4eaae91b83d2",
                "portIndex": 0
              },
              "to": {
                "nodeId": "ee701ea8-8f59-6241-0fbd-09004b5db542",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "20e965cd-4397-02b5-6ef1-d9da2fa14189",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e747b823-4d23-0a1d-20e0-4eaae91b83d2",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "f3c2f533-d515-a091-ea49-a3e42ea87e2c",
                "portIndex": 0
              },
              "to": {
                "nodeId": "6fc3ca8b-d533-fddc-5856-7722ba2ddeb2",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "e89b3570-5e82-bb93-c155-0bf0e009fb08",
                "portIndex": 0
              },
              "to": {
                "nodeId": "3fa4f4c6-a7d8-dc2b-5029-6023d02e235c",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "fbe835ea-a32c-1ae5-b5aa-db330d9be6e1",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e747b823-4d23-0a1d-20e0-4eaae91b83d2",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "6536c9fb-6ed1-fb30-1ed0-9a3b2192862d",
                "portIndex": 0
              },
              "to": {
                "nodeId": "872012aa-af6b-56f1-9e64-2a7c99e29c63",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "746497e8-f0b9-a317-ac47-8305dae32885",
                "portIndex": 0
              },
              "to": {
                "nodeId": "ef42b373-9e83-c96f-bbc4-574fae0925ca",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "bb269f1d-ff10-dc5c-0099-d7a6e775a06a",
                "portIndex": 0
              },
              "to": {
                "nodeId": "13ab1464-848b-9613-c67b-f79a665668f2",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "e2ee0d0c-25e4-4837-058e-430f9c981ab4",
                "portIndex": 0
              },
              "to": {
                "nodeId": "ed5bb83e-3907-de7d-9e3c-5306e016548a",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "13ab1464-848b-9613-c67b-f79a665668f2",
                "portIndex": 0
              },
              "to": {
                "nodeId": "b8662a57-ca09-d5d3-8605-a933a4798f28",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "8587033c-17ab-3e98-e2bd-482061cde76d",
                "portIndex": 0
              },
              "to": {
                "nodeId": "3fa4f4c6-a7d8-dc2b-5029-6023d02e235c",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "fad5eeca-e2ce-974b-8445-caa9104883d7",
                "portIndex": 0
              },
              "to": {
                "nodeId": "bc13f166-f189-75d6-cc20-7ecaf9ecfffc",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "067f2613-1695-d96d-561c-838e8899fd79",
                "portIndex": 0
              },
              "to": {
                "nodeId": "dab9bf40-7151-49d4-663f-32458984fc7e",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "10669504-cff4-60bc-cf7a-66672c688116",
                "portIndex": 0
              },
              "to": {
                "nodeId": "92cc573f-c38a-9b56-4714-8a93f2f62ae4",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "f3364c3b-36ce-677c-2cec-9fdabfe69df3",
                "portIndex": 0
              },
              "to": {
                "nodeId": "45b42b9d-d78b-3529-d216-ae23da9f4074",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "3fa4f4c6-a7d8-dc2b-5029-6023d02e235c",
                "portIndex": 0
              },
              "to": {
                "nodeId": "067f2613-1695-d96d-561c-838e8899fd79",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "e747b823-4d23-0a1d-20e0-4eaae91b83d2",
                "portIndex": 0
              },
              "to": {
                "nodeId": "1ba82490-2948-fe2c-226e-60a063030ee6",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "872012aa-af6b-56f1-9e64-2a7c99e29c63",
                "portIndex": 0
              },
              "to": {
                "nodeId": "bc13f166-f189-75d6-cc20-7ecaf9ecfffc",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "ed5bb83e-3907-de7d-9e3c-5306e016548a",
                "portIndex": 0
              },
              "to": {
                "nodeId": "92cc573f-c38a-9b56-4714-8a93f2f62ae4",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "45b42b9d-d78b-3529-d216-ae23da9f4074",
                "portIndex": 0
              },
              "to": {
                "nodeId": "13ab1464-848b-9613-c67b-f79a665668f2",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "aa02e49c-1c0f-b9aa-aa7e-3507fee7d3f9",
                "portIndex": 0
              },
              "to": {
                "nodeId": "872012aa-af6b-56f1-9e64-2a7c99e29c63",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "bc13f166-f189-75d6-cc20-7ecaf9ecfffc",
                "portIndex": 0
              },
              "to": {
                "nodeId": "10669504-cff4-60bc-cf7a-66672c688116",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "763ca7df-33cf-7732-769e-9e16285e856c",
                "portIndex": 0
              },
              "to": {
                "nodeId": "bb269f1d-ff10-dc5c-0099-d7a6e775a06a",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "dfa2d682-18fd-4fc0-1903-447bc28f4bdf",
                "portIndex": 0
              },
              "to": {
                "nodeId": "50fec26d-a6c5-1f83-56a4-35e21de22b3a",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "1ba82490-2948-fe2c-226e-60a063030ee6",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e2ee0d0c-25e4-4837-058e-430f9c981ab4",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "92cc573f-c38a-9b56-4714-8a93f2f62ae4",
                "portIndex": 0
              },
              "to": {
                "nodeId": "746497e8-f0b9-a317-ac47-8305dae32885",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "4f828b50-e7fe-d036-9f41-ce7ba340d340",
                "portIndex": 0
              },
              "to": {
                "nodeId": "fbe835ea-a32c-1ae5-b5aa-db330d9be6e1",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "4028c8c1-bc95-6c53-ea81-04e817edcd26",
                "portIndex": 0
              },
              "to": {
                "nodeId": "50fec26d-a6c5-1f83-56a4-35e21de22b3a",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "8312c9b6-9569-f07e-34a6-186c6c81124e",
                "portIndex": 0
              },
              "to": {
                "nodeId": "fbe835ea-a32c-1ae5-b5aa-db330d9be6e1",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "878d91fc-474e-3a3b-53aa-3b9c1e209bf9",
                "portIndex": 0
              },
              "to": {
                "nodeId": "067f2613-1695-d96d-561c-838e8899fd79",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "50fec26d-a6c5-1f83-56a4-35e21de22b3a",
                "portIndex": 0
              },
              "to": {
                "nodeId": "6fc3ca8b-d533-fddc-5856-7722ba2ddeb2",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "ee701ea8-8f59-6241-0fbd-09004b5db542",
                "portIndex": 0
              },
              "to": {
                "nodeId": "45b42b9d-d78b-3529-d216-ae23da9f4074",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "7ceadbff-1b1b-8b9e-cb6c-42cebd42fdbf",
                "portIndex": 0
              },
              "to": {
                "nodeId": "ed5bb83e-3907-de7d-9e3c-5306e016548a",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "6fc3ca8b-d533-fddc-5856-7722ba2ddeb2",
                "portIndex": 0
              },
              "to": {
                "nodeId": "7ceadbff-1b1b-8b9e-cb6c-42cebd42fdbf",
                "portIndex": 0
              }
            }]
          },
          "thirdPartyData": {
            "gui": {
              "name": "Inner workflow of custom transformer",
              "nodes": {
                "872012aa-af6b-56f1-9e64-2a7c99e29c63": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5322,
                    "y": 5238
                  }
                },
                "763ca7df-33cf-7732-769e-9e16285e856c": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4557,
                    "y": 5167
                  }
                },
                "dab9bf40-7151-49d4-663f-32458984fc7e": {
                  "uiName": "no_of_purchase_days_count",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5757,
                    "y": 5695
                  }
                },
                "e89b3570-5e82-bb93-c155-0bf0e009fb08": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5600,
                    "y": 5486
                  }
                },
                "bc13f166-f189-75d6-cc20-7ecaf9ecfffc": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5539,
                    "y": 5256
                  }
                },
                "ef42b373-9e83-c96f-bbc4-574fae0925ca": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5164,
                    "y": 5987
                  }
                },
                "4f828b50-e7fe-d036-9f41-ce7ba340d340": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4344,
                    "y": 4706
                  }
                },
                "45b42b9d-d78b-3529-d216-ae23da9f4074": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4303,
                    "y": 5136
                  }
                },
                "7ceadbff-1b1b-8b9e-cb6c-42cebd42fdbf": {
                  "uiName": "delta_calculation_weekwise",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5062,
                    "y": 5042
                  }
                },
                "f3c2f533-d515-a091-ea49-a3e42ea87e2c": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5347,
                    "y": 4826
                  }
                },
                "8587033c-17ab-3e98-e2bd-482061cde76d": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5803,
                    "y": 5489
                  }
                },
                "ee701ea8-8f59-6241-0fbd-09004b5db542": {
                  "uiName": "to find three_month_engagement_index",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4496,
                    "y": 4897
                  }
                },
                "50fec26d-a6c5-1f83-56a4-35e21de22b3a": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5016,
                    "y": 4922
                  }
                },
                "e2ee0d0c-25e4-4837-058e-430f9c981ab4": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4635,
                    "y": 5630
                  }
                },
                "4028c8c1-bc95-6c53-ea81-04e817edcd26": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5130,
                    "y": 4824
                  }
                },
                "8312c9b6-9569-f07e-34a6-186c6c81124e": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4548,
                    "y": 4708
                  }
                },
                "20e965cd-4397-02b5-6ef1-d9da2fa14189": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4759,
                    "y": 4708
                  }
                },
                "10669504-cff4-60bc-cf7a-66672c688116": {
                  "uiName": "pct_drop_daily_weekly",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5357,
                    "y": 5356
                  }
                },
                "1ba82490-2948-fe2c-226e-60a063030ee6": {
                  "uiName": "for finding inactive_days_band",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4715,
                    "y": 4909
                  }
                },
                "dfa2d682-18fd-4fc0-1903-447bc28f4bdf": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4908,
                    "y": 4819
                  }
                },
                "746497e8-f0b9-a317-ac47-8305dae32885": {
                  "uiName": "to create dict pickle file",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5040,
                    "y": 5889
                  }
                },
                "ed5bb83e-3907-de7d-9e3c-5306e016548a": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4816,
                    "y": 5701
                  }
                },
                "878d91fc-474e-3a3b-53aa-3b9c1e209bf9": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 6010,
                    "y": 5491
                  }
                },
                "fbe835ea-a32c-1ae5-b5aa-db330d9be6e1": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4446,
                    "y": 4790
                  }
                },
                "f3364c3b-36ce-677c-2cec-9fdabfe69df3": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 4206,
                    "y": 5003
                  }
                },
                "92cc573f-c38a-9b56-4714-8a93f2f62ae4": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4983,
                    "y": 5795
                  }
                },
                "b8662a57-ca09-d5d3-8605-a933a4798f28": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4478,
                    "y": 5539
                  }
                },
                "3fa4f4c6-a7d8-dc2b-5029-6023d02e235c": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5705,
                    "y": 5585
                  }
                },
                "13ab1464-848b-9613-c67b-f79a665668f2": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4455,
                    "y": 5445
                  }
                },
                "067f2613-1695-d96d-561c-838e8899fd79": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5934,
                    "y": 5601
                  }
                },
                "6fc3ca8b-d533-fddc-5856-7722ba2ddeb2": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5243,
                    "y": 4921
                  }
                },
                "aa02e49c-1c0f-b9aa-aa7e-3507fee7d3f9": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5430,
                    "y": 5153
                  }
                },
                "fad5eeca-e2ce-974b-8445-caa9104883d7": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5656,
                    "y": 5154
                  }
                },
                "6536c9fb-6ed1-fb30-1ed0-9a3b2192862d": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5205,
                    "y": 5149
                  }
                },
                "e747b823-4d23-0a1d-20e0-4eaae91b83d2": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4639,
                    "y": 4788
                  }
                },
                "bb269f1d-ff10-dc5c-0099-d7a6e775a06a": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4558,
                    "y": 5241
                  }
                }
              }
            }
          },
          "publicParams": []
        }
      }
    }, {
      "id": "091ce8c8-ea88-0cc9-f4ff-e70b94f458e9",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "from datetime import datetime, timedelta,date\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nfrom sklearn.svm import SVC\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom functools import reduce\n\nfrom icecream import ic\nfrom pathlib import Path\nimport dask.array as da\nfrom dask.dataframe import merge\nimport pickle\nimport pathlib\nimport dask.dataframe as dd\n\nimport pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom urllib.parse import quote  \n\n\n\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\nfrom sqlalchemy.orm import relationship\nimport datetime\nfrom sqlalchemy.dialects.mysql import LONGTEXT\n\n\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\n\n\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\n\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URL,\n)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        return db\n    finally:\n        db.close()\n        \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\npurchase_location = '/home/tnmops/seahorse3_bkp/'\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\netl_location = '/home/tnmops/seahorse3_bkp/'\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nlog_file = '/home/tnmops/seahorse3_bkp/'\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n    \nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCUSTOMER_NEEDED_COLUMN = [\n        'onnet_revenue',  'onnet_usage', 'onnet_voice_count', \n        'offnet_revenue', 'offnet_usage', 'offnet_voice_count',  \n        'idd_revenue', 'idd_usage', 'idd_voice_count',\n        'voice_rmg_revenue', 'voice_rmg_usage', 'voice_rmg_count', \n        'data_rmg_revenue',  'data_rmg_usage',  \n        'data_revenue', 'data_usage', 'sms_revenue', 'sms_usage',  'sms_idd_revenue', \n        'sms_idd_usage', 'magik_voice_amount', 'rbt_subscription_rev', 'emergency_credit_rev', \n        'package_revenue', 'voice_rev', 'sms_rev', 'onn_rev', 'off_rev', 'total_data_rev', \n        'vas_rev', 'vas_rev_others', 'total_revenue', 'total_voice_count', 'total_voice_duration', \n        'total_mainaccount_data_usage', 'total_sms_count', 'total_package_count', 'total_other_vas_count', \n        'total_voice_usage', 'total_data_usage', 'total_sms_usage'\n        ]\n\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \nimport logging\nimport os\n\ndef get_file_names():\n    return  { \n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"rfm_purchase\":   {\n        \"p1\": \"purchase_nov_full_df.csv\",\n        \"p2\": \"purchase_dec_full_df.csv\",\n        \"p3\": \"purchase_jan_full_df.csv\",\n        \"p4\": \"purchase_feb_full_df.csv\",\n        \"p5\": \"purchase_march_full_df.csv\" },\n        \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        }\n        }\n\n\n\n    # source_purchase_and_etl_location = \"/log/magikuser/tnm_autopilot_calender_month\"\n    # purchase_location = \"/log/magikuser/autopilot_data/purchase\"\n    # etl_location = \"/log/magikuser/autopilot_data/etl/etl_with_130_columns\"\n    # pack_info_location = '/log/magikuser/autopilot_data/packinfo'\n    # ml_location = '/log/magikuser/autopilot_data/ml'\n    #\n    \n\n\nclass SegmentInformation(Base):\n    __tablename__ = \"APA_segment_information_new\"\n    id = Column(Integer, primary_key=True, index=True)\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\n    end_date = Column(DateTime)\n    current_product = Column(String(80), nullable=True, unique=False)\n    current_products_names = Column(String(200), nullable=True, unique=False)\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\n    predicted_arpu = Column(Integer, nullable=True)\n    current_arpu = Column(Integer, nullable=True)\n    segment_length = Column(String(80), nullable=True, unique=False)\n    rule = Column(LONGTEXT, nullable=True)\n    actual_rule = Column(LONGTEXT, nullable=True)\n    uplift_percent = Column(Float(precision=2), nullable=True)\n    incremental_revenue = Column(Float(precision=2), nullable=True)\n    campaign_type = Column(String(80), nullable=True, unique=False)\n    campaign_name = Column(String(80), nullable=True, unique=False)\n    action_key = Column(String(80), nullable=True, unique=False)\n    robox_id = Column(String(80), nullable=True, unique=False)\n    dag_run_id = Column(String(80), nullable=True, unique=False)\n    samples = Column(Integer, nullable=False)\n    segment_name = Column(String(80), nullable=True, unique=False)\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\n    customer_status = Column(String(80), nullable=True, unique=False)\n    query = Column(LONGTEXT, nullable=True, unique=False)\n    cluster_no = Column(Integer, nullable=True)\n    confidence = Column(Float(precision=2), nullable=True)\n    recommendation_type = Column(String(80), nullable=True, unique=False)\n    cluster_description = Column(LONGTEXT, nullable=True)\n    actual_target_count = Column(Integer, nullable=True)\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\n    total_revenue= Column(Integer, nullable=True)\n    uplift_revenue=Column(Integer, nullable=True)\n\n    def __repr__(self):\n        return 'SegmentInformation(name=%s)' % self.name\n\nclass SegementInfo(BaseModel):\n    end_date: Optional[str] = None\n    dag_run_id: Optional[str] = None\n    current_product: Optional[str] = None\n    current_products_names: Optional[str] = None\n    recommended_product_id: Optional[str] = None\n    recommended_product_name: Optional[str] = None\n    predicted_arpu: Optional[int] = None\n    current_arpu: Optional[int] = None\n    segment_length: Optional[str] = None\n    rule: Optional[str] = None\n    actual_rule: Optional[str] = None\n    uplift_percent: Optional[float] = None\n    incremental_revenue: Optional[float] = None\n    campaign_type: Optional[str] = None\n    campaign_name: Optional[str] = None\n    action_key: Optional[str] = None\n    robox_id: Optional[str] = None\n    samples: Optional[int] = None\n    segment_name: Optional[str] = None\n    current_ARPU_band: Optional[str] = None\n    current_revenue_impact: Optional[str] = None\n    customer_status: Optional[str] = None\n    query: Optional[str] = None\n    cluster_no: Optional[int] = None\n    confidence: Optional[float] = None\n    recommendation_type: Optional[str] = None\n    cluster_description: Optional[str] = None\n    actual_target_count: Optional[str] = None\n    top_purchased_day_1: Optional[str] = None\n    top_purchased_day_2: Optional[str] = None\n    top_purchased_day_3: Optional[str] = None\n    next_purchase_date_range: Optional[str] = None\n    campaign_response_percentage: Optional[str] = None\n    total_revenue: Optional[int] = None\n    uplift_revenue: Optional[int] = None\n\nclass SegementRepo:\n    def create(db: Session, segement: SegementInfo):\n        db_item = SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\n                                            campaign_type=segement.campaign_type,\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\n                                            rule=segement.rule, samples=segement.samples,\n                                            campaign_name=segement.campaign_name,\n                                            recommended_product_id=segement.recommended_product_id,\n                                            recommended_product_name=segement.recommended_product_name,\n                                            current_product=segement.current_product,\n                                            current_products_names=segement.current_products_names,\n                                            segment_length=segement.segment_length,\n                                            current_ARPU_band=segement.current_ARPU_band,\n                                            current_revenue_impact=segement.current_revenue_impact,\n                                            customer_status=segement.customer_status,\n                                            segment_name=segement.segment_name, query=segement.query,\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\n                                            recommendation_type=segement.recommendation_type,\n                                            cluster_description=segement.cluster_description,\n                                            actual_target_count=segement.actual_target_count,\n                                            top_purchased_day_1=segement.top_purchased_day_1,\n                                            top_purchased_day_2=segement.top_purchased_day_2,\n                                            top_purchased_day_3=segement.top_purchased_day_3,\n                                            next_purchase_date_range=segement.next_purchase_date_range,\n                                            campaign_response_percentage=segement.campaign_response_percentage,\n                                            total_revenue=segement.total_revenue,\n                                            uplift_revenue=segement.uplift_revenue,\n                                            )\n        db.add(db_item)\n        db.commit()\n        db.refresh(db_item)\n        return db_item\n\n    def findByAutoPilotIdAndClusterNo(db: Session, _id, cluster_no):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.cluster_no == cluster_no) \\\n            .all()\n\n    def findByAutoPilotIdAndSegementName(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).first()\n\n    def findByAutoPilotIdAndSegementNameAll(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).all()\n    \n    def findByAutoPilotIdAndSegementNamewithoutcluster(db: Session, _id, segement_name):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name).all()\n            \n\n\n    def findByAutoPilotId(db: Session, _id):\n        return db.query(SegmentInformation).filter(SegmentInformation.dag_run_id == _id).all()\n\n    def findByAutoPilotIdRecommendedId(db: Session, _id,recommended_product_id):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.recommended_product_id == recommended_product_id).all()\n\n    def deleteById(db: Session, _ids):\n        for id in _ids:\n            db.query(SegmentInformation).filter(SegmentInformation.id == id).delete()\n            db.commit()\n\n    def update(db: Session, item_data):\n        updated_item = db.merge(item_data)\n        db.commit()\n        return updated_item\n\ndef weekday_product_purchase_count(dag_run_id):\n    \n    msisdn_name =  MSISDN_COL_NAME\n    purchase_no_months = ['m1', 'm2', 'm3']\n    file_name_dict = get_file_names()\n    print(file_name_dict)\n    print(\"finding weekday_puchase_count ongoing \")\n    purchase = {}\n    \n    for month in purchase_no_months:\n        purchase[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"purchase\").get(month)),)\n\n    \n    df_list = [purchase['m1'],purchase['m2'],purchase['m3']]\n    pivoted_dict = {}\n    for i, df in enumerate(df_list, 1):\n        df['cdr_date'] = dd.to_datetime(df['cdr_date'])  \n        df = df[df['cdr_date'].dt.dayofweek < 7]\n        df['day_of_week'] = df['cdr_date'].dt.day_name()\n        df['day_of_week'] = df['day_of_week'].astype('category').cat.as_known()\n        pivoted = df.pivot_table(index='product_id', columns='day_of_week', values='total_cnt', aggfunc='sum')\n        pivoted_dict[f\"m{i}\"] = pivoted\n    pivot_tables = [pivoted_dict['m1'],pivoted_dict['m2'], pivoted_dict['m3']]\n    \n    merged_pivot = reduce(lambda left, right: dd.merge(left, right, on='product_id', how='outer'), pivot_tables)\n    merged_pivot = merged_pivot.fillna(0)\n    \n    columns_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    \n    for day in columns_order:\n        merged_pivot[f\"{day.lower()}\"] = merged_pivot[f\"{day}_x\"] + merged_pivot[f\"{day}_y\"] + merged_pivot[day]\n    columns  = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\n    merged_pivot['total'] = merged_pivot[columns].sum(axis=1)\n    \n    merged_pivot = merged_pivot.sort_values(by='total', ascending=False)\n    \n    merged_pivot = merged_pivot.drop(columns=['Monday_x', 'Tuesday_x', 'Wednesday_x', 'Thursday_x', 'Friday_x', 'Saturday_x', 'Sunday_x',\n                           'Monday_y', 'Tuesday_y', 'Wednesday_y', 'Thursday_y', 'Friday_y', 'Saturday_y', 'Sunday_y',\n                           'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n    merged_pivot['top_purchased_day_1'] = merged_pivot[columns].idxmax(axis=1)\n    merged_pivot['top_purchased_day_2'] = merged_pivot[columns].apply(lambda x: x.nlargest(2).index[1], axis=1)\n    merged_pivot['top_purchased_day_3'] = merged_pivot[columns].apply(lambda x: x.nlargest(3).index[2], axis=1)\n    merged_pivot = merged_pivot.reset_index()\n\n    merged_pivot_pd =merged_pivot.compute()\n    \n    weekday_product_purchase_count_path = os.path.join(ml_location,  \"weekday_product_purchase_count.csv\")\n    \n    print(\"weekday_product_purchase_count   file output is ongoing \")\n    merged_pivot_pd.to_csv(weekday_product_purchase_count_path,header=True,index=False)\n    \n    \n    \ndef recommended_id_prediction(dag_run_id):\n    \n    file_path = ml_location+'predicted_result'\n    s = pd.read_parquet(file_path, engine='pyarrow').reset_index(drop=True)\n    \n    def transform_value(value):\n        \n        if value ==0:\n            return '0-7'\n        elif value==1:\n            return '7-14'\n        elif value ==2:\n            return '14-30'\n        else:\n            return '30+'\n\n    # Apply the transformation using the apply function\n    s['PredictedValues'] = s['PredictedValues'].fillna(0)\n    s['PredictedValues'] = s['PredictedValues'].astype(int)\n    s['range'] = s['PredictedValues'].apply(transform_value)\n    \n    result = s.groupby(['recommended_product_id', 'PredictedValues']).size().reset_index(name='PredictedValues_counts')\n    max_counts = result.groupby('recommended_product_id')['PredictedValues_counts'].idxmax()\n    result = result.loc[max_counts]\n    result['range'] = result['PredictedValues'].apply(transform_value)\n\n    path = os.path.join(ml_location,  'recommended_id_prediction.csv')   \n    result.to_csv(path,index=False,header=True)    \n    return result\n    \ndef nextthreemonth(purchase, dag_run_id):\n\n\n    file_path = ml_location+'output_data/APA_output.csv'\n\n    fin = dd.read_csv(file_path)\n    li = fin['recommended_product_id'].unique().compute().tolist()\n\n    file_name_dict = get_file_names()\n\n    pack = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"pack\").get('pack')),usecols=pack_cols)#packinfo loading\n    \n    source1 = dd.concat([purchase['p2'], purchase['p3'], purchase['p4']], axis=0, ignore_index=True, verify_integrity=False)\n    target1 = dd.concat([purchase['p5']])\n    \n    \n    result = pd.DataFrame({'msisdn': [], 'PredictedValues': [],'recommended_product_id': [], 'product_type': []})\n    result = dd.from_pandas(result, npartitions=2)\n\n    for i in li:\n        \n        msi = fin[fin['recommended_product_id'] == i][['msisdn']]                \n        \n        source =  merge(source1,msi,how='inner',on='msisdn')\n        target =  merge(target1,msi,how='inner',on='msisdn')\n        if len(source)<50   or len(target)<50:\n            continue     \n            \n        print('len source ',len(source))\n        print('source.column ',source.columns)\n        print('len target ',len(target))\n        print('target.column ',target.columns)\n        final = nextpurchase(source,target,'file',dag_run_id)\n\n        x,x1= final.drop(['NextPurchaseDayRange','msisdn','NextPurchaseDay'],axis=1),final['msisdn']   \n                        \n\n        c = pack[pack['product_id'] == i]['product_type'].compute().tolist()           \n            \n            \n        feature_path = os.path.join(ml_location,  c[0]+\"_feature.pkl\")  \n        columns = pickle.load(open(feature_path, 'rb'))\n            \n        output_list = [j for j in columns if j not in x.columns]\n            \n        for k in output_list:\n            x[k]=0 \n            \n        pickel_path = os.path.join(ml_location,c[0]+\".pkl\")\n        loaded_model = pickle.load(open(pickel_path, 'rb'))\n        feature_names = loaded_model.get_booster().feature_names\n        x=x[feature_names]\n            \n        y = loaded_model.predict(x)    \n        y_df = dd.from_array(y, columns=['PredictedValues'])\n\n        predict = dd.concat([x1, y_df], axis=1)\n        predict['recommended_product_id'] = i\n        predict['product_type'] = c[0]\n\n        result = dd.concat([result, predict], axis=0)            \n        \n    path = os.path.join(ml_location, 'predicted_result') \n    result.to_parquet(path,schema='infer') \n    \n    \ndef xgmodel(final, name, dag_run_id):\n\n\n\n    X, y = final.drop(['NextPurchaseDayRange','msisdn','NextPurchaseDay'],axis=1), final.NextPurchaseDayRange\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)\n    \n    \n    path = os.path.join(ml_location,  name+\"_feature.pkl\")\n    pickle.dump(X.columns.tolist(), open(path, 'wb'))\n        \n    \n    rus = RandomUnderSampler(random_state=42)\n    X_train, y_train = rus.fit_resample(X, y)\n    print('Resampled dataset shape %s' % Counter(y_train))    \n    \n\n    xgb_model = xgb.XGBClassifier().fit(X_train, y_train)\n    \n    print('Accuracy of XGB classifier on training set: {:.2f}'\n           .format(xgb_model.score(X_train, y_train)))\n    print('Accuracy of XGB classifier on test set: {:.2f}'\n           .format(xgb_model.score(X_test[X_train.columns], y_test)))\n\n\n    y_pred = xgb_model.predict(X_test)\n    print(classification_report(y_test, y_pred))\n \n    \n    path = os.path.join(ml_location,  name+\".pkl\")    \n    pickle.dump(xgb_model, open(path, 'wb'))\n    print('model created ')\n    \ndef segmentaion_fun1(r, f, m):\n    segments = []\n    for r, f, m in zip(r, f, m):\n        value = int(f\"{r}{f}{m}\")\n\n        if value in [555, 554, 544, 545, 454, 455, 445]:\n            segments.append(\"Champions\")\n        elif value in [543, 444, 435, 355, 354, 345, 344, 335]:\n            segments.append(\"Loyal_Customers\")\n        elif value in [553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323]:\n            segments.append(\"Potential_Loyalist\")\n        elif value in [512, 511, 422, 421, 412, 411, 311]:\n            segments.append(\"Recent_Customers\")\n        elif value in [525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313]:\n            segments.append(\"Promising_Customers\")\n        elif value in [535, 534, 443, 434, 343, 334, 325, 324]:\n            segments.append(\"Customers_needing_Attention\")\n        elif value in [331, 321, 312, 221, 213]:\n            segments.append(\"About_to_Sleep\")\n        elif value in [255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124]:\n            segments.append(\"At_Risk\")\n        elif value in [155, 154, 144, 214, 215, 115, 114, 113]:\n            segments.append(\"Cant_Loose_them\")\n        elif value in [332, 322, 231, 241, 251, 233, 232, 223, 222, 132, 123, 122, 212, 211]:\n            segments.append(\"Hibernating\")\n        else:\n            segments.append(\"Lost\")\n\n    return segments\n    \n\n\ndef join_rfm(x):\n    return str(int(x['R_Score'])) + str(int(x['F_Score'])) + str(int(x['M_Score']))\n    \n\ndef form_segements_purchase_rfm(ctm_class):\n    def process_duplicates(li):\n        for i in range(len(li) - 1):\n            print(li[i], i)\n            if i + 1 == len(li) - 1:\n                li[i] = ((li[i - 1] + li[i]) / 2)\n            elif li[i] == li[i + 1]:\n                li[i + 1] = ((li[i + 1] + li[i + 2]) / 2)\n        return li\n\n    # bins_recency = [-1,\n    #                 np.percentile(ctm_class[\"Recency\"], 20),\n    #                 np.percentile(ctm_class[\"Recency\"], 40),\n    #                 np.percentile(ctm_class[\"Recency\"], 60),\n    #                 np.percentile(ctm_class[\"Recency\"], 80),\n    #                 ctm_class[\"Recency\"].max().compute()]\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n    # # bins_recency = [-1,17,35,53,71, ctm_class[\"Recency\"].max().compute()]\n\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Recency']=ctm_class1['Recency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Recency').agg({'msisdn':'count'}).reset_index()\n    mean_recency = ctm_class1['Recency'].max().compute()/5\n    print(\"mean_recency\", mean_recency)\n    print(\"mean_recency *2\",mean_recency * 2)\n    print(\"mean_recency *3\",mean_recency * 3)\n    print(\"mean_recency *4\",mean_recency * 4)\n    \n    \n    bins_recency = [-1,\n                    int(mean_recency),\n                    int(mean_recency * 2),\n                    int(mean_recency * 3),\n                    int(mean_recency * 4),\n                    ctm_class[\"Recency\"].max().compute()]\n    \n    print('bins_recency is', bins_recency)\n\n\n    ctm_class['R_Score'] = ctm_class[\"Recency\"].map_partitions(pd.cut,\n                                                               bins=bins_recency,\n                                                               labels=[5, 4, 3, 2, 1]).astype(\"int\")\n\n    \n\n    # bins_frequency = [-1,\n    #                   np.percentile(ctm_class[\"Frequency\"], 20),\n    #                   np.percentile(ctm_class[\"Frequency\"], 40),\n    #                   np.percentile(ctm_class[\"Frequency\"], 60),\n    #                   np.percentile(ctm_class[\"Frequency\"], 80),\n    #                   ctm_class[\"Frequency\"].max().compute()]\n\n    # bins_frequency.sort()\n    # bins_frequency = process_duplicates(bins_frequency)\n\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Frequency']=ctm_class1['Frequency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Frequency').agg({'msisdn':'count'}).reset_index()\n    mean_frequency = ctm_class1['Frequency'].max().compute()/5\n    print(\"mean_frequency\", mean_frequency)\n    print(\"mean_frequency *2\",mean_frequency * 2)\n    print(\"mean_frequency *3\",mean_frequency * 3)\n    print(\"mean_frequency *4\",mean_frequency * 4)\n\n\n    bins_frequency = [-1,\n                    int(mean_frequency),\n                    int(mean_frequency * 2),\n                    int(mean_frequency * 3),\n                    int(mean_frequency * 4),\n                    ctm_class[\"Frequency\"].max().compute()]\n    \n    print('bins_frequency is', bins_frequency)\n    ctm_class['F_Score'] = ctm_class[\"Frequency\"].map_partitions(pd.cut,\n                                                                 bins=bins_frequency,\n                                                                 labels=[1, 2, 3, 4, 5]).astype(\"int\")\n\n    # bins_revenue = [-1,\n    #                 np.percentile(ctm_class[\"Revenue\"], 20),\n    #                 np.percentile(ctm_class[\"Revenue\"], 40),\n    #                 np.percentile(ctm_class[\"Revenue\"], 60),\n    #                 np.percentile(ctm_class[\"Revenue\"], 80),\n    #                 ctm_class[\"Revenue\"].max().compute()]\n    # bins_revenue.sort()\n    # bins_revenue = process_duplicates(bins_revenue)\n\n    # bins_revenue = [-1,189,380,571,762, ctm_class[\"Revenue\"].max().compute()]\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Revenue'] = ctm_class1['Revenue'].map_partitions(lambda series: series.apply(np.ceil))\n    ctm_class1['Revenue']=ctm_class1['Revenue'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Revenue').agg({'msisdn':'count'}).reset_index()\n    mean_revenue = ctm_class1['Revenue'].max().compute()/5\n    \n    print(\"mean_revenue\", mean_revenue)\n    print(\"mean_revenue *2\",mean_revenue * 2)\n    print(\"mean_revenue *3\",mean_revenue * 3)\n    print(\"mean_revenue *4\",mean_revenue * 4)\n    \n    bins_revenue = [-1,\n                    int(mean_revenue),\n                    int(mean_revenue * 2),\n                    int(mean_revenue * 3),\n                    int(mean_revenue * 4),\n                    ctm_class[\"Revenue\"].max().compute()]\n    \n    print('bins_revenue is', bins_revenue)\n\n\n    ctm_class['M_Score'] = ctm_class[\"Revenue\"].map_partitions(pd.cut,\n                                                               bins=bins_revenue,\n                                                               labels=[1, 2, 3, 4, 5]).astype(\"int\")\n    print(\"done with scoring\")\n    # Form RFM segment\n\n    ctm_class['RFM_Segment'] = ctm_class.apply(join_rfm, axis=1)\n    ctm_class['RFM_Segment'] = ctm_class['RFM_Segment'].astype(int)\n    print(\"formed rfm segement \")\n    ctm_class['R_Score'] = ctm_class['R_Score'].astype(int)\n    ctm_class['F_Score'] = ctm_class['F_Score'].astype(int)\n    ctm_class['M_Score'] = ctm_class['M_Score'].astype(int)\n    print(\"computing rfm\")\n    r = ctm_class['R_Score'].values.compute()\n    f = ctm_class['F_Score'].values.compute()\n    m = ctm_class['M_Score'].values.compute()\n    seg = segmentaion_fun1(r, f, m)\n    chunks = ctm_class.map_partitions(lambda x: len(x)).compute().to_numpy()\n    myarray = da.from_array(seg, chunks=tuple(chunks))\n    ctm_class['Segment'] = myarray\n    return ctm_class\n    \n    \ndef otliner_removal(df,col, per=0.97):\n    q = df[col].quantile(per)\n    print(f\"col is {col} and q is {q}\")\n    print(\"the length brfore is\", len(df))\n    outliers = df[df[col] > q]\n    df1 = df[~df['msisdn'].isin(outliers['msisdn'].compute().values)]\n    print(\"the length after is\", len(df1))\n    return df1\n    \ndef perform_rfm_on_purchase(df_data_rfm, period=95):\n\n    ic(\"inside perform_rfm\")\n    \n    df_data_rfm.purchase_date = dd.to_datetime(df_data_rfm.purchase_date)\n    df_data_rfm = df_data_rfm.fillna(0)\n    msisdn_name = MSISDN_COL_NAME\n    # current behaviour data\n    min_date = df_data_rfm['purchase_date'].min().compute()\n    cur_beh_data_rfm = df_data_rfm[(df_data_rfm.purchase_date <= pd.Timestamp(min_date) + pd.Timedelta(days=period))\n                                   & (df_data_rfm.purchase_date >= pd.Timestamp(min_date))].reset_index(drop=True)\n    # Get the maximum purchase date of each customer and create a dataframe with it together with the customer's id.\n    ctm_max_purchase = cur_beh_data_rfm.groupby(msisdn_name).purchase_date.max().reset_index()\n    ctm_max_purchase.columns = [msisdn_name, 'MaxPurchaseDate']\n\n    # Find the recency of each customer in days\n    ctm_max_purchase['Recency'] = (\n            ctm_max_purchase['MaxPurchaseDate'].max() - ctm_max_purchase['MaxPurchaseDate']).dt.days\n    ctm_max_purchase = ctm_max_purchase.drop(columns=['MaxPurchaseDate'])\n    print(\"done with recency \")\n    # frequency\n    ctm_frequency = cur_beh_data_rfm.groupby(msisdn_name).total_cnt.sum().reset_index()\n    ctm_frequency.columns = [msisdn_name, 'Frequency']\n    print(\"done with frequency \")\n    cur_beh_data_rfm['Revenue'] = cur_beh_data_rfm[RECHARGE_TRANSACTION_PRICE_COL_NAME]\n    ctm_revenue = cur_beh_data_rfm.groupby('msisdn').Revenue.sum().reset_index()\n    print(\"done with monitory \")\n    rfm_data_base = dd.concat([ctm_max_purchase, ctm_revenue, ctm_frequency], axis=1)\n    rfm_data_base = rfm_data_base.loc[:, ~rfm_data_base.columns.duplicated()]\n\n    #for tnm purpose start\n    #rfm_data_base=otliner_removal(rfm_data_base,'Frequency')\n    rfm_data_base=otliner_removal(rfm_data_base,'Revenue')\n    \n    #for tnm purpose end \n\n\n    return rfm_data_base\n    \ndef rfm_process_quantile_method_purchase(df_data):\n    try: \n        \n        df_data = df_data.fillna(0, )\n\n        print(len(df_data))\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        ctm_class = perform_rfm_on_purchase(df_data, period=95)\n\n        print(len(ctm_class))\n        \n        ctm_class = form_segements_purchase_rfm(ctm_class)\n\n        print(len(ctm_class))\n        \n        return ctm_class\n\n    except Exception as e:\n        print(e) \n        \n\ndef NextPurchaseDayRange(tx_class):\n    \n    tx_class['NextPurchaseDayRange'] = 0\n\n    # Use Dask function 'where' to perform conditional assignments\n    tx_class['NextPurchaseDayRange'] = tx_class['NextPurchaseDayRange'].where(\n        ~((tx_class['NextPurchaseDay'] > 7) & (tx_class['NextPurchaseDay'] <= 14)), 1\n    )\n    tx_class['NextPurchaseDayRange'] = tx_class['NextPurchaseDayRange'].where(\n        ~((tx_class['NextPurchaseDay'] > 14) & (tx_class['NextPurchaseDay'] <= 30)), 2\n    )\n    tx_class['NextPurchaseDayRange'] = tx_class['NextPurchaseDayRange'].where(\n        ~(tx_class['NextPurchaseDay'] > 30), 3\n    )\n\n    tx_class = tx_class.fillna(0)\n    \n    return tx_class\n    \n    \ndef nextpurchase(source, target,filename, dag_run_id):    \n    \n    \n    source['cdr_date'] = dd.to_datetime(source['cdr_date'])\n    target['cdr_date'] = dd.to_datetime(target['cdr_date'])\n\n    tx_user = source['msisdn'].unique().to_frame(name='msisdn')\n\n    # Last month\n    tx_next_first_purchase = target.groupby('msisdn').cdr_date.min().reset_index()\n    tx_next_first_purchase.columns = ['msisdn', 'MinPurchaseDate']\n\n    # Two-month data\n    tx_last_purchase = source.groupby('msisdn').cdr_date.max().reset_index()\n    tx_last_purchase.columns = ['msisdn', 'MaxPurchaseDate']\n\n    # Merge two data\n    tx_purchase_dates = merge(tx_last_purchase, tx_next_first_purchase, on='msisdn', how='left')\n\n    # Calculating difference\n    tx_purchase_dates['NextPurchaseDay'] = (tx_purchase_dates['MinPurchaseDate'] - tx_purchase_dates['MaxPurchaseDate']).dt.days\n\n    # Merge with tx_user\n    tx_user = merge(tx_user, tx_purchase_dates[['msisdn', 'NextPurchaseDay']], on='msisdn', how='left')\n    tx_user = tx_user.fillna(999)\n\n    # Create a Dask dataframe with msisdn and cdr_date\n    tx_day_order = source[['msisdn', 'cdr_date']].compute()\n\n    # Convert cdr_date to day\n    tx_day_order['InvoiceDay'] = tx_day_order['cdr_date'].dt.date\n    tx_day_order = tx_day_order.sort_values(['msisdn', 'cdr_date'])\n\n    # Drop duplicates\n    tx_day_order = tx_day_order.drop_duplicates(subset=['msisdn', 'cdr_date'], keep='first')\n\n    # Shifting last 3 purchase dates\n    tx_day_order['PrevInvoiceDate'] = tx_day_order.groupby('msisdn')['InvoiceDay'].shift(1)\n    tx_day_order['T2InvoiceDate'] = tx_day_order.groupby('msisdn')['InvoiceDay'].shift(2)\n    tx_day_order['T3InvoiceDate'] = tx_day_order.groupby('msisdn')['InvoiceDay'].shift(3)\n\n    tx_day_order['DayDiff'] = (tx_day_order['InvoiceDay'] - tx_day_order['PrevInvoiceDate']).dt.days\n    tx_day_order['DayDiff2'] = (tx_day_order['InvoiceDay'] - tx_day_order['T2InvoiceDate']).dt.days\n    tx_day_order['DayDiff3'] = (tx_day_order['InvoiceDay'] - tx_day_order['T3InvoiceDate']).dt.days\n    \n    \n\n    tx_day_diff = tx_day_order.groupby('msisdn').agg({'DayDiff': ['mean', 'std']}).reset_index()\n    tx_day_diff.columns = ['msisdn', 'DayDiffMean', 'DayDiffStd']\n\n    tx_day_order_last = tx_day_order.drop_duplicates(subset=['msisdn'], keep='last')\n    tx_day_order_last = tx_day_order_last.fillna(0)\n\n    tx_day_order_last = merge(tx_day_order_last, tx_day_diff, on='msisdn')\n    tx_user = merge(tx_user, tx_day_order_last[['msisdn','DayDiff', 'DayDiff2', 'DayDiff3', 'DayDiffMean', 'DayDiffStd']], on='msisdn')\n    \n    \n    \n    #create tx_class as a copy of tx_user before applying get_dummies\n    tx_class = tx_user.copy()\n\n    tx_class = NextPurchaseDayRange(tx_class)   \n\n    \n    rfm = rfm_process_quantile_method_purchase(source)\n\n    rfm = rfm.compute()\n    final= merge(rfm,tx_class,how='inner',on='msisdn')    \n    \n    final = final.categorize(columns=['Segment'])\n    final = dd.get_dummies(final, columns=['Segment'])\n    \n    path = os.path.join(ml_location,  filename)   \n    Path(path).mkdir(parents=True, exist_ok=True)\n    final.to_parquet(path)\n    \n    \n    return final.compute()\n    \n\n\ndef next_purchase_date_model_creation(dag_run_id, db):\n        file_name_dict = get_file_names()\n        \n        purchase = {}\n        for month in purchase_no_months_for_next_purchase:\n            purchase[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"rfm_purchase\").get(month)))\n\n        for key, value in purchase.items():\n            value = value.reset_index(drop=True)    \n        \n        \n        source = dd.concat([purchase['p1'], purchase['p2'], purchase['p3']], axis=0, ignore_index=True, verify_integrity=False)\n        target = dd.concat([purchase['p4']])    \n        \n        \n        pack = dd.read_csv(\"/home/tnmops/seahorse3_bkp/pack_info.csv\",usecols=['product_id','product_type'])\n        f = merge(source,pack,how='inner',on='product_id')\n        f1 = merge(target,pack,how='inner',on='product_id')\n        \n        \n        p_type = ['DATA','VOICE','COMBO']\n        \n        for i in p_type:\n\n            print('the product type is------------ ',i)\n            \n            source = f[f['product_type']==i]\n            target = f1[f1['product_type']==i]\n        \n        \n            final = nextpurchase(source,target,i+\"_model\",dag_run_id)\n            xgmodel(final,i, dag_run_id)\n        nextthreemonth(purchase, dag_run_id)\n        recommended_id_prediction(dag_run_id)\n\n\n        weekday_product_purchase_count_path = os.path.join(ml_location,  \"weekday_product_purchase_count.csv\")\n        weekday_product_purchase_count(dag_run_id)\n        weekday_product_purchase_count_df = pd.read_csv(weekday_product_purchase_count_path)\n        print('readed weekday_product_purchase_count_df')\n        next_purchase_date_path = os.path.join(ml_location, 'recommended_id_prediction.csv')   \n        \n        next_purchase_date_df = pd.read_csv(next_purchase_date_path)\n        print('readed next_purchase_date_df')\n        next_purchase_date_df['recommended_product_id']=next_purchase_date_df['recommended_product_id'].astype(int)\n        print('recommended_product_id converted to int')\n        segements_two = SegementRepo.findByAutoPilotId(db=db, _id=dag_run_id)\n        for seg in segements_two:\n            if seg.recommended_product_id is  None or seg.recommended_product_id ==0:\n                ic(\"deleteing segement if recommended_product_id is none \")\n                print(\"seg.id 1  is \",seg.id)\n                SegementRepo.deleteById(db=db, _ids=[seg.id])\n        segements_three = SegementRepo.findByAutoPilotId(db=db, _id=dag_run_id)\n     \n        for seg in segements_three:   \n            print(\"seg.id 2  is \",seg.id)\n            print('seg.recommended_product_id',seg.recommended_product_id)\n            \n            recommended_pid = int(seg.recommended_product_id)\n            print('recommended_pid is ',recommended_pid)\n            print('type recommended_pid is ',type(recommended_pid))\n\n            print('type next_purchase_date_df is ',next_purchase_date_df.dtypes)\n\n            print('len is ', weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id'] == recommended_pid])\n            print(\"weekday is \", weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0])\n     \n            seg.top_purchased_day_1 = weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0]\n            seg.top_purchased_day_2=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_2'].iloc[0]\n            seg.top_purchased_day_3=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_3'].iloc[0]\n            if recommended_pid in next_purchase_date_df['recommended_product_id'].unique():\n                seg.next_purchase_date_range=next_purchase_date_df[next_purchase_date_df['recommended_product_id']==recommended_pid]['range'].iloc[0]\n            else:\n                seg.next_purchase_date_range=\"30+\" #default\n            SegementRepo.update(db=db, item_data=seg)\n            \n        print(\"next purchase is completed\")\n        return pack\n    \n\ndef transform(dataframe):\n    db = get_db()\n    df = next_purchase_date_model_creation(\"manual__2023-07-10T11:06:51\",db)\n    return dataframe"
      }
    }, {
      "id": "b827c05d-8b4f-901c-d11f-2af2ae56201c",
      "operation": {
        "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
        "name": "Read DataFrame"
      },
      "parameters": {
        "data source": "70c171c2-cf38-7864-b7c9-e26211f7eff7"
      }
    }, {
      "id": "7aa6bc57-2961-2e85-cb94-e1d6e61dbe32",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom urllib.parse import quote  \nfrom datetime import datetime\nfrom icecream import ic\nfrom pathlib import Path\nimport dask.array as da\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\npurchase_location = '/home/tnmops/seahorse3_bkp/'\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\netl_location = '/home/tnmops/seahorse3_bkp/'\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nlog_file = '/home/tnmops/seahorse3_bkp/'\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\n\n\n\n\n\ndef write_pickle(data, path):\n    with open(path, 'wb') as handle:\n        print(f'opened {path} ')\n        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        print(f'data dumped  {data}')\n        \n\n\ndef get_features(df):\n    numerical_cols = df.select_dtypes(include=[np.number]).columns\n    df=df[numerical_cols]\n    X = df.drop(['msisdn', 'label'], axis=1)\n    y = df['label']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    print(f\"X_train.shape = {X_train.shape}, X_test.shape =  {X_test.shape}\")\n    # sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\n    sel = SelectFromModel(DecisionTreeClassifier(max_leaf_nodes=12))\n    sel.fit(X_train, y_train)\n    sel.get_support()\n    features = X_train.columns[sel.get_support()]\n    print(\"the feature selection features are \", features)\n    selected_features = [feature for feature in features if feature != 'label']\n    \n    return selected_features\n    \n    \n\n\n\ndef feature_selection(dag_run_id):\n    try:\n        path_d = os.path.join(ml_location, \"dict.pickle\")\n        features_path = os.path.join(ml_location, \"features.pickle\")\n        with open(path_d, 'rb') as handle:\n            data = pickle.load(handle)\n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\n        # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\n        # for key, value in data.items():\n        #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n            \n        filtered_dict = {k: v for k, v in data.items()}\n        result_features = {}\n        for item, val in filtered_dict.items():\n            # val is the path item is the segment name\n            df = pd.read_csv(val)\n            features = get_features(df)#-----------------------------\n            result_features[item] = features\n            print(f\"the features are {features} for the segement {item}\")\n        \n        return df\n        write_pickle(data=result_features, path=features_path)\n    except Exception as e:\n        print(e)\n        #raise HTTPException(status_code=400, detail=\"error occoureds in k_modes\" + str(e))\n    #return schemas.BaseResponse(statusCode=200, message=\"success\", status=\"success\")\n    \n    \ndef transform(dataframe):\n    df = feature_selection(\"manual__2023-07-10T11:06:51\" )\n    return df"
      }
    }, {
      "id": "2e771569-425d-6d2d-acca-baefc3094ecc",
      "operation": {
        "id": "643d8706-24db-4674-b5b4-10b5129251fc",
        "name": "Transform"
      },
      "parameters": {
        "Parameters of input Transformer": {

        }
      }
    }, {
      "id": "7088c32f-af5e-aa2a-0f18-91a1f307ade4",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "from datetime import datetime, timedelta,date\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nfrom sklearn.svm import SVC\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom icecream import ic\nfrom pathlib import Path\nimport dask.array as da\nfrom dask.dataframe import merge\nimport pickle\nimport pathlib\nimport dask.dataframe as dd\n\n\nimport pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom urllib.parse import quote  \n\n\n\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\nfrom sqlalchemy.orm import relationship\nimport datetime\nfrom sqlalchemy.dialects.mysql import LONGTEXT\n\n\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\n\n\n\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\n\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URL,\n)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        return db\n    finally:\n        db.close()\n        \n\n\n\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\npurchase_location = '/home/tnmops/seahorse3_bkp/'\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\netl_location = '/home/tnmops/seahorse3_bkp/'\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nlog_file = '/home/tnmops/seahorse3_bkp/'\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCUSTOMER_NEEDED_COLUMN = [\n        'onnet_revenue',  'onnet_usage', 'onnet_voice_count', \n        'offnet_revenue', 'offnet_usage', 'offnet_voice_count',  \n        'idd_revenue', 'idd_usage', 'idd_voice_count',\n        'voice_rmg_revenue', 'voice_rmg_usage', 'voice_rmg_count', \n        'data_rmg_revenue',  'data_rmg_usage',  \n        'data_revenue', 'data_usage', 'sms_revenue', 'sms_usage',  'sms_idd_revenue', \n        'sms_idd_usage', 'magik_voice_amount', 'rbt_subscription_rev', 'emergency_credit_rev', \n        'package_revenue', 'voice_rev', 'sms_rev', 'onn_rev', 'off_rev', 'total_data_rev', \n        'vas_rev', 'vas_rev_others', 'total_revenue', 'total_voice_count', 'total_voice_duration', \n        'total_mainaccount_data_usage', 'total_sms_count', 'total_package_count', 'total_other_vas_count', \n        'total_voice_usage', 'total_data_usage', 'total_sms_usage'\n        ]\n\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n\nimport logging\nimport os\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m3\": \"usage_feb.csv\",\n            \"m4\": \"usage_jan.csv\"},\n            \n        \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan.csv\",\n            \"c4\": \"campaign_detailed_fct_dec.csv\"            \n                        \n        }\n        }\n        \nclass SegmentInformation(Base):\n    __tablename__ = \"APA_segment_information_new\"\n    id = Column(Integer, primary_key=True, index=True)\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\n    end_date = Column(DateTime)\n    current_product = Column(String(80), nullable=True, unique=False)\n    current_products_names = Column(String(200), nullable=True, unique=False)\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\n    predicted_arpu = Column(Integer, nullable=True)\n    current_arpu = Column(Integer, nullable=True)\n    segment_length = Column(String(80), nullable=True, unique=False)\n    rule = Column(LONGTEXT, nullable=True)\n    actual_rule = Column(LONGTEXT, nullable=True)\n    uplift_percent = Column(Float(precision=2), nullable=True)\n    incremental_revenue = Column(Float(precision=2), nullable=True)\n    campaign_type = Column(String(80), nullable=True, unique=False)\n    campaign_name = Column(String(80), nullable=True, unique=False)\n    action_key = Column(String(80), nullable=True, unique=False)\n    robox_id = Column(String(80), nullable=True, unique=False)\n    dag_run_id = Column(String(80), nullable=True, unique=False)\n    samples = Column(Integer, nullable=False)\n    segment_name = Column(String(80), nullable=True, unique=False)\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\n    customer_status = Column(String(80), nullable=True, unique=False)\n    query = Column(LONGTEXT, nullable=True, unique=False)\n    cluster_no = Column(Integer, nullable=True)\n    confidence = Column(Float(precision=2), nullable=True)\n    recommendation_type = Column(String(80), nullable=True, unique=False)\n    cluster_description = Column(LONGTEXT, nullable=True)\n    actual_target_count = Column(Integer, nullable=True)\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\n    total_revenue= Column(Integer, nullable=True)\n    uplift_revenue=Column(Integer, nullable=True)\n\n    def __repr__(self):\n        return 'SegmentInformation(name=%s)' % self.name\n        \n        \nclass SegementInfo(BaseModel):\n    end_date: Optional[str] = None\n    dag_run_id: Optional[str] = None\n    current_product: Optional[str] = None\n    current_products_names: Optional[str] = None\n    recommended_product_id: Optional[str] = None\n    recommended_product_name: Optional[str] = None\n    predicted_arpu: Optional[int] = None\n    current_arpu: Optional[int] = None\n    segment_length: Optional[str] = None\n    rule: Optional[str] = None\n    actual_rule: Optional[str] = None\n    uplift_percent: Optional[float] = None\n    incremental_revenue: Optional[float] = None\n    campaign_type: Optional[str] = None\n    campaign_name: Optional[str] = None\n    action_key: Optional[str] = None\n    robox_id: Optional[str] = None\n    samples: Optional[int] = None\n    segment_name: Optional[str] = None\n    current_ARPU_band: Optional[str] = None\n    current_revenue_impact: Optional[str] = None\n    customer_status: Optional[str] = None\n    query: Optional[str] = None\n    cluster_no: Optional[int] = None\n    confidence: Optional[float] = None\n    recommendation_type: Optional[str] = None\n    cluster_description: Optional[str] = None\n    actual_target_count: Optional[str] = None\n    top_purchased_day_1: Optional[str] = None\n    top_purchased_day_2: Optional[str] = None\n    top_purchased_day_3: Optional[str] = None\n    next_purchase_date_range: Optional[str] = None\n    campaign_response_percentage: Optional[str] = None\n    total_revenue: Optional[int] = None\n    uplift_revenue: Optional[int] = None\n\n\n\nclass SegementRepo:\n    def create(db: Session, segement: SegementInfo):\n        db_SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\n                                            campaign_type=segement.campaign_type,\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\n                                            rule=segement.rule, samples=segement.samples,\n                                            campaign_name=segement.campaign_name,\n                                            recommended_product_id=segement.recommended_product_id,\n                                            recommended_product_name=segement.recommended_product_name,\n                                            current_product=segement.current_product,\n                                            current_products_names=segement.current_products_names,\n                                            segment_length=segement.segment_length,\n                                            current_ARPU_band=segement.current_ARPU_band,\n                                            current_revenue_impact=segement.current_revenue_impact,\n                                            customer_status=segement.customer_status,\n                                            segment_name=segement.segment_name, query=segement.query,\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\n                                            recommendation_type=segement.recommendation_type,\n                                            cluster_description=segement.cluster_description,\n                                            actual_target_count=segement.actual_target_count,\n                                            top_purchased_day_1=segement.top_purchased_day_1,\n                                            top_purchased_day_2=segement.top_purchased_day_2,\n                                            top_purchased_day_3=segement.top_purchased_day_3,\n                                            next_purchase_date_range=segement.next_purchase_date_range,\n                                            campaign_response_percentage=segement.campaign_response_percentage,\n                                            total_revenue=segement.total_revenue,\n                                            uplift_revenue=segement.uplift_revenue\n                                            )\n        db.add(db_item)\n        db.commit()\n        db.refresh(db_item)\n        return db_item\n\n    def findByAutoPilotIdAndClusterNo(db: Session, _id, cluster_no):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.cluster_no == cluster_no) \\\n            .all()\n\n    def findByAutoPilotIdAndSegementName(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).first()\n\n    def findByAutoPilotIdAndSegementNameAll(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).all()\n    \n    def findByAutoPilotIdAndSegementNamewithoutcluster(db: Session, _id, segement_name):\n        return db.query(SegmentInformation) \\\n            .filter( SegmentInformation.dag_run_id == _id) \\\n            .filter( SegmentInformation.segment_name == segement_name).all()\n            \n\n\n    def findByAutoPilotId(db: Session, _id):\n        return db.query( SegmentInformation).filter( SegmentInformation.dag_run_id == _id).all()\n\n    def findByAutoPilotIdRecommendedId(db: Session, _id,recommended_product_id):\n        return db.query( SegmentInformation) \\\n            .filter( SegmentInformation.dag_run_id == _id) \\\n            .filter( SegmentInformation.recommended_product_id == recommended_product_id).all()\n\n    def deleteById(db: Session, _ids):\n        for id in _ids:\n            db.query( SegmentInformation).filter( SegmentInformation.id == id).delete()\n            db.commit()\n\n    def update(db: Session, item_data):\n        updated_item = db.merge(item_data)\n        db.commit()\n        return updated_item\n        \n        \ndef getpackprice(product_id, packinfo_df):\n    price = 0\n    try:\n        if (packinfo_df is not None):\n            if (product_id in packinfo_df['product_id'].values):\n\n                price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\n                print(\"the price  of \", product_id, \" is \", price)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackprice \", e)\n\n    return price\n    \n    \n    \ndef GetCurrentPackprice(product_id, packinfo_df):\n    price = 0\n    \n    try:\n        print('product_id in GetCurrentPackprice is ',product_id)\n        print('type of product_id in GetCurrentPackprice is ',type(product_id))\n        if (packinfo_df is not None):\n            if isinstance(product_id, str):\n                split = product_id.split(\"|\")\n                \n                print('split in GetCurrentPackprice is ', split)\n                split_length = len(split)\n                if split_length > 1:\n                    highest = 0\n                    for i in range(split_length):\n                        product = int(float(split[i]))\n                        print('product is ', product)\n                        price = packinfo_df[packinfo_df['product_id'] == product].iloc[0][\"price\"]\n                        print('price is', price)\n                        if price > highest:\n                            highest = price\n                \n                    print(\"the price  of \", product_id, \" is \", price)\n                \n                else:\n                    \n                    product_id = int(split[0])\n                    \n                    price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\n                    highest = price\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackprice \", e)\n\n    return highest\n    \n\n\ndef calculate_percentage_of_ones(lst):\n\n    print('type of  lst is', type(lst))\n    total_count = len(lst)\n    ones_count = lst.count(1)\n    percentage = (ones_count / total_count) * 100\n    return percentage\n    \n    \ndef model_predict(df_test,df_control,usage_jan, dag_run_id):\n    \n    print(\"model_predict...\")\n    \n    df_under = pd.concat([df_test, df_control], axis=0)\n\n    df_under.drop(['TG_Response','CG_Response'], inplace = True,axis =1)\n\n    df_under =df_under[df_under['msisdn'].isin(usage_jan['msisdn'])]\n    df_under1 = df_under.sample(n=100000)\n    final = pd.merge(usage_jan,df_under,how='inner',on='msisdn')\n\n\n\n    cat_col = final.select_dtypes(include =['object','category'] ).columns.to_list()\n    msisdn = final.pop(\"msisdn\")\n    groups_test = final['action_group']\n    final.drop(columns=cat_col, inplace = True)\n\n    # load model from file\n    path = os.path.join(ml_location,  \"campaign_xgmodel.pickle\")\n    xgb_model = pickle.load(open(path, \"rb\"))\n\n    X=final.drop(['response'],axis=1)\n    y=final['response']  \n    \n\n    y_pred_full = xgb_model.predict(X)\n    print(classification_report(y, y_pred_full))#without random classifier\n\n    X['y_pred_full']=list(y_pred_full)\n    X['msisdn']=list(msisdn)\n    X=X[['msisdn','y_pred_full']]\n\n    X.to_csv(os.path.join(ml_location,'campaign_response_model_predicted.csv'),header=True,index=False)\n    print(\"model_predict is completed...\") \n    #percentage_of_ones = calculate_percentage_of_ones(list(y_pred_full))\n\n    #return percentage_of_ones\n    \n    \ndef uplift_data(X_full):\n    # Initialize lists to store results\n    products = []\n    real_uplifts = []\n    predicted_uplifts = []\n\n    # Get unique products\n    unique_products = X_full['Product_Promoted'].unique()\n\n    # Calculate real and predicted uplift for each product\n    for product in unique_products:\n        # Get data for this product\n        df_product = X_full[X_full['Product_Promoted'] == product]\n\n        # Calculate real uplift\n        real_uplift = df_product[df_product['action_group'] == 'TEST_GROUP']['response'].mean() - df_product[df_product['action_group'] == 'CONTROL_GROUP']['response'].mean()\n\n        # Calculate predicted uplift\n        predicted_uplift = df_product[df_product['action_group'] == 'TEST_GROUP']['predicted_prob'].mean() - df_product[df_product['action_group'] == 'CONTROL_GROUP']['predicted_prob'].mean()\n\n        # Append results\n        products.append(product)\n        real_uplifts.append(real_uplift)\n        predicted_uplifts.append(predicted_uplift)\n\n    # Store results in a DataFrame\n    uplift_df = pd.DataFrame({\n        'Product_Promoted': products,\n        'Real_Uplift': real_uplifts,\n        'Predicted_Uplift': predicted_uplifts\n    })\n    \n    return uplift_df\n    \n    \ndef xgboost_model(final,dag_run_id):\n    \n    X=final.drop(['response'],axis=1)\n    y=final['response']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\n    \n    xgb_model = xgb.XGBClassifier().fit(X_train, y_train)\n    \n       \n    \n    path = os.path.join(ml_location, \"campaign_xgmodel.pickle\") \n\n    pickle.dump(xgb_model, open(path, 'wb'))\n    \n    \n    print('Accuracy of XGB classifier on training set: {:.2f}'\n           .format(xgb_model.score(X_train, y_train)))\n    print('Accuracy of XGB classifier on test set: {:.2f}'\n           .format(xgb_model.score(X_test[X_train.columns], y_test)))\n    \n    y_pred = xgb_model.predict(X_test)\n    \n    print(classification_report(y_test, y_pred))\n    \n    \ndef model_fit(df_test, df_control, usage_jan, dag_run_id): \n    \n    print(\"model_fit started\")\n    \n    print(type(df_test['TG_Response'].sum()))\n    \n    # Perform undersampling within 'TEST_GROUP' only\n#     df_test_under = df_test[df_test['TG_Response'] == 0].sample(df_test['TG_Response'].sum(), random_state=42)\n    df_test_under = df_test[df_test['TG_Response'] == 0].sample(int(df_test['TG_Response'].sum()), random_state=42)\n\n\n    # Concatenate undersampled class 0 DataFrame with original class 1 DataFrame within 'TEST_GROUP'\n    df_test_under = pd.concat([df_test_under, df_test[df_test['TG_Response'] == 1]], axis=0)\n\n    # Combine the undersampled 'TEST_GROUP' DataFrame with the original 'CONTROL_GROUP' DataFrame\n    \n    df_under = pd.concat([df_test_under, df_control], axis=0)  \n   \n    \n    df_under.drop(['TG_Response','CG_Response'], inplace = True,axis =1)\n    \n    df_under_test_group = df_under[df_under[\"action_group\"] == \"TEST_GROUP\"]\n    \n    final_all_groups = pd.merge(usage_jan,df_under,how='inner',on='msisdn')\n    final = pd.merge(usage_jan,df_under_test_group,how='inner',on='msisdn')\n    \n    final.fillna(0,inplace=True)\n    final_all_groups.fillna(0,inplace=True)   \n    \n    cat_col = final.select_dtypes(include =['object','category'] ).columns.to_list()\n    msisdn = final.pop(\"msisdn\")\n    groups_test = final['action_group']\n    final.drop(columns=cat_col, inplace = True)\n    \n    msisdn_all = final_all_groups.pop(\"msisdn\")\n    groups = final_all_groups['action_group']\n    final_all_groups.drop(columns=cat_col, inplace = True)\n    \n    \n    xgboost_model(final,dag_run_id)   \n    \n    #full data prediction \n    X_full=final_all_groups.drop(['response'],axis=1)\n    y_full=final_all_groups['response']    \n    \n    \n    path = os.path.join(ml_location,  \"campaign_xgmodel.pickle\")\n    xgb_model = pickle.load(open(path, \"rb\"))\n    \n    \n    y_pred_prob_full = xgb_model.predict_proba(X_full)\n    X_full['predicted_prob'] =  y_pred_prob_full[:,1]\n    \n    \n    X_full['action_group']  =  groups\n    X_full['msisdn']  =  msisdn_all\n    X_full['response'] = y_full\n    \n    uplift_df = uplift_data(X_full)    \n    \n    df_product = X_full[X_full['Product_Promoted'] == 5.0]    \n    uplift_df['Difference_Percentage'] = ((uplift_df['Predicted_Uplift'] - uplift_df['Real_Uplift']) / uplift_df['Real_Uplift']) * 100\n    \n    path = os.path.join(ml_location,  \"campaign_uplift_model.csv\")\n    uplift_df.to_csv(path, index=False)\n    \n    print(\"model_fit completed\")\n    \n\ndef campaign_response_model(dag_run_id, db):\n    try:\n        c=0\n        \n        file_name_dict = get_file_names()\n\n        print('inside campaign_response_model started')\n        pack_info = pd.read_csv(os.path.join(pack_info_location, \"pack_info.csv\"))\n        for i, j in zip(campaign_df, campaign_usage):\n        \n            jan = pd.read_csv(os.path.join(etl_location, file_name_dict.get(\"campaign_data\").get(i)))    \n            usage_jan= pd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(j)))\n            jan_filter = jan[['msisdn','action_group','TG_Response','CG_Response','Product_Promoted']]\n            jan_filter.dropna(inplace = True)\n\n\n\n            jan_filter.loc[:, 'TG_Response'] = np.where(jan_filter['TG_Response']>0 , 1, jan_filter['TG_Response'])\n            jan_filter.loc[:, 'CG_Response'] = np.where(jan_filter['CG_Response']>0 , 1, jan_filter['CG_Response'])\n\n\n            # Separate DataFrame based on 'action_group'\n            df_test = jan_filter[jan_filter['action_group'] == 'TEST_GROUP']\n            df_control = jan_filter[jan_filter['action_group'] == 'CONTROL_GROUP']\n\n\n            df_test['response'] = df_test['TG_Response'] \n            df_control['response'] = df_control['CG_Response'] \n            print('adding to  table')\n            if c==0:        \n                model_fit(df_test,df_control, usage_jan, dag_run_id)        \n                c=1\n\n            else:\n                model_predict(df_test,df_control,usage_jan, dag_run_id)\n        campaign_response_model_predicted=pd.read_csv(os.path.join(ml_location,'campaign_response_model_predicted.csv'))\n        segements_two = SegementRepo.findByAutoPilotId(db=db, _id=dag_run_id)\n        for seg in segements_two:\n            item = seg.segment_name\n            segements_data = item.split(\"-\")\n            trent = segements_data[0]\n            rfm = segements_data[1]\n            service = segements_data[2]\n            filename = trent+\"-\"+rfm+\"-\"+service+\".csv\"\n            print('filename is ',filename)\n            segment_df_path = os.path.join(ml_location,  filename)\n            segment_df = pd.read_csv(segment_df_path)\n            campaign_response_model_predicted1=campaign_response_model_predicted[campaign_response_model_predicted['msisdn'].isin(segment_df['msisdn'])]\n            print('len(campaign_response_model_predicted1)',len(campaign_response_model_predicted1))\n            print(\"len(usage_jan)\",len(usage_jan))\n            if(len(campaign_response_model_predicted1) == 0 or campaign_response_model_predicted1 is None ):\n                perc=0\n                print(f\"for item{item} the campaign_response_percentage is {perc}\")\n            else:\n                perc = calculate_percentage_of_ones(list(campaign_response_model_predicted1['y_pred_full']))\n                print(f\"for item{item} the campaign_response_percentage is {perc}\")\n            seg.campaign_response_percentage=perc\n            campaign_type = seg.campaign_type\n            if campaign_type == \"upsell\" or campaign_type == \"crossell\":\n                current_product_id = seg.current_product\n                rec_product_id = int(seg.recommended_product_id)\n                current_product_price = GetCurrentPackprice(current_product_id, pack_info)\n                reco_product_price = getpackprice(rec_product_id,pack_info)\n                prodd_diff= reco_product_price-current_product_price\n\n                actual_target_count = seg.actual_target_count\n                total_revenue = seg.total_revenue\n                uplift_revenue= seg.uplift_revenue\n                current_arpu=  total_revenue/actual_target_count\n                seg.current_arpu =current_arpu\n                predicted_arpu = current_arpu+uplift_revenue \n                seg.predicted_arpu = predicted_arpu\n                seg.incremental_revenue =prodd_diff *(actual_target_count*perc/100)\n                seg.uplift_percent = ((predicted_arpu-current_arpu)/current_arpu)*100\n            \n            SegementRepo.update(db=db, item_data=seg)\n\n                    \n        print(\"campaign response compleated\")\n           \n    except Exception as e:\n        print(e)\n        \n        \n\n\ndef transform(dataframe):\n    db = get_db()\n    campaign_response_model(\"manual__2023-07-10T11:06:51\",db)\n    return dataframe"
      }
    }, {
      "id": "03b7c3a8-5da5-b3dd-8ef8-9acc3ee69547",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nimport sklearn\nimport json\nimport dask.dataframe as dd\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nfrom kmodes.kmodes import KModes\n# import configuration.config as cfg\n# import configuration.features as f\nimport traceback\nimport numpy as np\n# from fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom pathlib import Path\nimport requests\n# from sql_app.repositories import AssociationRepo\nimport pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\nfrom urllib.parse import quote  \n\n\n\n\n\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\n\n\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\npurchase_location = '/home/tnmops/seahorse3_bkp/'\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\netl_location = '/home/tnmops/seahorse3_bkp/'\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nlog_file = '/home/tnmops/seahorse3_bkp/'\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\npack_name = \"product_name\"\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCUSTOMER_NEEDED_COLUMN = [\n'onnet_revenue',\n'onnet_usage_da',\n'onnet_usage',\n'onnet_voice_count',\n'onnet_da_revenue',\n'offnet_revenue',\n'offnet_usage_da',\n'offnet_usage',\n'offnet_voice_count',\n'offnet_da_revenue',\n'idd_revenue',\n'idd_usage_da',\n'idd_usage',\n'idd_voice_count',\n'idd_da_revenue',\n'voice_rmg_revenue',\n'voice_rmg_usage_da',\n'voice_rmg_usage',\n'voice_rmg_count',\n'voice_rmg_da_revenue',\n'data_rmg_revenue',\n'data_rmg_usage_da',\n'data_rmg_usage',\n'data_rmg_da_revenue',\n'data_revenue',\n'data_usage_da',\n'data_usage',\n'da_data_rev',\n'sms_revenue',\n'sms_usage_da',\n'sms_usage',\n'sms_da_revenue',\n'sms_idd_revenue',\n'sms_idd_usage_da',\n'sms_idd_usage',\n'sms_idd_da_revenue',\n'magik_voice_amount',\n'rbt_subscription_rev',\n'emergency_credit_rev',\n'package_revenue',\n'voice_rev',\n'sms_rev',\n'onn_rev',\n'off_rev',\n'total_data_rev',\n'vas_rev',\n'vas_rev_others',\n'total_revenue',\n'total_voice_count',\n'total_voice_duration',\n'total_mainaccount_data_usage',\n'total_sms_count',\n'total_package_count',\n'total_other_vas_count',\n'total_voice_usage',\n'total_data_usage',\n'total_sms_usage']\n\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n\n\ndef perform_k_modes(df, path, dag_run_id):\n    try:\n        # K_modes_location = os.path.join(cfg.Config.ml_location, dag_run_id, \"kmodes\")\n        # Path(K_modes_location).mkdir(parents=True, exist_ok=True)\n        objList = df.select_dtypes(include=\"object\").columns\n        X = df[objList]\n        X = pd.get_dummies(X)\n\n        # elbow = find_k_kmodes(X=X)\n\n        km = KModes(n_clusters=2)\n        km.fit(X)\n        X['label'] = km.labels_\n        df['label'] = km.labels_\n        if df['label'].unique() is None or df['label'][0] is None :\n             df['label'] =0\n\n        print(df['label'].value_counts())\n        df.to_csv(path, header=True, index=False)\n        print(f\"outputed to path {path}\")\n        # for label in df['label'].unique():\n        #     df_temp = df[df['label'] == label]\n        #     name_path = f\"{df_name}_cluster_{label}.csv\"\n        #     output_path = os.path.join(K_modes_location, dag_run_id, name_path)\n        #     name_list[name_path] = output_path\n        #     print(f\"the output path is \", output_path)\n        #     df_temp.to_csv(output_path, header=True, index=False)\n        #     print(f\"done exporting \")\n\n        return df\n    except Exception as e:\n        print(e)\n        \n\n\n\n\ndef k_modes(dag_run_id):\n    try:\n        path_d = os.path.join(ml_location, \"dict.pickle\")\n        path_dd = os.path.join(ml_location, \"cluster_analysis.csv\")\n        with open(path_d, 'rb') as handle:\n            data = pickle.load(handle)\n        \n        \n#         for key, value in data_dict.items():\n#             data_dict[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n\n        # for key, value in data.items():\n        #     if value is None:\n        #         continue\n        #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\n        # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\n        filtered_dict = {k: v for k, v in data.items()}\n        data_list = []\n\n        for item, val in filtered_dict.items():\n            # val is the path item is the segment name\n            df = pd.read_csv(val)\n            print(f\"the cluster is {item}\")\n            val = perform_k_modes(df, val, dag_run_id)\n            val['segement_name'] = item\n            data_list.append(val)\n\n        df_final = pd.concat(data_list)\n        rfm_path = os.path.join(ml_location, \"rfm\")\n        rfm_dd = dd.read_parquet(rfm_path)\n        rfm_df=rfm_dd.compute()\n\n        df_final1=df_final[['msisdn','trend','Segment','label']]\n        df_final1=pd.merge(df_final1,rfm_df[['msisdn','Recency'\t,'Revenue'\t,'Frequency',\t'R_Score',\t'F_Score',\t'M_Score',\t'RFM_Segment']],\n                           on = 'msisdn',how = 'left')\n                           \n\n        df_final1['dag_run_id']=str(dag_run_id)\n        df_final1.to_csv(os.path.join(ml_location, \"rfm_segement_mode.csv\"), index=False, header=True)\n#         for df_chunk in pd.read_csv(os.path.join(ml_location, \"rfm_segement_mode.csv\"),\n#                                     chunksize=10000):\n#             # insert each chunk into database table\n#             df_chunk.to_sql('APA_rfm_segement_mode', engine, if_exists='append', index=False)\n\n        df_final = df_final.groupby(['segement_name', 'label']).agg({\"msisdn\": \"count\"}).reset_index()\n        df_final['dag_run_id']=str(dag_run_id)\n        \n        df_final.to_csv(path_dd,index=False,header = True)\n#         for df_chunk in pd.read_csv(os.path.join(ml_location, \"cluster_analysis.csv\"),chunksize=10000):\n#             # insert each chunk into database table\n#             df_chunk.to_sql('APA_rfm_segement_mode_groupby', engine, if_exists='append', index=False)\n        \n        return df_final\n\n\n    except Exception as e:\n        print(e)\n        \n        \n\n\n\n\ndef transform(dataframe):\n    df = k_modes(\"manual__2023-07-10T11:06:51\")\n    return df"
      }
    }, {
      "id": "b9ff44b6-80a4-5885-1234-fa06a9ed8928",
      "operation": {
        "id": "65240399-2987-41bd-ba7e-2944d60a3404",
        "name": "Create Custom Transformer"
      },
      "parameters": {
        "inner workflow": {
          "workflow": {
            "nodes": [{
              "id": "739e85cd-5157-7abe-9a3c-82661e817fd3",
              "operation": {
                "id": "f94b04d7-ec34-42f7-8100-93fe235c89f8",
                "name": "Source"
              },
              "parameters": {

              }
            }, {
              "id": "6f19e088-1210-38ae-979c-a6615eca3cab",
              "operation": {
                "id": "e652238f-7415-4da6-95c6-ee33808561b2",
                "name": "Sink"
              },
              "parameters": {

              }
            }, {
              "id": "51869fce-808d-c9ce-f078-348e78b4bf92",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "def transform(dataframe):\n    return dataframe"
              }
            }, {
              "id": "31f29c3f-17bf-dd19-93e6-f0b8c2c6c6c3",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "def transform(dataframe):\n    return dataframe"
              }
            }],
            "connections": [{
              "from": {
                "nodeId": "739e85cd-5157-7abe-9a3c-82661e817fd3",
                "portIndex": 0
              },
              "to": {
                "nodeId": "51869fce-808d-c9ce-f078-348e78b4bf92",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "51869fce-808d-c9ce-f078-348e78b4bf92",
                "portIndex": 0
              },
              "to": {
                "nodeId": "31f29c3f-17bf-dd19-93e6-f0b8c2c6c6c3",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "31f29c3f-17bf-dd19-93e6-f0b8c2c6c6c3",
                "portIndex": 0
              },
              "to": {
                "nodeId": "6f19e088-1210-38ae-979c-a6615eca3cab",
                "portIndex": 0
              }
            }]
          },
          "thirdPartyData": {
            "gui": {
              "name": "Inner workflow of custom transformer",
              "nodes": {
                "739e85cd-5157-7abe-9a3c-82661e817fd3": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5233,
                    "y": 4951
                  }
                },
                "6f19e088-1210-38ae-979c-a6615eca3cab": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5235,
                    "y": 5297
                  }
                },
                "51869fce-808d-c9ce-f078-348e78b4bf92": {
                  "uiName": "pre_process",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5198,
                    "y": 5068
                  }
                },
                "31f29c3f-17bf-dd19-93e6-f0b8c2c6c6c3": {
                  "uiName": "matrix_operation",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5158,
                    "y": 5176
                  }
                }
              }
            }
          },
          "publicParams": []
        }
      }
    }, {
      "id": "c894bdc6-be02-71a9-46ea-2d76497b7381",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_march_20230809132937.csv\",\n            \"m2\": \"recharge_feb_20230809132937.csv\",\n            \"m3\": \"recharge_jan_20230809132937.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n\n\nclass purchaseBanding(object):\n    def __init__(self, data):\n        self.data = data\n        # self.categorize()\n\n    def count_category(self, x, m):\n\n        if x == 1:\n            resp = 'b)1'\n        elif 1 < x <= 4:\n            resp = 'c)1-4'\n        elif 4 < x <= 10:\n            resp = 'd)4-10'\n        elif 10 < x <= 30:\n            resp = 'e)10-30'\n        elif x > 30:\n            resp = 'f)30 above'\n        else:\n            resp = 'a)zero'\n        return m + \"_\" + resp\n\n    def categorize(self):\n\n        months = purchase_no_months_seg\n        purchase_count_col_name = TRANSACTION_COUNT_COL_NAME\n        msisdn_name = MSISDN_COL_NAME\n        temp_df = None\n        for month in months:\n            needed_col = TRANSACTION_NEEDED_COL\n            dataset = self.data.get(month)[needed_col]\n            dataset = dataset.groupby([msisdn_name]).agg({purchase_count_col_name: 'sum'})\n            dataset['count_pattern'] = dataset[purchase_count_col_name].apply(self.count_category, args=(month,))\n            dataset['purchase_count_pattern_' + month] = dataset['count_pattern']\n            dataset['purchase_'+purchase_count_col_name + month] = dataset[purchase_count_col_name]\n            dataset = dataset.drop(purchase_count_col_name, axis=1)\n            dataset = dataset.drop('count_pattern', axis=1)\n            if temp_df is None:\n                temp_df = dataset\n            else:\n                temp_df = temp_df.merge(dataset, on=msisdn_name, how=\"left\")\n                temp_df[\"purchase_count_pattern_\" + month] = temp_df[\"purchase_count_pattern_\" + month].fillna(month+\"_a)zero\")\n                temp_df['purchase_'+purchase_count_col_name + month]=temp_df['purchase_'+purchase_count_col_name + month].fillna(0)\n        return temp_df.reset_index()\n        \n\n\n\ndef purchase_process(dag_run_id):\n    try:\n        file_name_dict = get_file_names()\n        print(\"purchase preprocess  ongoing \")\n        data = {}\n        dtype_purchase = RECHARGE_TRANSACTION_DTYPES\n        for month in purchase_no_months_seg:\n            data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"purchase\").get(month)),\n                                      dtype= RECHARGE_TRANSACTION_DTYPES)\n\n        df_data = dd.concat(list(data.values()))\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        path_pur = os.path.join(ml_location, \"purchase_all_months\")\n        Path(path_pur).mkdir(parents=True, exist_ok=True)\n        df_data.to_parquet(path_pur)\n        # ic(\"the length of m1 in purchase \", len(data['m1']))\n        # ic(\"the length of m2 in purchase \", len(data['m2']))\n        # ic(\"the length of m3 in purchase \", len(data['m3']))\n        #\n        # ic(\"the length of unique msisdn m3 in purchase \", data['m1']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in purchase \", data['m2']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in purchase \", data['m3']['msisdn'].nunique().compute())\n        rb = purchaseBanding(data)\n        print(\"purchase categorizing ongoing \")\n        df = rb.categorize()\n        print(\"purchase categorizing done \")\n        path = os.path.join(ml_location, \"purchase_band\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(\"purchase categorizing  to file op ongoing \")\n        df.to_parquet(path)\n        print(\"purchase categorizing  to file op done  \")\n        return df.compute()\n    except Exception as e:\n        print(e)\n        \n\n\n\n\n\ndef transform(dataframe):\n    df  = purchase_process(\"manual__2023-07-10T11:06:51\")\n    return df"
      }
    }, {
      "id": "e776af2a-0657-f30b-22fb-dc047d638e11",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom urllib.parse import quote  \n\n\n\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\nfrom sqlalchemy.orm import relationship\nimport datetime\nfrom sqlalchemy.dialects.mysql import LONGTEXT\n\n\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nimport sklearn\nimport json\nimport dask.dataframe as dd\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\n# import configuration.config as cfg\n# import configuration.features as f\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom pathlib import Path\nimport requests\n# from sql_app.repositories import AssociationRepo\n\n# config = cfg.Config().to_json()\n# features = f.Features().to_json()\n\n\n\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\n\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URL,\n)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        return db\n    finally:\n        db.close()\n        \n        \n        \nclass SegmentInformation(Base):\n    __tablename__ = \"APA_segment_information_new\"\n    id = Column(Integer, primary_key=True, index=True)\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\n    end_date = Column(DateTime)\n    current_product = Column(String(80), nullable=True, unique=False)\n    current_products_names = Column(String(200), nullable=True, unique=False)\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\n    predicted_arpu = Column(Integer, nullable=True)\n    current_arpu = Column(Integer, nullable=True)\n    segment_length = Column(String(80), nullable=True, unique=False)\n    rule = Column(LONGTEXT, nullable=True)\n    actual_rule = Column(LONGTEXT, nullable=True)\n    uplift_percent = Column(Float(precision=2), nullable=True)\n    incremental_revenue = Column(Float(precision=2), nullable=True)\n    campaign_type = Column(String(80), nullable=True, unique=False)\n    campaign_name = Column(String(80), nullable=True, unique=False)\n    action_key = Column(String(80), nullable=True, unique=False)\n    robox_id = Column(String(80), nullable=True, unique=False)\n    dag_run_id = Column(String(80), nullable=True, unique=False)\n    samples = Column(Integer, nullable=False)\n    segment_name = Column(String(80), nullable=True, unique=False)\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\n    customer_status = Column(String(80), nullable=True, unique=False)\n    query = Column(LONGTEXT, nullable=True, unique=False)\n    cluster_no = Column(Integer, nullable=True)\n    confidence = Column(Float(precision=2), nullable=True)\n    recommendation_type = Column(String(80), nullable=True, unique=False)\n    cluster_description = Column(LONGTEXT, nullable=True)\n    actual_target_count = Column(Integer, nullable=True)\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\n    total_revenue= Column(Integer, nullable=True)\n    uplift_revenue=Column(Integer, nullable=True)\n\n    def __repr__(self):\n        return 'SegmentInformation(name=%s)' % self.name\n\n\n\nclass SegementInfo(BaseModel):\n    end_date: Optional[str] = None\n    dag_run_id: Optional[str] = None\n    current_product: Optional[str] = None\n    current_products_names: Optional[str] = None\n    recommended_product_id: Optional[str] = None\n    recommended_product_name: Optional[str] = None\n    predicted_arpu: Optional[int] = None\n    current_arpu: Optional[int] = None\n    segment_length: Optional[str] = None\n    rule: Optional[str] = None\n    actual_rule: Optional[str] = None\n    uplift_percent: Optional[float] = None\n    incremental_revenue: Optional[float] = None\n    campaign_type: Optional[str] = None\n    campaign_name: Optional[str] = None\n    action_key: Optional[str] = None\n    robox_id: Optional[str] = None\n    samples: Optional[int] = None\n    segment_name: Optional[str] = None\n    current_ARPU_band: Optional[str] = None\n    current_revenue_impact: Optional[str] = None\n    customer_status: Optional[str] = None\n    query: Optional[str] = None\n    cluster_no: Optional[int] = None\n    confidence: Optional[float] = None\n    recommendation_type: Optional[str] = None\n    cluster_description: Optional[str] = None\n    actual_target_count: Optional[str] = None\n    top_purchased_day_1: Optional[str] = None\n    top_purchased_day_2: Optional[str] = None\n    top_purchased_day_3: Optional[str] = None\n    next_purchase_date_range: Optional[str] = None\n    campaign_response_percentage: Optional[str] = None\n    total_revenue: Optional[int] = None\n    uplift_revenue: Optional[int] = None\n\n\n\nclass SegementRepo:\n    def create(db: Session, segement: SegementInfo):\n        db_item = SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\n                                            campaign_type=segement.campaign_type,\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\n                                            rule=segement.rule, samples=segement.samples,\n                                            campaign_name=segement.campaign_name,\n                                            recommended_product_id=segement.recommended_product_id,\n                                            recommended_product_name=segement.recommended_product_name,\n                                            current_product=segement.current_product,\n                                            current_products_names=segement.current_products_names,\n                                            segment_length=segement.segment_length,\n                                            current_ARPU_band=segement.current_ARPU_band,\n                                            current_revenue_impact=segement.current_revenue_impact,\n                                            customer_status=segement.customer_status,\n                                            segment_name=segement.segment_name, query=segement.query,\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\n                                            recommendation_type=segement.recommendation_type,\n                                            cluster_description=segement.cluster_description,\n                                            actual_target_count=segement.actual_target_count,\n                                            top_purchased_day_1=segement.top_purchased_day_1,\n                                            top_purchased_day_2=segement.top_purchased_day_2,\n                                            top_purchased_day_3=segement.top_purchased_day_3,\n                                            next_purchase_date_range=segement.next_purchase_date_range,\n                                            campaign_response_percentage=segement.campaign_response_percentage,\n                                            total_revenue=segement.total_revenue,\n                                            uplift_revenue=segement.uplift_revenue,\n                                            )\n        db.add(db_item)\n        db.commit()\n        db.refresh(db_item)\n        return db_item\n\n    def findByAutoPilotIdAndClusterNo(db: Session, _id, cluster_no):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.cluster_no == cluster_no) \\\n            .all()\n\n    def findByAutoPilotIdAndSegementName(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).first()\n\n    def findByAutoPilotIdAndSegementNameAll(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).all()\n    \n    def findByAutoPilotIdAndSegementNamewithoutcluster(db: Session, _id, segement_name):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name).all()\n            \n\n\n    def findByAutoPilotId(db: Session, _id):\n        return db.query(SegmentInformation).filter(SegmentInformation.dag_run_id == _id).all()\n\n    def findByAutoPilotIdRecommendedId(db: Session, _id,recommended_product_id):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.recommended_product_id == recommended_product_id).all()\n\n    def deleteById(db: Session, _ids):\n        for id in _ids:\n            db.query(SegmentInformation).filter(SegmentInformation.id == id).delete()\n            db.commit()\n\n    def update(db: Session, item_data):\n        updated_item = db.merge(item_data)\n        db.commit()\n        return updated_item\n\n\n\ndef load_picke_file(filename):\n    with open(filename, 'rb') as handle:\n        data = pickle.load(handle)\n    return data\n    \n    \n    \nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\npurchase_location = '/home/tnmops/seahorse3_bkp/'\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\netl_location = '/home/tnmops/seahorse3_bkp/'\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nlog_file = '/home/tnmops/seahorse3_bkp/'\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n    \nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\npack_name = \"product_name\"\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCUSTOMER_NEEDED_COLUMN = [\n'onnet_revenue',\n'onnet_usage_da',\n'onnet_usage',\n'onnet_voice_count',\n'onnet_da_revenue',\n'offnet_revenue',\n'offnet_usage_da',\n'offnet_usage',\n'offnet_voice_count',\n'offnet_da_revenue',\n'idd_revenue',\n'idd_usage_da',\n'idd_usage',\n'idd_voice_count',\n'idd_da_revenue',\n'voice_rmg_revenue',\n'voice_rmg_usage_da',\n'voice_rmg_usage',\n'voice_rmg_count',\n'voice_rmg_da_revenue',\n'data_rmg_revenue',\n'data_rmg_usage_da',\n'data_rmg_usage',\n'data_rmg_da_revenue',\n'data_revenue',\n'data_usage_da',\n'data_usage',\n'da_data_rev',\n'sms_revenue',\n'sms_usage_da',\n'sms_usage',\n'sms_da_revenue',\n'sms_idd_revenue',\n'sms_idd_usage_da',\n'sms_idd_usage',\n'sms_idd_da_revenue',\n'magik_voice_amount',\n'rbt_subscription_rev',\n'emergency_credit_rev',\n'package_revenue',\n'voice_rev',\n'sms_rev',\n'onn_rev',\n'off_rev',\n'total_data_rev',\n'vas_rev',\n'vas_rev_others',\n'total_revenue',\n'total_voice_count',\n'total_voice_duration',\n'total_mainaccount_data_usage',\n'total_sms_count',\n'total_package_count',\n'total_other_vas_count',\n'total_voice_usage',\n'total_data_usage',\n'total_sms_usage']\n\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \nmsisdn_name = MSISDN_COL_NAME\nclass AssociationInfo(BaseModel):\n    dag_run_id: Optional[str] = None\n    current_pack: Optional[str] = None\n    current_pack_ids: Optional[str] = None\n    number_of_current_packs: Optional[int] = None\n    recommended_pack: Optional[str] = None\n    recommended_pack_id: Optional[str] = None\n    support: Optional[float] = None\n    lift: Optional[float] = None\n    confidence: Optional[float] = None\n    service_type: Optional[str] = None\n    type_info: Optional[str] = None\n    segement_name: Optional[str] = None\n    recommendation_type: Optional[str] = None\n    \nclass AssociationRepo:\n    def create(db: Session, info: AssociationInfo):\n        db_item = AssociationInfo(dag_run_id=info.dag_run_id, lift=info.lift,\n                                         service_type=info.service_type,\n                                         current_pack=info.current_pack, type_info=info.type_info,\n                                         confidence=info.confidence,\n                                         number_of_current_packs=info.number_of_current_packs,\n                                         support=info.support, recommended_pack=info.recommended_pack,\n                                         segement_name=info.segement_name,\n                                         recommended_pack_id=info.recommended_pack_id,\n                                         current_pack_ids=info.current_pack_ids,\n                                         recommendation_type=info.recommendation_type\n                                         )\n        db.add(db_item)\n        db.commit()\n        db.refresh(db_item)\n        return db_item\n        \nclass AssociationInfo(Base):\n    __tablename__ = \"APA_ASSOCIATION_INFO\"\n    id = Column(Integer, primary_key=True, index=True)\n    dag_run_id = Column(String(80), nullable=False)\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\n    current_pack_ids=Column(String(80), nullable=True, unique=False)\n    current_pack=Column(String(200), nullable=True, unique=False)\n    number_of_current_packs = Column(Integer, nullable=True)\n    recommended_pack = Column(String(80), nullable=True, unique=False)\n    recommended_pack_id=Column(String(80), nullable=True, unique=False)\n    support = Column(Float(precision=2), nullable=True)\n    lift = Column(Float(precision=2), nullable=True)\n    confidence = Column(Float(precision=2), nullable=True)\n    service_type = Column(String(80), nullable=True, unique=False)\n    type_info = Column(String(80), nullable=True, unique=False)\n    segement_name = Column(String(80), nullable=True, unique=False)\n    recommendation_type = Column(String(80), nullable=True, unique=False)\n    \nclass Fpgrowth(object):\n    def __init__(self, purchase, dag_run_id, item):\n        self.purchase = purchase\n        self.frequent_itemsets = None\n        self.results = None\n        self.results_processed = None\n        self.item_set_max_length = 4\n        self.item_set_min_length = 1\n        self.dag_run_id = dag_run_id\n        self.item = item\n\n        status, msg = self.form_associations()\n        print(msg)\n\n        if not status:\n            return None\n        self.process_association()\n\n        self.results_processed.to_csv(os.path.join(ml_location,  \"association.csv\"), header=True,\n                                      index=False)\n        self.results_processed.to_csv(os.path.join(ml_location,  str(item) + \"_association.csv\"),\n                                      header=True,\n                                      index=False)\n\n    def form_associations(self):\n        print('len of purchase before fropping na ', len(self.purchase))\n        self.purchase.dropna(inplace=True)\n        print('len of purchase ', len(self.purchase))\n\n        basket = (self.purchase.groupby([msisdn_name, TRANSACTION_PRODUCT_NAME])[\n                      TRANSACTION_PRODUCT_NAME]\n                  .count().unstack().reset_index().fillna(0)\n                  .set_index(msisdn_name))\n\n        print('basket created')\n        print('len of basket ', len(basket))\n\n        basket_sets = basket.applymap(encode_units)\n        basket_sets_filter = basket_sets[(basket_sets > 0).sum(axis=1) >= 2]\n        frequent_itemsets = fpgrowth(basket_sets_filter, min_support=0.03, use_colnames=True)\n        print('frequent_itemsets created')\n        print('len of frequent_itemsets ', len(frequent_itemsets))\n        if frequent_itemsets is None or len(frequent_itemsets) == 0:\n            # retun none so that the next cluster\n            return False, \"the result does not have any lenth the lenfth is \" + str(len(frequent_itemsets))\n        frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n\n        self.frequent_itemsets = frequent_itemsets[(frequent_itemsets['length'] >= self.item_set_min_length) & (\n                frequent_itemsets['length'] <= self.item_set_max_length)]\n        print('frequent_itemsets filtered')\n        self.results = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=2).sort_values('lift',\n                                                                                                        ascending=False).reset_index(\n            drop=True)\n        print('association_rules created')\n        print('self.results is ', self.results)\n        print(' len self.results is ', len(self.results))\n        print(' type self.results is ', type(self.results))\n        if len(self.results) > 2:\n            return True, \"got the asociaitons\"\n        return False, \"the result does not have any lenth the lenfth is \" + str(len(self.results))\n\n    def process_association(self):\n        print(' inside process_association')\n        results = self.results\n        results['antecedents_length'] = results['antecedents'].apply(lambda x: len(x))\n        results['consequents_length'] = results['consequents'].apply(lambda x: len(x))\n        # antecedents1 is a set converted from frozen set antecedents\n        results['antecedents1'] = results['antecedents'].apply(set)\n        results['consequents1'] = results['consequents'].apply(set)\n\n        results.drop(['antecedents', 'consequents', 'leverage', 'conviction'], inplace=True, axis=1, errors='ignore')\n\n        self.results_processed = results\n\n\n\n\ndef encode_units(x):\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\n        \ndef getpackname(product_id, packinfo_df):\n    product_name = \"No product name\"\n    try:\n        if (packinfo_df is not None):\n            if (product_id in packinfo_df['product_id'].values):\n\n                product_name = str(packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"product_name\"])\n                print(\"the product name of \", product_id, \" is \", product_name)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackname \", e)\n\n    return product_name\n    \ndef getpacktype(product_id, packinfo_df):\n    product_name = \"No product name\"\n    try:\n        if (packinfo_df is not None):\n            if (product_id in packinfo_df['product_id'].values):\n\n                product_name = str(packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"type\"])\n                print(\"the product name of \", product_id, \" is \", product_name)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackname \", e)\n\n    return product_name\n    \ndef get_rule_query(segements):\n\n    return None\n        \nclass UpsellCrossell(object):\n    def __init__(self, consequents_length=1, exclude_types=None,\n                 dag_run_id=None, db=None, pack_info=None,\n                 result=None, cluster_number=None, segement_name=None):\n        self.segement_name_list = None\n        if exclude_types is None:\n            exclude_types = ['SMS']\n        self.dag_run_id = dag_run_id\n        self.exclude_types = exclude_types\n        self.associations_df = result\n        self.db = db\n        self.pack_info = pack_info\n        self.df_cross_df = None\n        self.df_upsell_df = None\n        self.consequents_length = consequents_length\n\n        # self.read_fiels()\n        self.check_files()\n        # self.determine_service_type()\n        # print('df_cross_df is', self.df_cross_df)\n        # if self.df_cross_df is not None and len(self.df_cross_df) > 0:\n        #     self.find_crossell()\n        # print(\"outputed crosssell files \")\n        # for number in [1, 2, 3]:\n        #     print('going to find upsell for number', number)\n        #     self.find_upsell(type_service='upsell', anticendent_number=number,)\n        # print(\"done with upsell cross sell\")\n        #\n        # print(\"outputed crosssell files \")\n\n    def read_fiels(self):\n        try:\n            self.pack_info = pd.read_csv(os.path.join(ml_location, 'packinfo.csv'))\n            self.associations_df = pd.read_csv(os.path.join(ml_location, self.dag_run_id, 'association.csv'))\n        except Exception as e:\n            raise ValueError(e)\n\n    def check_files(self):\n        if self.pack_info is None or self.associations_df is None:\n            print(f\"the packinfo or association is null\")\n            raise ValueError(f\"the packinfo or association is null\")\n\n    def determine_service_type(self):\n        try:\n            print('self.associations_df is ', self.associations_df)\n            print('self.associations_df.columns is ', self.associations_df.columns)\n            if len(self.associations_df) <= 2:\n                return\n            self.associations_df = self.associations_df[self.associations_df['consequents_length'] == 1]\n            df = self.associations_df.apply(self.determine_service, axis=1)\n            if len(df) == 0:\n                return\n            for type_info in self.exclude_types:\n                df = df[~df['type_service'].str.contains(type_info)]\n\n            self.df_upsell_df = df[df['service'] == 'upsell']\n            self.df_cross_df = df[df['service'] != 'upsell']\n\n\n        except Exception as e:\n            print(\"error occoured in determine service\" + str(e))\n            raise ValueError(e)\n\n    def determine_service(self, x):\n        print('x is', x)\n\n        group_type = PACK_INFO_SUB_CATEGORY\n\n        def get_pack_type(pack_name):\n            type_pack = None\n            print(\"isnide get_pack_type\")\n            print(\"pack_name is \", pack_name)\n            print(\"group_type is \", group_type)\n            # print(\"f.Features.PACK_INFO_PACK_COLUMN_NAME is \", f.Features.PACK_INFO_PACK_COLUMN_NAME)\n            # print(\"self.pack_info is \", self.pack_info)\n            print(\"len self.pack_info is \", len(self.pack_info))\n\n            data = self.pack_info[self.pack_info[PACK_INFO_PACK_COLUMN_NAME] == pack_name][group_type]\n            print(\"data is \", data)\n            if len(data) > 0:\n                type_pack = data.values[0]\n                print(\"type_pack is \", type_pack)\n            return type_pack\n\n        x['antecedents1'] = str(x['antecedents1'])\n        x['consequents1'] = str(x['consequents1'])\n\n        antecedents_list = list(eval(x['antecedents1']))\n        print('antecedents_list is ', antecedents_list)\n        consequents_list = list(eval(x['consequents1']))\n        print('consequents_list is ', consequents_list)\n        antecedents_length = len(antecedents_list)\n\n        i = 0\n        service = \"upsell\"\n        type_service = None\n        temp_set = set()\n        try:\n            # since for each antecident we need to find eg: antecident length 1 ,2 ,3\n            while i < antecedents_length:\n                print('i is', i)\n                temp_set.add(get_pack_type(antecedents_list[i]))\n                print('temp_set is', temp_set)\n                i = i + 1\n                print('i is', i)\n            temp_set.add(get_pack_type(consequents_list[0]))\n            print('temp_set 1  is', temp_set)\n\n            if len(temp_set) > 1:\n                service = \"corsssell\"\n                type_service = ','.join(temp_set)\n            elif len(temp_set) == 1:\n                type_service = next(iter(temp_set))\n\n            x['service'] = service\n            x['type_service'] = type_service\n            return x\n        except Exception as e:\n            print(\"error occoured in determine_service\", e)\n            raise ValueError(e)\n\n    def find_crossell(self, segement_name, cluster_number):\n        try:\n            segement_name = f\"{segement_name}-{str(int(cluster_number))}\"\n            # self.segement_name_list = segement_name.split(\"|\")\n\n            print(\"self.df_cross_df is \", self.df_cross_df)\n            df = self.df_cross_df.apply(self.cross_sell_parser, axis=1)\n\n            if len(df) > 0:\n                df = df.sort_values(by=\"confidence\", ascending=False)\n                segements = SegementRepo.findByAutoPilotIdAndSegementName(self.db, _id=self.dag_run_id,\n                                                                          segement_name=segement_name,\n                                                                          cluster_number=int(cluster_number))\n                if segements is None:\n                    return\n                self.insert_segementinfo(segements, 1, df, \"crossell\")\n                SegementRepo.deleteById(self.db, [segements.id])\n                # confidence = int(df.head(1)['confidence'].values[0] * 100)\n                # current_pack = list(eval(df.head(1)['antecedents1'].values[0]))[0]\n                # recommended_pack = str(df.head(1)['conci'].values[0])\n                # number_of_current_packs = int(df.head(1)['antecedents_length'].values[0])\n                # if confidence > .50:\n                #     segements = SegementRepo.findByAutoPilotIdAndSegementName(self.db, _id=self.dag_run_id,\n                #                                                               segement_name=f\"{segement_name}_{str(int(cluster_number))}\",\n                #                                                               cluster_number=int(cluster_number))\n                #     segements.campaign_type = \"cross_sell\"\n                #     segements.confidence = confidence\n                #     segements.recommended_product_name = recommended_pack\n                #     segements.current_product = current_pack\n                #\n                #     SegementRepo.update(self.db, segements)\n\n                # self.update_info(\"antecedents1\", \"cross_sell\", segement_name, cluster_number)\n            # ------------------------adding to db -------------------------------#\n            for index, row in df.iterrows():\n                print(row['confidence'])\n                info = AssociationInfo()\n                info.service_type = str(row['type_service'])\n                info.type_info = str(row['val'])\n                info.dag_run_id = self.dag_run_id\n                info.support = round(float(row['support']), 2)\n                info.confidence = round(float(row['confidence']), 2)\n                info.lift = round(float(row['lift']), 2)\n                print(\"row['conci'] is \", row['conci'])\n\n                info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\n                info.recommended_pack_id = str(row['conci'])\n\n                info.number_of_current_packs = int(row['antecedents_length'])\n                info.current_pack = \"|\".join(\n                    [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n\n                info.segement_name = segements.segment_name\n                current_pack_types = \"|\".join(\n                    [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n                print(\"added crossel info to db\")\n                AssociationRepo.create(db=self.db, info=info)\n            # ------------------------adding to db -------------------------------#\n\n\n        # ----------------------------to br looked at ------------------------------------------------------#\n        # df_2 = df[df['val'] == 'crossell'].sort_values(by='confidence', ascending=False)\n        #\n        # df_2.drop_duplicates(subset=['consequents1'], inplace=True)\n        # df_cross_conci = df_2.drop_duplicates(subset=['conci'])\n        # df_cross_anti = df_2.drop_duplicates(subset=['anti'])\n        # df_2 = pd.concat([df_cross_conci, df_cross_anti])\n        # df_2.drop_duplicates(subset=['anti', 'conci'], keep='first')\n        # # df_2.drop(['antecedent support', 'consequent support', 'support', 'lift',\n        # #            'antecedents1', 'consequents1'], axis=1, inplace=True, errors='ignore')\n        #\n        # df_3 = df_2.merge(\n        #     self.pack_info[\n        #         f.Features.ALL_PACK_FEATURES].add_prefix(\n        #         \"ANTECEDENT_\"), left_on='anti', right_on='ANTECEDENT_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\n        #     how='left')\n        # df_3 = df_3.merge(\n        #     self.pack_info[\n        #         f.Features.ALL_PACK_FEATURES].add_prefix(\n        #         \"CONSEQUENT_\"), left_on='conci', right_on='CONSEQUENT_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\n        #     how='left')\n        # # print(\"length of df_3 before condition is  \", len(df_3))\n        # # df_3 = df_3[df_3['ANTECEDENT_price'] < df_3['CONSEQUENT_price']]\n        # # print(\"length of df_3 after condition  is  \", len(df_3))\n        #\n        # if os.path.exists(os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'crossell_one.csv')):\n        #\n        #     crossell_one_df = pd.read_csv(\n        #         os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'crossell_one.csv'))\n        #     df_3 = df_3.drop(['val', 'anti', 'conci'], axis=1)\n        #     final_crossell_one_df = pd.concat([crossell_one_df, df_3])\n        #     final_crossell_one_df.to_csv(\n        #         os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'crossell_one.csv'),\n        #         header=True,\n        #         index=False)\n        # else:\n        #\n        #     df_3.drop(['val', 'anti', 'conci'], axis=1, errors='ignore').to_csv(\n        #         os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'crossell_one.csv'),\n        #         header=True, index=False)\n\n        # ----------------------------to br looked at ------------------------------------------------------#\n        except Exception as e:\n            traceback.print_exc()\n            raise ValueError(e)\n\n    def cross_sell_parser(self, x):\n        cross_sell = False\n        try:\n            print('x is', x)\n            anti = list(eval(x['antecedents1']))\n            conci = list(eval(x['consequents1']))\n            print('here 2 ')\n            print(f\"anti {anti} conci {conci}\")\n            if len(anti) == 1:\n                anti_catagory = self.pack_info[self.pack_info[pack_name] == anti[0]]['bundle_type'].values\n                anti_class = self.pack_info[self.pack_info[pack_name] == anti[0]]['bundle_type'].values\n                if len(anti_catagory) > 0:\n                    anti_catagory = anti_catagory[0]\n                    anti_class = anti_class[0]\n                conci_catagory = self.pack_info[self.pack_info[pack_name] == conci[0]]['bundle_type'].values\n                conci_class = self.pack_info[self.pack_info[pack_name] == conci[0]]['bundle_type'].values\n                if len(conci_catagory) > 0:\n                    conci_catagory = conci_catagory[0]\n                    conci_class = conci_class[0]\n                print(f\"the anti c  is {anti_catagory}  and conci cata is  {conci_catagory}\")\n                if ((anti_catagory != conci_catagory) and (anti_class != conci_class)):\n                    cross_sell = True\n            anti_name = None\n            if len(anti) == 2:\n                cross_sell = True\n                for anti_item in anti:\n                    print(\"the anti item \", anti_item)\n                    anti_catagory = self.pack_info[self.pack_info[pack_name] == anti_item]['bundle_type'].values\n                    conci_catagory = self.pack_info[self.pack_info[pack_name] == conci[0]]['bundle_type'].values\n                    print(f\"the anticent name {anti_item} conci name {conci[0]}\")\n                    print(f\"the anticent catagory {anti_catagory} conci cata {conci_catagory}\")\n                    if (anti_catagory == conci_catagory):\n                        cross_sell = False\n                        anti_name = None\n                    else:\n                        anti_name = anti_item\n\n            if cross_sell:\n                x['val'] = \"crossell\"\n                if len(anti) == 2:\n                    x['anti'] = anti_name\n                else:\n                    x['anti'] = anti[0]\n\n                x['conci'] = conci[0]\n                return x\n            x['val'] = \"not_valid_crossell\"\n            x['anti'] = anti[0]\n            x['conci'] = conci[0]\n            return x\n        except Exception as e:\n            print('the error occoured in find crosssell', e)\n            raise ValueError(e)\n\n    def find_upsell(self, type_service, anticendent_number, segement_name, cluster_number):\n        try:\n\n            is_upsell = 1\n            if type_service != 'upsell':\n                is_upsell = 0\n\n            segements = SegementRepo.findByAutoPilotIdAndSegementName(self.db, _id=self.dag_run_id,\n                                                                      segement_name=f\"{segement_name}-{str(int(cluster_number))}\",\n                                                                      cluster_number=int(cluster_number))\n            if segements is None:\n                return\n            if anticendent_number == 1:\n                print(\"inside antecedent 1 \")\n                df = self.df_upsell_df[self.df_upsell_df['antecedents_length'] == 1]\n                data = df.sort_values(by='confidence', ascending=False)\n                print(len(data.drop_duplicates(subset=['consequents1'])))\n                print(\"data.columns inside antecedent 1 is \", data.columns)\n                data1 = data.apply(self.check_upsell, anticendent_number=1, axis=1)\n                print('check_upsell completed')\n                # ------------------- insert db ------------------------------\n\n                # ------------------------adding to db -------------------------------#\n                for index, row in data1.iterrows():\n                    print(row['confidence'])\n                    info = AssociationInfo()\n                    info.service_type = str(row['type_service'])\n                    info.type_info = str(row['upsell_case'])\n                    info.dag_run_id = self.dag_run_id\n                    info.support = round(float(row['support']), 2)\n                    info.confidence = round(float(row['confidence']), 2)\n                    info.lift = round(float(row['lift']), 2)\n                    print(\"row['conci'] is \", row['conci'])\n                    info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\n                    info.recommended_pack_id = str(row['conci'])\n\n                    info.number_of_current_packs = int(row['antecedents_length'])\n                    info.current_pack = \"|\".join(\n                        [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n                    info.segement_name = segements.segment_name\n                    print('info.number_of_current_packs is', info.number_of_current_packs)\n                    print('info.current_pack is', info.current_pack)\n                    print('info.recommended_pack is', info.recommended_pack)\n\n                    print('info is', info)\n                    current_pack_types = \"|\".join(\n                        [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n                    AssociationRepo.create(db=self.db, info=info)\n                    print('all  inserted')\n\n                self.insert_segementinfo(segements, anticendent_number, data1, \"upsell\")\n\n                # ------------------------adding to db -------------------------------#\n                # if data1 is None or len(data1) == 0:\n                #     ic(f\"the segement {segement_name} and cluser {cluster_number}  has no upsell 1 data1 \")\n                #     return\n                # ic(f\"the segement {segement_name} and cluser {cluster_number} \")\n                # data2 = data1[data1['is_upsell'] == is_upsell]\n                # if data2 is None or len(data2) == 0:\n                #     ic(f\"the segement {segement_name} and cluser {cluster_number}  has no upsell 1\")\n                #     return\n                # data2 = data2.sort_values(by='confidence', ascending=False)\n                # data2 = data2.head(5)\n                # segments_list = []\n                # for index, row in data2.iterrows():\n                #     info = schemas.SegementInfo()\n                #     info.segment_name = segements.segment_name\n                #     info.dag_run_id = self.dag_run_id\n                #     info.current_product = str(row['antecedents1'])\n                #     info.current_products_names = str(row['antecedents1'])\n                #     info.recommended_product_id = str(row['conci'])\n                #     info.recommended_product_name = str(row['conci'])\n                #     info.predicted_arpu = None\n                #     info.current_arpu = None\n                #     info.segment_length = segements.segment_length\n                #     info.rule = None\n                #     info.actual_rule = None\n                #     info.uplift = None\n                #     info.incremental_revenue = None\n                #     info.campaign_type = \"upsell\"\n                #     info.campaign_name = None\n                #     info.action_key = None\n                #     info.robox_id = None\n                #     info.samples = segements.samples\n                #     info.segment_name = segements.segment_name\n                #     info.current_ARPU_band = None\n                #     info.current_revenue_impact = None\n                #     info.customer_status = segements.customer_status\n                #     info.query = segements.query\n                #     info.cluster_no = segements.cluster_no\n                #     info.confidence = round(float(row['confidence']), 2)\n                #     segments_list.append(info)\n                #\n                # for segment in segments_list:\n                #     SegementRepo.create(self.db, segment)\n                # ------------------------------------------------------------\n\n            # ---------------------ipo venda -----------------------------------------#\n            # data2 = data1[data1['is_upsell'] == is_upsell]\n            # print('here1')\n            # data3 = data2.drop_duplicates(subset=['consequents1'])\n            # data3['antecedent_one'] = data3['antecedents1'].apply(lambda x: list(eval(x))[0])\n            # print('here2')\n            # data3['consequents1'] = data3['consequents1'].apply(lambda x: list(eval(x))[0])\n            # print('here3')\n            # data3 = data3.merge(\n            #     self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\n            #         \"ANTECEDENT_\"),\n            #     left_on='antecedent_one', right_on=\"ANTECEDENT_\" + f.Features.PACK_INFO_PACK_COLUMN_NAME,\n            #     how='left')\n            # data3 = data3.merge(\n            #     self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\n            #         \"CONSEQUENT_\"),\n            #     left_on='consequents1', right_on='CONSEQUENT_' + f.Features.PACK_INFO_PACK_COLUMN_NAME, how='left')\n            # # data3.drop(\n            # #     ['antecedent_one', 'consequents1', 'service', 'antecedent support', 'consequent support', 'lift'],\n            # #     axis=1, inplace=True)\n            # data4 = data3[data3['ANTECEDENT_' + f.Features.PACK_INFO_CATEGORY] == data3[\n            #     'CONSEQUENT_' + f.Features.PACK_INFO_CATEGORY]]\n            #\n            # file_name = os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'upsell_one.csv')\n            # if os.path.exists(file_name):\n            #     upsell_one_df = pd.read_csv(file_name)\n            #     final_upsell_one_df = pd.concat([upsell_one_df, data4])\n            #     final_upsell_one_df.to_csv(file_name, header=True,\n            #                                index=False)\n            #\n            # else:\n            #\n            #     data4.to_csv(os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'upsell_one.csv'),\n            #                  header=True,\n            #                  index=False)\n            # print(\"outputed 1 upsell anticident \")\n            # ---------------------ipo venda -----------------------------------------#\n\n            elif anticendent_number == 2:\n                print(\"inside antecedent 2 \")\n                df = self.df_upsell_df[self.df_upsell_df['antecedents_length'] == 2]\n                if len(df) == 0:\n                    ic(f\"the anticendent_number 2 is not present for upsell {segement_name}_{str(int(cluster_number))}\")\n                    return None\n                data = df.sort_values(by='confidence', ascending=False)\n                print(len(data.drop_duplicates(subset=['consequents1'])))\n                print(\"data.columns inside antecedent 2  is \", data.columns)\n                data1 = data.apply(self.check_upsell, anticendent_number=2, axis=1)\n                # ------------------------adding to db -------------------------------#\n                for index, row in data1.iterrows():\n                    print(row['confidence'])\n                    info = AssociationInfo()\n                    info.service_type = str(row['type_service'])\n                    info.type_info = str(row['upsell_case'])\n                    info.dag_run_id = self.dag_run_id\n                    info.support = round(float(row['support']), 2)\n                    info.confidence = round(float(row['confidence']), 2)\n                    info.lift = round(float(row['lift']), 2)\n                    print(\"row['conci'] is \", row['conci'])\n\n                    info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\n                    info.recommended_pack_id = str(row['conci'])\n\n                    info.segement_name = segements.segment_name\n                    info.number_of_current_packs = int(row['antecedents_length'])\n                    info.current_pack = \"|\".join(\n                        [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n                    print('info.number_of_current_packs is', info.number_of_current_packs)\n                    print('info.current_pack is', info.current_pack)\n                    print('info.recommended_pack is', info.recommended_pack)\n                    current_pack_types = \"|\".join(\n                        [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n                    AssociationRepo.create(db=self.db, info=info)\n                # ------------------------adding to db -------------------------------#\n                self.insert_segementinfo(segements, anticendent_number, data1, \"upsell\")\n                # ---------------------ipo venda -----------------------------------------#\n                # data2 = data1[data1['is_upsell'] == 1]\n                # data3 = data2.drop_duplicates(subset=['consequents1'])\n                # data3['antecedents_one'] = data3['antecedents1'].apply(lambda x: list(eval(x))[0])\n                # data3['antecedents_two'] = data3['antecedents1'].apply(lambda x: list(eval(x))[1])\n                # data3['consequent'] = data3['consequents1'].apply(lambda x: list(eval(x))[0])\n                # # data3.drop(['antecedents1', 'consequents1', 'service', 'antecedent support', 'consequent support'],\n                # #            axis=1,\n                # #            inplace=True)\n                # data3 = data3.merge(\n                #     self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\n                #         \"ANTECEDENT_1_\"),\n                #     left_on='antecedents_one', right_on='ANTECEDENT_1_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\n                #     how='left')\n                # data3 = data3.merge(\n                #     self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\n                #         \"ANTECEDENT_2_\"),\n                #     left_on='antecedents_two', right_on='ANTECEDENT_2_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\n                #     how='left')\n                # data3 = data3.merge(\n                #     self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\n                #         \"CONSEQUENT_\"),\n                #     left_on='consequent', right_on='CONSEQUENT_' + f.Features.PACK_INFO_PACK_COLUMN_NAME, how='left')\n                # file_name = os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'upsell_two.csv');\n                # if os.path.exists(file_name):\n                #     upsell_two_df = pd.read_csv(file_name)\n                #     final_upsell_two_df = pd.concat([upsell_two_df, data3])\n                #     final_upsell_two_df.to_csv(file_name, header=True,\n                #                                index=False)\n                #\n                # else:\n                #     data3.to_csv(file_name, header=True, index=False)\n                # print(\"outputed 2 upsell anticident \")\n\n                # ---------------------ipo venda -----------------------------------------#\n\n            elif anticendent_number == 3:\n                print(\"inside antecedent 3 \")\n                df = self.df_upsell_df[self.df_upsell_df['antecedents_length'] == 3]\n                if len(df) == 0:\n                    ic(f\"the anticendent_number 3 is not present for upsell {segement_name}_{str(int(cluster_number))}\")\n                    return None\n\n                data = df.sort_values(by='confidence', ascending=False)\n                print(len(data.drop_duplicates(subset=['consequents1'])))\n                print(\"data.columns inside antecedent 3 is \", data.columns)\n                data1 = data.apply(self.check_upsell, anticendent_number=3, axis=1)\n                # ------------------------adding to db -------------------------------#\n                for index, row in data1.iterrows():\n                    print(row['confidence'])\n                    info = AssociationInfo()\n                    info.service_type = str(row['type_service'])\n                    info.type_info = str(row['upsell_case'])\n                    info.dag_run_id = self.dag_run_id\n                    info.support = round(float(row['support']), 2)\n                    info.confidence = round(float(row['confidence']), 2)\n                    info.lift = round(float(row['lift']), 2)\n                    print(\"row['conci'] is \", row['conci'])\n\n                    info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\n                    info.recommended_pack_id = str(row['conci'])\n\n                    info.segement_name = segements.segment_name\n                    info.number_of_current_packs = int(row['antecedents_length'])\n                    info.current_pack = \"|\".join(\n                        [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n                    print('info.number_of_current_packs is', info.number_of_current_packs)\n                    print('info.current_pack is', info.current_pack)\n                    print('info.recommended_pack is', info.recommended_pack)\n                    current_pack_types = \"|\".join(\n                        [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n                    AssociationRepo.create(db=self.db, info=info)\n                # ------------------------adding to db -------------------------------#\n                self.insert_segementinfo(segements, anticendent_number, data1, \"upsell\")\n\n            SegementRepo.deleteById(self.db, [segements.id])\n            # ---------------------ipo venda -----------------------------------------#\n            #     data2 = data1[data1['is_upsell'] == 1]\n            #     data3 = data2.drop_duplicates(subset=['consequents1'])\n            #     data3['antecedents_one'] = data3['antecedents1'].apply(lambda x: list(eval(x))[0])\n            #     data3['antecedents_two'] = data3['antecedents1'].apply(lambda x: list(eval(x))[1])\n            #     data3['antecedents_three'] = data3['antecedents1'].apply(lambda x: list(eval(x))[2])\n            #     data3['consequent'] = data3['consequents1'].apply(lambda x: list(eval(x))[0])\n            #     # data3.drop(['antecedents1', 'consequents1', 'service', 'antecedent support', 'consequent support'],\n            #     #            axis=1,\n            #     #            inplace=True)\n            #     data3 = data3.merge(\n            #         self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\n            #             \"ANTECEDENT_1_\"),\n            #         left_on='antecedents_one', right_on='ANTECEDENT_1_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\n            #         how='left')\n            #     data3 = data3.merge(\n            #         self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\n            #             \"ANTECEDENT_2_\"),\n            #         left_on='antecedents_two', right_on='ANTECEDENT_2_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\n            #         how='left')\n            #     data3 = data3.merge(\n            #         self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\n            #             \"ANTECEDENT_3_\"),\n            #         left_on='antecedents_three', right_on='ANTECEDENT_3_' + f.Features.PACK_INFO_PACK_COLUMN_NAME,\n            #         how='left')\n            #     data3 = data3.merge(\n            #         self.pack_info[f.Features.ALL_PACK_FEATURES].add_prefix(\n            #             \"CONSEQUENT_\"),\n            #         left_on='consequent', right_on='CONSEQUENT_' + f.Features.PACK_INFO_PACK_COLUMN_NAME, how='left')\n            #     file_name = os.path.join(cfg.Config.pack_info_location, self.dag_run_id, 'upsell_three.csv');\n            #     if os.path.exists(file_name):\n            #         upsell_three_df = pd.read_csv(file_name)\n            #         final_upsell_three_df = pd.concat([upsell_three_df, data3])\n            #         final_upsell_three_df.to_csv(file_name, header=True,\n            #                                      index=False)\n            #     else:\n            #         data3.to_csv(file_name, header=True, index=False)\n            #     print(\"outputed 3 upsell anticident \")\n            #\n            # else:\n            #     raise ValueError(\"anticient number more that 3 \")\n\n            # ---------------------ipo venda -----------------------------------------#\n        except Exception as e:\n            print(\"error ocoured in output service\", e)\n            raise ValueError(e)\n\n    def check_upsell(self, x, anticendent_number):\n        print('Inside check_upsell')\n\n        col = PACK_INFO_PACK_PRICE_COLUMN_NAME\n\n        def get_price(pack_name):\n            price = None\n            data = self.pack_info[self.pack_info[PACK_INFO_PACK_COLUMN_NAME] == pack_name][col]\n            if (len(data) > 0):\n                price = data.values[0]\n            return price\n\n        anti = None\n        conci = None\n        print('x is', x)\n        print(\"x['consequents1'] is\", x['consequents1'])\n        print(\"anticendent_number is\", anticendent_number)\n        x['is_upsell'] = 1\n        x['upsell_case'] = \"upsell\"\n\n        try:\n            conci = list(eval(x['consequents1']))[0]\n            print(\"teh conci is \", conci)\n            conci_price = get_price(conci)\n            print(f\"the conci price is {conci_price}\")\n            x['conci'] = conci\n\n            if anticendent_number == 1:\n                anti = list(eval(x['antecedents1']))[0]\n                anti_price = get_price(anti)\n\n                if conci_price < anti_price:\n                    x['is_upsell'] = 0\n                    x['upsell_case'] = \"not_a_valid_upsell\"\n                    x['conci'] = conci\n            else:\n                amti_list = list(eval(x['antecedents1']))\n                for anti_obj in amti_list:\n\n                    anti_price = get_price(anti_obj)\n                    if conci_price <= anti_price:\n                        x['is_upsell'] = 0\n                        x['upsell_case'] = \"not_a_valid_upsell\"\n                        x['conci'] = conci\n\n            return x\n        except Exception as e:\n            print(\" the error occoured in check_upsell \", e)\n            raise ValueError(e)\n\n    def update_info(self, anticident_name, type_of_service, segement_name, cluster_number):\n        segements = SegementRepo.findByAutoPilotIdAndSegementName(self.db, segement_name=segement_name,\n                                                                  cluster_number=cluster_number)\n        pass\n\n    def insert_segementinfo(self, segements, anticendent_number, data1, type_of_service):\n        if data1 is None or len(data1) == 0:\n            ic(f\"the segement {segements.segment_name} has no upsell 1 data1 \")\n            return\n        if type_of_service == \"upsell\":\n            data1 = data1[data1['is_upsell'] == 1]\n            if data1 is None or len(data1) == 0:\n                ic(f\"the segement {segements.segment_name}  has no upsell 1\")\n                return\n        data2 = data1.sort_values(by='confidence', ascending=False)\n        data2 = data2.head(5)\n        segments_list = []\n        for index, row in data2.iterrows():\n            get_rule_query(segements)\n            info = SegementInfo()\n            info.segment_name = segements.segment_name\n            info.cluster_description=segements.cluster_description\n            info.dag_run_id = self.dag_run_id\n            current_product_id = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n            info.current_product = current_product_id\n            info.current_products_names = \"|\".join(\n                [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n            # if anticendent_number == 1:\n            #     info.current_products_names = str(row['antecedents1'])\n            # elif anticendent_number == 2:\n            #     info.current_products_names = str(row['antecedents1']) + str(row['antecedents2'])\n            # else:\n            #     info.current_products_names = str(row['antecedents1']) + str(row['antecedents2']) + str(\n            #         row['antecedents3'])\n\n            rec_product_id = str(row['conci'])\n            info.recommended_product_id = rec_product_id\n\n            info.recommended_product_name = str(getpackname(row['conci'], self.pack_info))\n\n            info.predicted_arpu = None\n            info.current_arpu = None\n            info.segment_length = segements.segment_length\n            info.rule = None\n            info.actual_rule = None\n            info.uplift_revenue = None\n            info.incremental_revenue = None\n            info.campaign_type = type_of_service\n            info.campaign_name = None\n            info.action_key = None\n            info.robox_id = None\n            info.samples = segements.samples\n            info.segment_name = segements.segment_name\n            info.current_ARPU_band = None\n            info.current_revenue_impact = None\n            info.customer_status = segements.customer_status\n            info.query = segements.query\n            info.cluster_no = segements.cluster_no\n            info.confidence = round(float(row['confidence']), 2)\n            current_pack_types = \"|\".join(\n                [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n            info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n\n            segments_list.append(info)\n\n        for segment in segments_list:\n            SegementRepo.create(self.db, segment)\n            \n            \ndef association_process(dag_run_id, db):\n    result_dict_path = os.path.join(ml_location, \"purchased_for_association\", 'dict.pickle')#data\n    data_path = os.path.join(ml_location,  \"dict.pickle\")#datadict\n    data_dict = load_picke_file(data_path)\n    pack_info = os.path.join(etl_location, 'pack_info.csv')\n    pack_info_df = pd.read_csv(pack_info)\n    data = load_picke_file(result_dict_path)\n    # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\n    # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\n    # filtered_data__dict = {k: v for k, v in data_dict.items() if k in needed_segements}\n    \n    \n    # for key, value in data_dict.items():\n    #     data_dict[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n        \n    # for key, value in data.items():\n    #     if value is None:\n    #         continue\n    #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n        \n    filtered_dict = {k: v for k, v in data.items()}\n    filtered_data__dict = {k: v for k, v in data_dict.items()}\n    \n    purchase_list=[]\n    for item, val in filtered_dict.items():\n        if val is None:\n            continue\n        \n        # if filtered_data__dict.get(item) is None:\n        #     continue\n        # else:\n        data = pd.read_csv(filtered_data__dict.get(item))\n        \n        purchase = pd.read_csv(val)\n        for cluster in data['label'].unique():\n            data_temp = data[data['label'] == cluster]\n            purchase_filtered = purchase[purchase[msisdn_name].isin(data_temp[msisdn_name])]\n            pruchase_grp_df=purchase_filtered.groupby([TRANSACTION_PRODUCT_NAME]).agg({msisdn_name:'nunique'}).reset_index()\n            pruchase_grp_df['segment_name'] = f\"{item}-{str(cluster)}\"\n            print( ' len of pruchase_grp_df if  ' ,len(pruchase_grp_df))\n            purchase_list.append(pruchase_grp_df)\n            purchase_filtered['product_id'] = purchase_filtered['product_id'].astype(int)\n            \n#             print(purchase_filtered['product_id'])\n            print('len of purchase in association_process ', len(purchase_filtered))\n            fp = Fpgrowth(purchase_filtered, dag_run_id, item=item)\n            print('fp.results is ', fp.results)\n            if fp.results is None or len(fp.results) == 0:\n                print('skipped this iteration')\n                continue\n            print('Fpgrowth completed')\n            uc = UpsellCrossell(dag_run_id=dag_run_id, pack_info=pack_info_df, result=fp.results, db=db)\n            uc.determine_service_type()\n            if len(uc.associations_df) <= 2:\n                continue\n            if uc.df_cross_df is not None and len(uc.df_cross_df) > 0:\n                uc.find_crossell(segement_name=item, cluster_number=cluster)\n            print(\"outputed crosssell files \")\n            if uc.df_upsell_df is None or len(uc.df_upsell_df) == 0:\n                continue\n            for number in [1, 2, 3]:\n                print('going to find upsell for number', number)\n                uc.find_upsell(type_service='upsell', anticendent_number=number, segement_name=item,\n                               cluster_number=cluster)\n            print(\"done with upsell cross sell\")\n\n            print('UpsellCrossell completed')\n\n\n    purchase_list_df = pd.concat(purchase_list)\n    purchase_list_df['dag_run_id']=str(dag_run_id)\n    purchase_list_df.to_csv(os.path.join(ml_location,  \"total_purchased_products.csv\"), header=True,\n                       index=False)\n    # for df_chunk in pd.read_csv(os.path.join(ml_location,  \"total_purchased_products.csv\"),\n    #                             chunksize=10000):\n    #     # insert each chunk into database table\n    #     df_chunk.to_sql(name='APA_total_purchased_products', con=engine, if_exists='append', index=False)\n    return purchase_list_df\n    \n    \n    \n\n\ndef transform(dataframe):\n    db = get_db()\n    df = association_process(\"manual__2023-07-10T11:06:51\",db)\n    \n    return df"
      }
    }, {
      "id": "d3229fcd-d746-b2de-b09c-f5722655b83c",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom urllib.parse import quote  \n\n\n\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\nfrom sqlalchemy.orm import relationship\nimport datetime\nfrom sqlalchemy.dialects.mysql import LONGTEXT\n\n\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nimport sklearn\nimport json\nimport dask.dataframe as dd\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\n# import configuration.config as cfg\n# import configuration.features as f\nimport traceback\nimport numpy as np\nfrom fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom pathlib import Path\nimport requests\n# from sql_app.repositories import AssociationRepo\n\n# config = cfg.Config().to_json()\n# features = f.Features().to_json()\n\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n'recharge_total_cntm2',\n'recharge_total_cntm3',\n'm1_total_voice_usage',\n'm1_total_data_usage',\n'm2_total_voice_usage',\n'm2_total_data_usage',\n'm3_total_voice_usage',\n'm3_total_data_usage',\n'purchase_total_cntm1',\n'purchase_total_cntm2',\n'purchase_total_cntm3', \n'm3_total_revenue_m2_total_revenue_pct_drop',\n'm3_data_revenue_m2_data_revenue_pct_drop',\n'm2_voice_rev_m1_voice_rev_pct_drop',\n'm2_total_revenue_m1_total_revenue_pct_drop',\n'm2_data_revenue_m1_data_revenue_pct_drop',\n'm3_voice_rev_m2_voice_rev_pct_drop',\n'm1_m2_m3_average_voice',\n'm1_m2_m3_average_data', \n'm1_no_of_days',\n'm2_no_of_days',\n'm3_no_of_days',\n'eng_index',\n'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCUSTOMER_NEEDED_COLUMN = [\n'onnet_revenue',  'onnet_usage', 'onnet_voice_count', \n'offnet_revenue', 'offnet_usage', 'offnet_voice_count',  \n'idd_revenue', 'idd_usage', 'idd_voice_count',\n'voice_rmg_revenue', 'voice_rmg_usage', 'voice_rmg_count', \n'data_rmg_revenue',  'data_rmg_usage',  \n'data_revenue', 'data_usage', 'sms_revenue', 'sms_usage',  'sms_idd_revenue', \n'sms_idd_usage', 'magik_voice_amount', 'rbt_subscription_rev', 'emergency_credit_rev', \n'package_revenue', 'voice_rev', 'sms_rev', 'onn_rev', 'off_rev', 'total_data_rev', \n'vas_rev', 'vas_rev_others', 'total_revenue', 'total_voice_count', 'total_voice_duration', \n'total_mainaccount_data_usage', 'total_sms_count', 'total_package_count', 'total_other_vas_count', \n'total_voice_usage', 'total_data_usage', 'total_sms_usage'\n]\n\n\n\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n\n\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\npurchase_location = '/home/tnmops/seahorse3_bkp/'\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\netl_location = '/home/tnmops/seahorse3_bkp/'\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nlog_file = '/home/tnmops/seahorse3_bkp/'\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n\nmsisdn_name = MSISDN_COL_NAME\n\n\n\ndef matrix_filter(dag_run_id):\n    try:\n\n        # read purchase information\n        path = os.path.join(ml_location, \"purchase_filtered\")\n        matrix_path = os.path.join(ml_location, \"matrix.csv\")\n        matrix_basic_path = os.path.join(ml_location, \"propensity_matrix_basic.csv\")\n        matrix_addon_path = os.path.join(ml_location, \"propensity_matrix_addon.csv\")\n        purchase_filter_path = os.path.join(ml_location, \"purchased_for_association\")\n\n        Path(purchase_filter_path).mkdir(parents=True, exist_ok=True)\n\n        purchase = dd.read_parquet(path)\n        path_d = os.path.join(ml_location, \"dict.pickle\")\n        with open(path_d, 'rb') as handle:\n            data = pickle.load(handle)\n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\n        # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\n        \n\n        # for key, value in data.items():\n        #     if value is None:\n        #         continue\n        #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n            \n        print(\"dict pickle file loaded\")\n        filtered_dict = {k: v for k, v in data.items()}\n        result = {}\n        purchase_list = []\n        for item, val in filtered_dict.items():\n            # val is the path item is the segment name\n\n            df = pd.read_csv(val)\n            purchase_filter = purchase[purchase[msisdn_name].isin(df[msisdn_name])]\n           \n            # get all the unique products  in the filtered purchase\n            products = purchase_filter[TRANSACTION_PRODUCT_NAME].unique()\n            print(products.dtype)\n            products = products.astype(int)\n            purchase_list = []\n            print(\"products are ==>\", products.compute())\n            for product in products:\n                purchase_filter_one_product = purchase_filter[\n                    purchase_filter[TRANSACTION_PRODUCT_NAME] == product]\n                # read the mathix\n                product = str(product)\n                matrix_check = pd.read_csv(matrix_path, nrows=2)\n                print(\"matrix dataframe \\n\", matrix_check)\n                if product not in matrix_check.columns:\n                    print(\"product is not present\")\n                    continue\n\n                matrix = dd.read_csv(matrix_path, usecols=[MSISDN_COL_NAME, product])\n                purchase_filter1 = purchase_filter_one_product.merge(matrix, on=msisdn_name, how='inner')\n                purchase_filter2 = purchase_filter1[purchase_filter1[product] > threshold]\n                purchase_filter2 = purchase_filter2.drop(columns=[product])\n                purchase_list.append(purchase_filter2)\n\n            #20-4-23 change after manjus changes\n            if purchase_list is None or len(purchase_list)==0:\n                result[item] = None\n                result_path = os.path.join(purchase_filter_path, 'dict.pickle')\n                with open(result_path, 'wb') as handle:\n                    pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n                continue\n\n            #20-4-23 change after manjus changes\n\n            purchase_final = dd.concat(purchase_list)\n\n            print('purchase_final.isnull().sum()', purchase_final.isnull().sum().compute())\n\n            op_path = os.path.join(purchase_filter_path, item + \".csv\")\n            purchase_final = purchase_final.compute()\n            purchase_final.to_csv(op_path)\n            result[item] = op_path\n            result_path = os.path.join(purchase_filter_path, 'dict.pickle')\n            with open(result_path, 'wb') as handle:\n                pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n            \n            print(\"file added successfully\")\n            \n        return matrix\n        \n    except Exception as e:\n        print(e)\n        \n\n\n\ndef transform(dataframe):\n    df = matrix_filter(\"manual__2023-07-10T11:06:51\")\n    return df.compute()"
      }
    }, {
      "id": "44f1d3eb-74ab-318d-319c-40f04be64a9a",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom fastapi import Depends, FastAPI, HTTPException\nimport pathlib\nimport dask.dataframe as dd\nimport datetime\nimport dask.array as da\nfrom pathlib import Path\nfrom icecream import ic\nimport  traceback\nfrom datetime import datetime\ngeneric_dict = {\"numerical_col\": \"Revenue\"}\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n    \ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \ndef join_rfm(x):\n    return str(int(x['R_Score'])) + str(int(x['F_Score'])) + str(int(x['M_Score']))\n    \ndef segmentaion_fun1(r, f, m):\n    segments = []\n    for r, f, m in zip(r, f, m):\n        value = int(f\"{r}{f}{m}\")\n\n        if value in [555, 554, 544, 545, 454, 455, 445]:\n            segments.append(\"Champions\")\n        elif value in [543, 444, 435, 355, 354, 345, 344, 335]:\n            segments.append(\"Loyal_Customers\")\n        elif value in [553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323]:\n            segments.append(\"Potential_Loyalist\")\n        elif value in [512, 511, 422, 421, 412, 411, 311]:\n            segments.append(\"Recent_Customers\")\n        elif value in [525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313]:\n            segments.append(\"Promising_Customers\")\n        elif value in [535, 534, 443, 434, 343, 334, 325, 324]:\n            segments.append(\"Customers_needing_Attention\")\n        elif value in [331, 321, 312, 221, 213]:\n            segments.append(\"About_to_Sleep\")\n        elif value in [255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124]:\n            segments.append(\"At_Risk\")\n        elif value in [155, 154, 144, 214, 215, 115, 114, 113]:\n            segments.append(\"Cant_Loose_them\")\n        elif value in [332, 322, 231, 241, 251, 233, 232, 223, 222, 132, 123, 122, 212, 211]:\n            segments.append(\"Hibernating\")\n        else:\n            segments.append(\"Lost\")\n\n    return segments\n    \n    \ndef form_segements_purchase_rfm(ctm_class):\n    def process_duplicates(li):\n        for i in range(len(li) - 1):\n            print(li[i], i)\n            if i + 1 == len(li) - 1:\n                li[i] = ((li[i - 1] + li[i]) / 2)\n            elif li[i] == li[i + 1]:\n                li[i + 1] = ((li[i + 1] + li[i + 2]) / 2)\n        return li\n\n    # bins_recency = [-1,\n    #                 np.percentile(ctm_class[\"Recency\"], 20),\n    #                 np.percentile(ctm_class[\"Recency\"], 40),\n    #                 np.percentile(ctm_class[\"Recency\"], 60),\n    #                 np.percentile(ctm_class[\"Recency\"], 80),\n    #                 ctm_class[\"Recency\"].max().compute()]\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n    # # bins_recency = [-1,17,35,53,71, ctm_class[\"Recency\"].max().compute()]\n\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Recency']=ctm_class1['Recency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Recency').agg({'msisdn':'count'}).reset_index()\n    mean_recency = ctm_class1['Recency'].max().compute()/5\n    print(\"mean_recency\", mean_recency)\n    print(\"mean_recency *2\",mean_recency * 2)\n    print(\"mean_recency *3\",mean_recency * 3)\n    print(\"mean_recency *4\",mean_recency * 4)\n    \n    \n    bins_recency = [-1,\n                    int(mean_recency),\n                    int(mean_recency * 2),\n                    int(mean_recency * 3),\n                    int(mean_recency * 4),\n                    ctm_class[\"Recency\"].max().compute()]\n    \n    print('bins_recency is', bins_recency)\n\n\n    ctm_class['R_Score'] = ctm_class[\"Recency\"].map_partitions(pd.cut,\n                                                               bins=bins_recency,\n                                                               labels=[5, 4, 3, 2, 1]).astype(\"int\")\n\n    \n\n    # bins_frequency = [-1,\n    #                   np.percentile(ctm_class[\"Frequency\"], 20),\n    #                   np.percentile(ctm_class[\"Frequency\"], 40),\n    #                   np.percentile(ctm_class[\"Frequency\"], 60),\n    #                   np.percentile(ctm_class[\"Frequency\"], 80),\n    #                   ctm_class[\"Frequency\"].max().compute()]\n\n    # bins_frequency.sort()\n    # bins_frequency = process_duplicates(bins_frequency)\n\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Frequency']=ctm_class1['Frequency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Frequency').agg({'msisdn':'count'}).reset_index()\n    mean_frequency = ctm_class1['Frequency'].max().compute()/5\n    print(\"mean_frequency\", mean_frequency)\n    print(\"mean_frequency *2\",mean_frequency * 2)\n    print(\"mean_frequency *3\",mean_frequency * 3)\n    print(\"mean_frequency *4\",mean_frequency * 4)\n\n\n    bins_frequency = [-1,\n                    int(mean_frequency),\n                    int(mean_frequency * 2),\n                    int(mean_frequency * 3),\n                    int(mean_frequency * 4),\n                    ctm_class[\"Frequency\"].max().compute()]\n    \n    print('bins_frequency is', bins_frequency)\n    ctm_class['F_Score'] = ctm_class[\"Frequency\"].map_partitions(pd.cut,\n                                                                 bins=bins_frequency,\n                                                                 labels=[1, 2, 3, 4, 5]).astype(\"int\")\n\n    # bins_revenue = [-1,\n    #                 np.percentile(ctm_class[\"Revenue\"], 20),\n    #                 np.percentile(ctm_class[\"Revenue\"], 40),\n    #                 np.percentile(ctm_class[\"Revenue\"], 60),\n    #                 np.percentile(ctm_class[\"Revenue\"], 80),\n    #                 ctm_class[\"Revenue\"].max().compute()]\n    # bins_revenue.sort()\n    # bins_revenue = process_duplicates(bins_revenue)\n\n    # bins_revenue = [-1,189,380,571,762, ctm_class[\"Revenue\"].max().compute()]\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Revenue'] = ctm_class1['Revenue'].map_partitions(lambda series: series.apply(np.ceil))\n    ctm_class1['Revenue']=ctm_class1['Revenue'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Revenue').agg({'msisdn':'count'}).reset_index()\n    mean_revenue = ctm_class1['Revenue'].max().compute()/5\n    \n    print(\"mean_revenue\", mean_revenue)\n    print(\"mean_revenue *2\",mean_revenue * 2)\n    print(\"mean_revenue *3\",mean_revenue * 3)\n    print(\"mean_revenue *4\",mean_revenue * 4)\n    \n    bins_revenue = [-1,\n                    int(mean_revenue),\n                    int(mean_revenue * 2),\n                    int(mean_revenue * 3),\n                    int(mean_revenue * 4),\n                    ctm_class[\"Revenue\"].max().compute()]\n    \n    print('bins_revenue is', bins_revenue)\n\n\n    ctm_class['M_Score'] = ctm_class[\"Revenue\"].map_partitions(pd.cut,\n                                                               bins=bins_revenue,\n                                                               labels=[1, 2, 3, 4, 5]).astype(\"int\")\n    print(\"done with scoring\")\n    # Form RFM segment\n\n    ctm_class['RFM_Segment'] = ctm_class.apply(join_rfm, axis=1)\n    ctm_class['RFM_Segment'] = ctm_class['RFM_Segment'].astype(int)\n    print(\"formed rfm segement \")\n    ctm_class['R_Score'] = ctm_class['R_Score'].astype(int)\n    ctm_class['F_Score'] = ctm_class['F_Score'].astype(int)\n    ctm_class['M_Score'] = ctm_class['M_Score'].astype(int)\n    print(\"computing rfm\")\n    r = ctm_class['R_Score'].values.compute()\n    f = ctm_class['F_Score'].values.compute()\n    m = ctm_class['M_Score'].values.compute()\n    seg = segmentaion_fun1(r, f, m)\n    chunks = ctm_class.map_partitions(lambda x: len(x)).compute().to_numpy()\n    myarray = da.from_array(seg, chunks=tuple(chunks))\n    ctm_class['Segment'] = myarray\n    return ctm_class\n    \n    \ndef otliner_removal(df,col, per=0.97):\n    q = df[col].quantile(per)\n    print(f\"col is {col} and q is {q}\")\n    print(\"the length brfore is\", len(df))\n    outliers = df[df[col] > q]\n    df1 = df[~df['msisdn'].isin(outliers['msisdn'].compute().values)]\n    print(\"the length after is\", len(df1))\n    return df1\n    \n\ndef perform_rfm_on_purchase(df_data_rfm, period=95):\n\n    ic(\"inside perform_rfm\")\n    \n    df_data_rfm.purchase_date = dd.to_datetime(df_data_rfm.purchase_date)\n    df_data_rfm = df_data_rfm.fillna(0)\n    msisdn_name = MSISDN_COL_NAME\n    # current behaviour data\n    min_date = df_data_rfm['purchase_date'].min().compute()\n    cur_beh_data_rfm = df_data_rfm[(df_data_rfm.purchase_date <= pd.Timestamp(min_date) + pd.Timedelta(days=period))\n                                   & (df_data_rfm.purchase_date >= pd.Timestamp(min_date))].reset_index(drop=True)\n    # Get the maximum purchase date of each customer and create a dataframe with it together with the customer's id.\n    ctm_max_purchase = cur_beh_data_rfm.groupby(msisdn_name).purchase_date.max().reset_index()\n    ctm_max_purchase.columns = [msisdn_name, 'MaxPurchaseDate']\n\n    # Find the recency of each customer in days\n    ctm_max_purchase['Recency'] = (\n            ctm_max_purchase['MaxPurchaseDate'].max() - ctm_max_purchase['MaxPurchaseDate']).dt.days\n    ctm_max_purchase = ctm_max_purchase.drop(columns=['MaxPurchaseDate'])\n    print(\"done with recency \")\n    # frequency\n    ctm_frequency = cur_beh_data_rfm.groupby(msisdn_name).total_cnt.sum().reset_index()\n    ctm_frequency.columns = [msisdn_name, 'Frequency']\n    print(\"done with frequency \")\n    cur_beh_data_rfm['Revenue'] = cur_beh_data_rfm[RECHARGE_TRANSACTION_PRICE_COL_NAME]\n    ctm_revenue = cur_beh_data_rfm.groupby('msisdn').Revenue.sum().reset_index()\n    print(\"done with monitory \")\n    rfm_data_base = dd.concat([ctm_max_purchase, ctm_revenue, ctm_frequency], axis=1)\n    rfm_data_base = rfm_data_base.loc[:, ~rfm_data_base.columns.duplicated()]\n\n    #for tnm purpose start\n    #rfm_data_base=otliner_removal(rfm_data_base,'Frequency')\n    rfm_data_base=otliner_removal(rfm_data_base,'Revenue')\n    \n    #for tnm purpose end \n\n\n    return rfm_data_base\n    \n\n\ndef arpu_trend(data):\n    months = usage_no_months\n    temp_df = None\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    #for daily summarised\n\n    usage={}\n\n    for month in usage_no_months:\n        usage[month] = dd.read_csv(\n            os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n        \n    usage_3m = dd.concat(list(usage.values()))\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    temp_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n\n    print('len(temp_df) is',len(temp_df) )\n\n    #for daily summarised\n\n\n\n    for month in months:\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n        # total_voice_rev = f.Features.CUSTOMER_VOICE_REVENUE[2]\n        needed_col = [MSISDN_COL_NAME, total_revenue]\n        df = data.get(month)[needed_col]\n        df['tot_rev'] = df[total_revenue]\n        df = df.drop(total_revenue, axis=1)\n        df = df.fillna(0)\n        #df = otliner_removal(df)\n\n        df['rev_segment'] = df.tot_rev.apply(put_revenue_segement)\n        df = df.rename(columns={\"rev_segment\": \"rev_segment_\" + month})\n        df['tot_rev' + month] = df['tot_rev']\n\n        df = df.drop('tot_rev', axis=1)\n        \n\n        if temp_df is None:\n            temp_df = df\n        else:\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n            temp_df[\"rev_segment_\" + month] = temp_df[\"rev_segment_\" + month].fillna(\"Zero\")\n            temp_df[\"tot_rev\" + month] = temp_df[\"tot_rev\" + month].fillna(0)\n\n    temp_df['tot_sum'] = temp_df['tot_revm1'] + temp_df['tot_revm2'] + temp_df['tot_revm3']\n    temp_df_non = temp_df\n    print(\"len of temp_df_non  after filtering is is  \", len(temp_df_non))\n    replace_map = get_banding_confitions().get(\"common\")\n\n    df1 = temp_df_non.replace(replace_map)\n    print(type(temp_df_non))\n\n    #to convert rev_segment_cols to int \n    rev_segment_cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    df1[rev_segment_cols] = df1[rev_segment_cols].astype('int32')\n    #to convert rev_segment_cols to int \n\n    cols = df1.columns[df1.columns.str.startswith('tot_revm')].tolist()\n\n    #to find trend using segment\n    #cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    #to find trend using segment\n\n    for col in cols:\n        df1[col] = df1[col].astype('int')\n        df1[col] = df1[col]\n\n    output = df1[cols].map_partitions(lambda part: part.apply(lambda x: pandas_wrapper(x), axis=1), meta=tuple)\n    output1 = output.to_frame(name='trend')\n    op = dd.concat([df1, output1], axis=1)\n\n    return op\n    \n    \ndef get_banding_confitions():\n        return {\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\n                               6: \"high_high\"}\n        }\n        \n        \n\n\ndef put_revenue_segement(x):\n    y = 'Zero'\n    if x == 0.0000:\n        y = 'Zero'\n    elif (x > 0) & (x <= 16):\n        y = 'very_low'\n    elif (x > 16) & (x <= 36):\n        y = 'low'\n    elif (x > 36) & (x <= 61):\n        y = 'medium'\n    elif (x > 61) & (x <= 175):\n        y = 'high'\n    else:\n        y = 'high_high'\n    return y\n    \n    \n    \ndef trends(numbers):\n    if len(numbers) != 3:\n        return \"Invalid input\"\n\n    if numbers[0] == numbers[1] == numbers[2]:\n        return \"flat\"\n    elif numbers[0] > numbers[1] > numbers[2]:\n        return \"Downtrend\"\n    elif numbers[0] < numbers[1] < numbers[2]:\n        return \"Uptrend\"\n    else:\n        return \"Zigzag\"\n        \n        \n        \n        \n\n\n\ndef pandas_wrapper(row):\n    return trends([row[2], row[1], row[0]])\n    \n    \ndef rfm_process_quantile_method_purchase(dag_run_id):\n    try:\n        msisdn_name = MSISDN_COL_NAME\n        \n        \n        file_name_dict = get_file_names()\n        dtype_purchase = RECHARGE_TRANSACTION_DTYPES\n        data = {}\n        print('purchase is going to  read')\n        for month in purchase_no_months:\n            data[month] = dd.read_csv(\n                os.path.join(purchase_location, file_name_dict.get(\"purchase\").get(month)),\n                dtype=dtype_purchase)\n        print('purchase readed')\n        df_data = dd.concat(list(data.values()))\n\n        #for daily summarised\n\n        usage={}\n\n        for month in usage_no_months:\n            usage[month] = dd.read_csv(\n                os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n            \n        #usage_3m = dd.concat(list(usage.values()))\n        trend_df = arpu_trend(usage)\n        trend_df = trend_df.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n        df_data = df_data[df_data[msisdn_name].isin(trend_df[msisdn_name].compute().unique())]\n        # profile\n        dtype = {'account_type': 'float64',\n                 'current_state': 'float64',\n                 'no_of_balances': 'float64',\n                 'primary_offer_id': 'float64',\n                 'subscr_no': 'float64',\n                 'payment_type':'object'}\n        profile = dd.read_csv(os.path.join(purchase_location, file_name_dict.get(\"profile\")), dtype=dtype)\n        profile[PROFILE_DATE_ENTER_ACTIVE] = dd.to_datetime(profile[PROFILE_DATE_ENTER_ACTIVE],\n                                                                       errors='coerce')\n\n        date_threshold = datetime.strptime('2023-02-12', '%Y-%m-%d')\n\n        profile['aon'] = (date_threshold - profile[PROFILE_DATE_ENTER_ACTIVE]).dt.days\n\n        \n        profile = profile[profile['aon'] >=90]\n        profile = profile[profile['current_state'] ==2]\n        profile = profile[profile['payment_type'] =='Prepaid']\n        print('df.columns',df_data.columns)\n        # df_data = df_data.merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n        #                                                               how='inner')\n        df_data=df_data[df_data['msisdn'].isin(profile['msisdn'].compute().unique())]\n\n        #for daily summarised\n\n        df_data = df_data.fillna(0, )\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        ctm_class = perform_rfm_on_purchase(df_data, period=95)#rfm using daily summarised\n        path = os.path.join(ml_location,\"rfm_before_segementation_purchase\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n        ctm_class = form_segements_purchase_rfm(ctm_class)\n        # Apply the function to the 'Segment' column\n        print('type(ctm_class)',type(ctm_class))\n\n        #ctm_class['Segment'] = ctm_class['Segment'].map(lambda s: 'purchase_rfm_' + s)\n\n        print(\"done with rfm outputing the file ongoing\")\n        path = os.path.join(ml_location, \"rfm_purchase\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n\n    \n        ic(\"rfm purchase segement value counts\", ctm_class['Segment'].value_counts().compute())\n        print(\"done with rfm outputing the file done \")\n        return ctm_class.compute()\n    except Exception as e:\n        print(e)\n        \n        \n\n    \n\n\n\ndef transform(dataframe):\n    df = rfm_process_quantile_method_purchase(\"manual__2023-07-10T11:06:51\")\n    return df"
      }
    }, {
      "id": "44acfe6c-f2ea-48b3-e490-5affa1c7cd15",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom fastapi import Depends, FastAPI, HTTPException\nimport pathlib\nimport dask.dataframe as dd\nimport datetime\nimport dask.array as da\nfrom pathlib import Path\nfrom icecream import ic\nimport  traceback\nfrom datetime import datetime\ngeneric_dict = {\"numerical_col\": \"Revenue\"}\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n\n\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_feb.csv\",\n            \"m2\": \"purchase_m1_20230206203137.csv\",\n            \"m3\": \"purchase_m2_20230206203137.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n\n\ndef join_rfm(x):\n    return str(int(x['R_Score'])) + str(int(x['F_Score'])) + str(int(x['M_Score']))\n    \n\n\ndef segmentaion_fun1(r, f, m):\n    segments = []\n    for r, f, m in zip(r, f, m):\n        value = int(f\"{r}{f}{m}\")\n\n        if value in [555, 554, 544, 545, 454, 455, 445]:\n            segments.append(\"Champions\")\n        elif value in [543, 444, 435, 355, 354, 345, 344, 335]:\n            segments.append(\"Loyal_Customers\")\n        elif value in [553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323]:\n            segments.append(\"Potential_Loyalist\")\n        elif value in [512, 511, 422, 421, 412, 411, 311]:\n            segments.append(\"Recent_Customers\")\n        elif value in [525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313]:\n            segments.append(\"Promising_Customers\")\n        elif value in [535, 534, 443, 434, 343, 334, 325, 324]:\n            segments.append(\"Customers_needing_Attention\")\n        elif value in [331, 321, 312, 221, 213]:\n            segments.append(\"About_to_Sleep\")\n        elif value in [255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124]:\n            segments.append(\"At_Risk\")\n        elif value in [155, 154, 144, 214, 215, 115, 114, 113]:\n            segments.append(\"Cant_Loose_them\")\n        elif value in [332, 322, 231, 241, 251, 233, 232, 223, 222, 132, 123, 122, 212, 211]:\n            segments.append(\"Hibernating\")\n        else:\n            segments.append(\"Lost\")\n\n    return segments\n    \n\n\n\ndef form_segements(ctm_class):\n    def process_duplicates(li):\n        for i in range(len(li) - 1):\n            print(li[i], i)\n            if i + 1 == len(li) - 1:\n                li[i] = ((li[i - 1] + li[i]) / 2)\n            elif li[i] == li[i + 1]:\n                li[i + 1] = ((li[i + 1] + li[i + 2]) / 2)\n        return li\n\n    # bins_recency = [-1,\n    #                 np.percentile(ctm_class[\"Recency\"], 20),\n    #                 np.percentile(ctm_class[\"Recency\"], 40),\n    #                 np.percentile(ctm_class[\"Recency\"], 60),\n    #                 np.percentile(ctm_class[\"Recency\"], 80),\n    #                 ctm_class[\"Recency\"].max().compute()]\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n    # bins_recency = [-1,17,35,53,71, ctm_class[\"Recency\"].max().compute()]\n\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Recency']=ctm_class1['Recency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Recency').agg({'msisdn':'count'}).reset_index()\n    mean_recency = ctm_class1['Recency'].max().compute()/5\n    print(\"mean_recency\", mean_recency)\n    print(\"mean_recency *2\",mean_recency * 2)\n    print(\"mean_recency *3\",mean_recency * 3)\n    print(\"mean_recency *4\",mean_recency * 4)\n    \n    \n    bins_recency = [-1,\n                    int(mean_recency),\n                    int(mean_recency * 2),\n                    int(mean_recency * 3),\n                    int(mean_recency * 4),\n                    ctm_class[\"Recency\"].max().compute()]\n    \n    print('bins_recency is', bins_recency)\n    \n\n    ctm_class['R_Score'] = ctm_class[\"Recency\"].map_partitions(pd.cut,\n                                                               bins=bins_recency,\n                                                               labels=[5, 4, 3, 2, 1]).astype(\"int\")\n\n    \n\n    # bins_frequency = [-1,\n    #                   np.percentile(ctm_class[\"Frequency\"], 20),\n    #                   np.percentile(ctm_class[\"Frequency\"], 40),\n    #                   np.percentile(ctm_class[\"Frequency\"], 60),\n    #                   np.percentile(ctm_class[\"Frequency\"], 80),\n    #                   ctm_class[\"Frequency\"].max().compute()]\n\n    # bins_frequency.sort()\n    # bins_frequency = process_duplicates(bins_frequency)\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Frequency']=ctm_class1['Frequency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Frequency').agg({'msisdn':'count'}).reset_index()\n    mean_frequency = ctm_class1['Frequency'].max().compute()/5\n    print(\"mean_frequency\", mean_frequency)\n    print(\"mean_frequency *2\",mean_frequency * 2)\n    print(\"mean_frequency *3\",mean_frequency * 3)\n    print(\"mean_frequency *4\",mean_frequency * 4)\n\n\n    bins_frequency = [-1,\n                    int(mean_frequency),\n                    int(mean_frequency * 2),\n                    int(mean_frequency * 3),\n                    int(mean_frequency * 4),\n                    ctm_class[\"Frequency\"].max().compute()]\n    \n    print('bins_frequency is', bins_frequency)\n    ctm_class['F_Score'] = ctm_class[\"Frequency\"].map_partitions(pd.cut,\n                                                                 bins=bins_frequency,\n                                                                 labels=[1, 2, 3, 4, 5]).astype(\"int\")\n\n    \n    \n    # bins_revenue = [-1,\n    #                 np.percentile(ctm_class[\"Revenue\"], 20),\n    #                 np.percentile(ctm_class[\"Revenue\"], 40),\n    #                 np.percentile(ctm_class[\"Revenue\"], 60),\n    #                 np.percentile(ctm_class[\"Revenue\"], 80),\n    #                 ctm_class[\"Revenue\"].max().compute()]\n    # bins_revenue.sort()\n    # bins_revenue = process_duplicates(bins_revenue)\n\n    #bins_revenue = [-1,189,380,571,762, ctm_class[\"Revenue\"].max().compute()]\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Revenue'] = ctm_class1['Revenue'].map_partitions(lambda series: series.apply(np.ceil))\n    ctm_class1['Revenue']=ctm_class1['Revenue'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Revenue').agg({'msisdn':'count'}).reset_index()\n    mean_revenue = ctm_class1['Revenue'].max().compute()/5\n    \n    print(\"mean_revenue\", mean_revenue)\n    print(\"mean_revenue *2\",mean_revenue * 2)\n    print(\"mean_revenue *3\",mean_revenue * 3)\n    print(\"mean_revenue *4\",mean_revenue * 4)\n    \n    bins_revenue = [-1,\n                    int(mean_revenue),\n                    int(mean_revenue * 2),\n                    int(mean_revenue * 3),\n                    int(mean_revenue * 4),\n                    ctm_class[\"Revenue\"].max().compute()]\n    \n    print('bins_revenue is', bins_revenue)\n\n    ctm_class['M_Score'] = ctm_class[\"Revenue\"].map_partitions(pd.cut,\n                                                               bins=bins_revenue,\n                                                               labels=[1, 2, 3, 4, 5]).astype(\"int\")\n    print(\"done with scoring\")\n    # Form RFM segment\n\n    ctm_class['RFM_Segment'] = ctm_class.apply(join_rfm, axis=1)\n    ctm_class['RFM_Segment'] = ctm_class['RFM_Segment'].astype(int)\n    print(\"formed rfm segement \")\n    ctm_class['R_Score'] = ctm_class['R_Score'].astype(int)\n    ctm_class['F_Score'] = ctm_class['F_Score'].astype(int)\n    ctm_class['M_Score'] = ctm_class['M_Score'].astype(int)\n    print(\"computing rfm\")\n    r = ctm_class['R_Score'].values.compute()\n    f = ctm_class['F_Score'].values.compute()\n    m = ctm_class['M_Score'].values.compute()\n    seg = segmentaion_fun1(r, f, m)\n    chunks = ctm_class.map_partitions(lambda x: len(x)).compute().to_numpy()\n    myarray = da.from_array(seg, chunks=tuple(chunks))\n    ctm_class['Segment'] = myarray\n    return ctm_class\n    \n    \n\ndef otliner_removal(df,col, per=0.97):\n    q = df[col].quantile(per)\n    print(f\"col is {col} and q is {q}\")\n    print(\"the length brfore is\", len(df))\n    outliers = df[df[col] > q]\n    df1 = df[~df['msisdn'].isin(outliers['msisdn'].compute().values)]\n    print(\"the length after is\", len(df1))\n    return df1\n    \n    \ndef get_banding_confitions():\n        return {\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\n                               6: \"high_high\"}\n        }\n        \n        \n\n\ndef arpu_trend(data):\n    months = usage_no_months\n    temp_df = None\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    #for daily summarised\n\n    usage={}\n\n    for month in usage_no_months:\n        usage[month] = dd.read_csv(\n            os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n        \n    usage_3m = dd.concat(list(usage.values()))\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    temp_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n\n    print('len(temp_df) is',len(temp_df) )\n\n    #for daily summarised\n\n\n\n    for month in months:\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n        # total_voice_rev = f.Features.CUSTOMER_VOICE_REVENUE[2]\n        needed_col = [MSISDN_COL_NAME, total_revenue]\n        df = data.get(month)[needed_col]\n        df['tot_rev'] = df[total_revenue]\n        df = df.drop(total_revenue, axis=1)\n        df = df.fillna(0)\n        #df = otliner_removal(df)\n\n        df['rev_segment'] = df.tot_rev.apply(put_revenue_segement)\n        df = df.rename(columns={\"rev_segment\": \"rev_segment_\" + month})\n        df['tot_rev' + month] = df['tot_rev']\n\n        df = df.drop('tot_rev', axis=1)\n        \n\n        if temp_df is None:\n            temp_df = df\n        else:\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n            temp_df[\"rev_segment_\" + month] = temp_df[\"rev_segment_\" + month].fillna(\"Zero\")\n            temp_df[\"tot_rev\" + month] = temp_df[\"tot_rev\" + month].fillna(0)\n\n    temp_df['tot_sum'] = temp_df['tot_revm1'] + temp_df['tot_revm2'] + temp_df['tot_revm3']\n    temp_df_non = temp_df\n    print(\"len of temp_df_non  after filtering is is  \", len(temp_df_non))\n    replace_map = get_banding_confitions().get(\"common\")\n\n    df1 = temp_df_non.replace(replace_map)\n    print(type(temp_df_non))\n\n    #to convert rev_segment_cols to int \n    rev_segment_cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    df1[rev_segment_cols] = df1[rev_segment_cols].astype('int32')\n    #to convert rev_segment_cols to int \n\n    cols = df1.columns[df1.columns.str.startswith('tot_revm')].tolist()\n\n    #to find trend using segment\n    #cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    #to find trend using segment\n\n    for col in cols:\n        df1[col] = df1[col].astype('int')\n        df1[col] = df1[col]\n\n    output = df1[cols].map_partitions(lambda part: part.apply(lambda x: pandas_wrapper(x), axis=1), meta=tuple)\n    output1 = output.to_frame(name='trend')\n    op = dd.concat([df1, output1], axis=1)\n\n    return op\n    \n    \ndef perform_rfm_on_daily_summ(df_data_rfm, period=95):\n\n    ic(\"inside perform_rfm in daily summarised\")\n\n    df_data_rfm.purchase_date = dd.to_datetime(df_data_rfm.purchase_date)\n    df_data_rfm = df_data_rfm.fillna(0)\n    msisdn_name = MSISDN_COL_NAME\n    # current behaviour data\n    min_date = df_data_rfm['purchase_date'].min().compute()\n    cur_beh_data_rfm = df_data_rfm[(df_data_rfm.purchase_date <= pd.Timestamp(min_date) + pd.Timedelta(days=period))\n                                   & (df_data_rfm.purchase_date >= pd.Timestamp(min_date))].reset_index(drop=True)\n    # Get the maximum purchase date of each customer and create a dataframe with it together with the customer's id.\n    ctm_max_purchase = cur_beh_data_rfm.groupby(msisdn_name).purchase_date.max().reset_index()\n    ctm_max_purchase.columns = [msisdn_name, 'MaxPurchaseDate']\n\n    # Find the recency of each customer in days\n    ctm_max_purchase['Recency'] = (\n            ctm_max_purchase['MaxPurchaseDate'].max() - ctm_max_purchase['MaxPurchaseDate']).dt.days\n    ctm_max_purchase = ctm_max_purchase.drop(columns=['MaxPurchaseDate'])\n    print(\"done with recency \")\n    # frequency\n    ctm_frequency = cur_beh_data_rfm.groupby(msisdn_name).size().reset_index()\n    ctm_frequency.columns = [msisdn_name, 'Frequency']\n    print(\"done with frequency \")\n    cur_beh_data_rfm['Revenue'] = cur_beh_data_rfm[DAILY_TRANSACTION_PRICE_COL_NAME]\n    ctm_revenue = cur_beh_data_rfm.groupby('msisdn').Revenue.sum().reset_index()\n    print(\"done with monitory \")\n    rfm_data_base = dd.concat([ctm_max_purchase, ctm_revenue, ctm_frequency], axis=1)\n    rfm_data_base = rfm_data_base.loc[:, ~rfm_data_base.columns.duplicated()]\n\n    #for btc purpose start\n    #rfm_data_base=otliner_removal(rfm_data_base,'Frequency')\n    rfm_data_base=otliner_removal(rfm_data_base,'Revenue')\n    \n    #for btc purpose end \n\n\n    return rfm_data_base\n    \n\n\ndef put_revenue_segement(x):\n    y = 'Zero'\n    if x == 0.0000:\n        y = 'Zero'\n    elif (x > 0) & (x <= 16):\n        y = 'very_low'\n    elif (x > 16) & (x <= 36):\n        y = 'low'\n    elif (x > 36) & (x <= 61):\n        y = 'medium'\n    elif (x > 61) & (x <= 175):\n        y = 'high'\n    else:\n        y = 'high_high'\n    return y\n    \n\n\ndef trends(numbers):\n    if len(numbers) != 3:\n        return \"Invalid input\"\n\n    if numbers[0] == numbers[1] == numbers[2]:\n        return \"flat\"\n    elif numbers[0] > numbers[1] > numbers[2]:\n        return \"Downtrend\"\n    elif numbers[0] < numbers[1] < numbers[2]:\n        return \"Uptrend\"\n    else:\n        return \"Zigzag\"\n        \n\n\ndef pandas_wrapper(row):\n    return trends([row[2], row[1], row[0]])\n    \n\n\ndef rfm_process_quantile_method_daily_summerized(dag_run_id):\n    try:\n        msisdn_name = MSISDN_COL_NAME\n        \n        \n        file_name_dict = get_file_names()\n        dtype_purchase = DAILY_TRANSACTION_DTYPES\n        data = {}\n        print('purchase is going to  read')\n        for month in purchase_no_months:\n            data[month] = dd.read_csv(\n                os.path.join(purchase_location, file_name_dict.get(\"daily_summerized\").get(month)),\n                dtype=dtype_purchase)\n        print('purchase readed')\n        df_data = dd.concat(list(data.values()))\n        \n        #for daily summarised\n\n        usage={}\n\n        for month in usage_no_months:\n            usage[month] = dd.read_csv(\n                os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n            \n        #usage_3m = dd.concat(list(usage.values()))\n        trend_df = arpu_trend(usage)\n        trend_df = trend_df.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n        df_data = df_data[df_data[msisdn_name].isin(trend_df[msisdn_name].compute().unique())]\n        print(\"usage filtered---\")\n        \n        # profile\n        dtype = {'account_type': 'float64',\n                 'current_state': 'float64',\n                 'no_of_balances': 'float64',\n                 'primary_offer_id': 'float64',\n                 'subscr_no': 'float64',\n                 'payment_type':'object'}\n        profile = dd.read_csv(os.path.join(purchase_location, file_name_dict.get(\"profile\")), dtype=dtype)\n        profile[PROFILE_DATE_ENTER_ACTIVE] = dd.to_datetime(profile[PROFILE_DATE_ENTER_ACTIVE],\n                                                                       errors='coerce')\n\n        date_threshold = datetime.strptime('2023-02-12', '%Y-%m-%d')\n\n        profile['aon'] = (date_threshold - profile[PROFILE_DATE_ENTER_ACTIVE]).dt.days\n\n        \n        profile = profile[profile['aon'] >=90]\n        profile = profile[profile['current_state'] ==2]\n        profile = profile[profile['payment_type'] =='Prepaid']\n        print('df.columns',df_data.columns)\n        # df_data = df_data.merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n        #                                                               how='inner')\n        df_data=df_data[df_data['msisdn'].isin(profile['msisdn'].compute().unique())]\n        df_data=df_data[['msisdn', 'fct_dt', 'total_revenue', 'avg_total_revenue']]\n\n\n        #for daily summarised\n\n        df_data = df_data.fillna(0 )\n        df_data = df_data.rename(columns={DAILY_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        ctm_class = perform_rfm_on_daily_summ(df_data, period=95)#rfm using daily summarised\n        path = os.path.join(ml_location, \"rfm_before_segementation\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n        ctm_class = form_segements(ctm_class)\n\n        print(\"done with rfm outputing the file ongoing\")\n        path = os.path.join(ml_location, \"rfm\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n\n        ic(\"rfm segement value counts\", ctm_class['Segment'].value_counts().compute())\n        print(\"done with rfm outputing the file done \")\n\n    except Exception as e:\n        print(e)\n        \n\ndef transform(dataframe):\n    rfm_process_quantile_method_daily_summerized(\"manual__2023-07-10T11:06:51\")\n    return dataframe"
      }
    }, {
      "id": "444458a0-a78d-e08c-16d2-3cb867961cbf",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nimport math\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\nfrom urllib.parse import quote  \n\n\n\nimport pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom urllib.parse import quote  \n\n\n\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\nfrom sqlalchemy.orm import relationship\nimport datetime\nfrom sqlalchemy.dialects.mysql import LONGTEXT\n\n\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\n\n\n\n\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\n\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URL,\n)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        return db\n    finally:\n        db.close()\n        \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\npurchase_location = '/home/tnmops/seahorse3_bkp/'\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\netl_location = '/home/tnmops/seahorse3_bkp/'\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nlog_file = '/home/tnmops/seahorse3_bkp/'\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCUSTOMER_NEEDED_COLUMN = [\n        'onnet_revenue',  'onnet_usage', 'onnet_voice_count', \n        'offnet_revenue', 'offnet_usage', 'offnet_voice_count',  \n        'idd_revenue', 'idd_usage', 'idd_voice_count',\n        'voice_rmg_revenue', 'voice_rmg_usage', 'voice_rmg_count', \n        'data_rmg_revenue',  'data_rmg_usage',  \n        'data_revenue', 'data_usage', 'sms_revenue', 'sms_usage',  'sms_idd_revenue', \n        'sms_idd_usage', 'magik_voice_amount', 'rbt_subscription_rev', 'emergency_credit_rev', \n        'package_revenue', 'voice_rev', 'sms_rev', 'onn_rev', 'off_rev', 'total_data_rev', \n        'vas_rev', 'vas_rev_others', 'total_revenue', 'total_voice_count', 'total_voice_duration', \n        'total_mainaccount_data_usage', 'total_sms_count', 'total_package_count', 'total_other_vas_count', \n        'total_voice_usage', 'total_data_usage', 'total_sms_usage'\n        ]\n\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n    \nimport logging\nimport os\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_jan.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_march.csv\"},\n\n        \"recharge\": {\n            \"m1\": \"recharge_july_20230411084648.csv\",\n            \"m2\": \"recharge_june_20230411084648.csv\",\n            \"m3\": \"recharge_may_20230411084648.csv\"\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_july_20230411090513.csv\",\n            \"m2\": \"purchase_june_20230411090513.csv\",\n            \"m3\": \"purchase_may_20230411090513.csv\"\n        },\n         \"daily_summerized\": {\n            \"m1\": \"daily_summarized_july_20230517122646.csv\",\n            \"m2\": \"daily_summarized_june_20230517122646.csv\",\n            \"m3\": \"daily_summarized_may_20230517122646.csv\",  \n          \n        },\n        \"weekwise\": \n        {\n          \n           \"m1\": \"weekwise_july_20230517180929.csv\",\n           \"m2\": \"weekwise_june_20230517180929.csv\",\n           \"m3\": \"weekwise_may_20230517180929.csv\", \n          \n        },\n        \"weekly_daily\": \n        {\n          \n            \"m1\": \"daily_weekly_avg_july_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_june_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_may_20230517163429.csv\" ,  \n          \n        },\n        \"rfm_purchase\": \n        {\n  \n            \"p1\": \"purchase_march_20230411090513.csv\",\n            \"p2\": \"purchase_april_20230411090513.csv\",\n            \"p3\": \"purchase_may_20230411090513.csv\",\n            \"p4\": \"purchase_june_20230411090513.csv\",\n            \"p5\": \"purchase_july_20230411090513.csv\"\n  \n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_july_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_june_20230517092908.csv\" ,            \n                      \n        },\n      \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n        \"profile\":\"profile_july_20230801055632.csv\"\n      \n    }\n\n\n\n    # source_purchase_and_etl_location = \"/log/magikuser/tnm_autopilot_calender_month\"\n    # purchase_location = \"/log/magikuser/autopilot_data/purchase\"\n    # etl_location = \"/log/magikuser/autopilot_data/etl/etl_with_130_columns\"\n    # pack_info_location = '/log/magikuser/autopilot_data/packinfo'\n    # ml_location = '/log/magikuser/autopilot_data/ml'\n    #\n    \n\nclass SegmentInformation(Base):\n    __tablename__ = \"APA_segment_information_new\"\n    id = Column(Integer, primary_key=True, index=True)\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\n    end_date = Column(DateTime)\n    current_product = Column(String(80), nullable=True, unique=False)\n    current_products_names = Column(String(200), nullable=True, unique=False)\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\n    predicted_arpu = Column(Integer, nullable=True)\n    current_arpu = Column(Integer, nullable=True)\n    segment_length = Column(String(80), nullable=True, unique=False)\n    rule = Column(LONGTEXT, nullable=True)\n    actual_rule = Column(LONGTEXT, nullable=True)\n    uplift_percent = Column(Float(precision=2), nullable=True)\n    incremental_revenue = Column(Float(precision=2), nullable=True)\n    campaign_type = Column(String(80), nullable=True, unique=False)\n    campaign_name = Column(String(80), nullable=True, unique=False)\n    action_key = Column(String(80), nullable=True, unique=False)\n    robox_id = Column(String(80), nullable=True, unique=False)\n    dag_run_id = Column(String(80), nullable=True, unique=False)\n    samples = Column(Integer, nullable=False)\n    segment_name = Column(String(80), nullable=True, unique=False)\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\n    customer_status = Column(String(80), nullable=True, unique=False)\n    query = Column(LONGTEXT, nullable=True, unique=False)\n    cluster_no = Column(Integer, nullable=True)\n    confidence = Column(Float(precision=2), nullable=True)\n    recommendation_type = Column(String(80), nullable=True, unique=False)\n    cluster_description = Column(LONGTEXT, nullable=True)\n    actual_target_count = Column(Integer, nullable=True)\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\n    total_revenue= Column(Integer, nullable=True)\n    uplift_revenue=Column(Integer, nullable=True)\n\n    def __repr__(self):\n        return 'SegmentInformation(name=%s)' % self.name\n\n\nclass SegementInfo(BaseModel):\n    end_date: Optional[str] = None\n    dag_run_id: Optional[str] = None\n    current_product: Optional[str] = None\n    current_products_names: Optional[str] = None\n    recommended_product_id: Optional[str] = None\n    recommended_product_name: Optional[str] = None\n    predicted_arpu: Optional[int] = None\n    current_arpu: Optional[int] = None\n    segment_length: Optional[str] = None\n    rule: Optional[str] = None\n    actual_rule: Optional[str] = None\n    uplift_percent: Optional[float] = None\n    incremental_revenue: Optional[float] = None\n    campaign_type: Optional[str] = None\n    campaign_name: Optional[str] = None\n    action_key: Optional[str] = None\n    robox_id: Optional[str] = None\n    samples: Optional[int] = None\n    segment_name: Optional[str] = None\n    current_ARPU_band: Optional[str] = None\n    current_revenue_impact: Optional[str] = None\n    customer_status: Optional[str] = None\n    query: Optional[str] = None\n    cluster_no: Optional[int] = None\n    confidence: Optional[float] = None\n    recommendation_type: Optional[str] = None\n    cluster_description: Optional[str] = None\n    actual_target_count: Optional[str] = None\n    top_purchased_day_1: Optional[str] = None\n    top_purchased_day_2: Optional[str] = None\n    top_purchased_day_3: Optional[str] = None\n    next_purchase_date_range: Optional[str] = None\n    campaign_response_percentage: Optional[str] = None\n    total_revenue: Optional[int] = None\n    uplift_revenue: Optional[int] = None\n\n\nclass SegementRepo:\n    def create(db: Session, segement: SegementInfo):\n        db_item = SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\n                                            campaign_type=segement.campaign_type,\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\n                                            rule=segement.rule, samples=segement.samples,\n                                            campaign_name=segement.campaign_name,\n                                            recommended_product_id=segement.recommended_product_id,\n                                            recommended_product_name=segement.recommended_product_name,\n                                            current_product=segement.current_product,\n                                            current_products_names=segement.current_products_names,\n                                            segment_length=segement.segment_length,\n                                            current_ARPU_band=segement.current_ARPU_band,\n                                            current_revenue_impact=segement.current_revenue_impact,\n                                            customer_status=segement.customer_status,\n                                            segment_name=segement.segment_name, query=segement.query,\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\n                                            recommendation_type=segement.recommendation_type,\n                                            cluster_description=segement.cluster_description,\n                                            actual_target_count=segement.actual_target_count,\n                                            top_purchased_day_1=segement.top_purchased_day_1,\n                                            top_purchased_day_2=segement.top_purchased_day_2,\n                                            top_purchased_day_3=segement.top_purchased_day_3,\n                                            next_purchase_date_range=segement.next_purchase_date_range,\n                                            campaign_response_percentage=segement.campaign_response_percentage,\n                                            total_revenue=segement.total_revenue,\n                                            uplift_revenue=segement.uplift_revenue,\n                                            )\n        db.add(db_item)\n        db.commit()\n        db.refresh(db_item)\n        return db_item\n\n    def findByAutoPilotIdAndClusterNo(db: Session, _id, cluster_no):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.cluster_no == cluster_no) \\\n            .all()\n\n    def findByAutoPilotIdAndSegementName(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).first()\n\n    def findByAutoPilotIdAndSegementNameAll(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).all()\n    \n    def findByAutoPilotIdAndSegementNamewithoutcluster(db: Session, _id, segement_name):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name).all()\n            \n\n\n    def findByAutoPilotId(db: Session, _id):\n        return db.query(SegmentInformation).filter(SegmentInformation.dag_run_id == _id).all()\n\n    def findByAutoPilotIdRecommendedId(db: Session, _id,recommended_product_id):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.recommended_product_id == recommended_product_id).all()\n\n    def deleteById(db: Session, _ids):\n        for id in _ids:\n            db.query(SegmentInformation).filter(SegmentInformation.id == id).delete()\n            db.commit()\n\n    def update(db: Session, item_data):\n        updated_item = db.merge(item_data)\n        db.commit()\n        return updated_item\n\n\ndef otliner_removal(df, per=0.97):\n    try:\n\n        q = df['total_revenue'].quantile(per)\n        print(\"the length brfore is\", len(df))\n        df = df[df['total_revenue'] < q]\n        print(\"the length after is\", len(df))\n        return df\n    except Exception as e:\n        print(e)\n        raise RuntimeError(e)\n        \ndef filter_data(temp_df):\n    cols = [p + \"_\" + s for s in CUSTOMER_NEEDED_COLUMN for p in usage_no_months]\n    cols.append(\"msisdn\")\n    temp_df = temp_df[cols]\n\n    return temp_df\n    \ndef train_model(data, sample=False):\n    def randomsample(X, y):\n        rus = RandomUnderSampler(random_state=42)\n        X_res, y_res = rus.fit_resample(X, y)\n        return X_res, y_res\n\n    def run_decision_tree(X_train, X_test, y_train, y_test):\n        clf = DecisionTreeClassifier(max_leaf_nodes=12)\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        print('Accuracy run_decision_tree: ', accuracy_score(y_test, y_pred))\n        print(\"confusion matrix run_decision_tree\", confusion_matrix(y_test, y_pred))\n        print(classification_report(y_test, y_pred))\n        return clf\n\n    X = data.drop(['msisdn', 'label'], axis=1)\n    y = data['label']\n\n    if sample:\n        X, y = randomsample(X=X, y=y)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    print(f\"X_train.shape = {X_train.shape}, X_test.shape =  {X_test.shape}\")\n    # sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\n    sel = SelectFromModel(DecisionTreeClassifier(max_leaf_nodes=12))\n    sel.fit(X_train, y_train)\n    sel.get_support()\n    features = X_train.columns[sel.get_support()]\n    print(\"the feature selection features are \", features)\n    X_train_rfc = sel.transform(X_train)\n    X_test_rfc = sel.transform(X_test)\n    clf1 = run_decision_tree(X_train_rfc, X_test_rfc, y_train, y_test)\n    features_importance = list(clf1.feature_importances_)\n    dic = {features[i]: features_importance[i] for i in range(len(features))}\n    # t = None\n    # try:\n    #     t = pd.read_csv('temp.csv')\n    #     tt = t.append(dic, ignore_index=True)\n    # except:\n    #     t = pd.DataFrame(columns=list(X_train.columns))\n    #     tt = t.append(dic, ignore_index=True)\n    #\n    # print(tt.head(5))\n    # tt.to_csv('temp.csv', header=True, index=False)\n    return clf1, features\n    \ndef get_top_product_ids(row):\n    return row.nlargest(2).index.tolist()\n    \ndef getpackname(product_id, packinfo_df):\n    product_name = \"No product name\"\n    try:\n        if (packinfo_df is not None):\n            if (product_id in packinfo_df['product_id'].values):\n\n                product_name = str(packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"product_name\"])\n                print(\"the product name of \", product_id, \" is \", product_name)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackname \", e)\n\n    return product_name\n    \n\n\ndef getpackprice(product_id, packinfo_df):\n    price = 0\n    try:\n        if (packinfo_df is not None):\n            if (product_id in packinfo_df['product_id'].values):\n\n                price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\n                print(\"the price  of \", product_id, \" is \", price)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackprice \", e)\n\n    return price\n    \n    \ndef insert_segement_info_churn(item, dag_run_id, data_filter_churners, recommenting_product,recommenting_product_name,\n                                      incremental_revenue,uplift, current_arpu,predicted_arpu ,total_revenue,db):\n    info = SegementInfo()\n    info.segment_name = item\n    info.cluster_description = None\n    info.dag_run_id = dag_run_id\n    current_product_id = None\n    info.current_product = current_product_id\n    # info.current_products_names = \"|\".join(\n    #     [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n    info.recommended_product_id\n    d = str(recommenting_product)\n    info.recommended_product_name = str(recommenting_product_name)\n    info.predicted_arpu = int(predicted_arpu)\n    info.current_arpu = int(current_arpu)\n    info.segment_length = len(data_filter_churners)\n    info.rule = None\n    info.actual_rule = None\n    info.uplift_revenue = float(uplift)\n    info.incremental_revenue = float(incremental_revenue)\n    info.campaign_type = \"churn\"\n    info.campaign_name = None\n    info.action_key = None\n    info.robox_id = None\n    info.samples = info.segment_length\n    info.segment_name = item\n    info.current_ARPU_band = None\n    info.current_revenue_impact = None\n    info.customer_status = None\n    info.query = None\n    info.cluster_no = None\n    info.confidence = None\n    info.total_revenue = total_revenue\n    # info.top_purchased_day_1=top_purchased_day_1\n    # info.top_purchased_day_2=top_purchased_day_2\n    # info.top_purchased_day_3=top_purchased_day_3\n    # info.next_purchase_date_range=next_purchase_date_range\n\n    info.recommendation_type = f\"nbo\"\n\n    SegementRepo.create(db, info)\n    \n\ndef load_picke_file(filename):\n    with open(filename, 'rb') as handle:\n        data = pickle.load(handle)\n    return data\n    \ndef churn_process(dag_run_id,db):\n    try:\n        \n        pack_info = os.path.join(etl_location, 'pack_info.csv')\n        pack_info_df = pd.read_csv(pack_info)\n        file_name_dict = get_file_names()\n        print(\"finding trend ongoing \")\n        data = {}\n        for month in usage_no_months:\n            data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\n                                      dtype=CUSTOMER_DTYPES)\n        months = usage_no_months\n        temp_df = None\n        for month in months:\n            df = data.get(month)\n            df = df.fillna(0)\n            total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n            # df['tot_rev'] = df[total_revenue]\n            df = otliner_removal(df.copy())#-------------------------------------------------------------\n            df = df.add_prefix(f\"{month}_\")\n            df = df.rename(columns={f\"{month}_msisdn\": \"msisdn\"})\n            if temp_df is None:\n                temp_df = df\n            else:\n\n                temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n                temp_df = temp_df.fillna(0)\n        temp_df = temp_df.compute()\n        temp_df = filter_data(temp_df)#----------------------------------------------------\n        temp_df['label'] = ((temp_df['m3_total_revenue'] > 0) &\n                            (temp_df['m2_total_revenue'] > 0) &\n                            (temp_df['m1_total_revenue'] == 0)).astype(int)\n        \n        data_for_uplift = temp_df[['msisdn','m1_total_revenue']]\n        temp_df = temp_df.loc[:, ~temp_df.columns.str.startswith('m1')]\n        clf1, features = train_model(temp_df, sample=True)#----------------------------------------------\n        ic(f\"the features selected are \", features)\n        data_for_prediction = temp_df[features]\n        \n        \n        msisdns = temp_df.pop(\"msisdn\")\n        probabilities = clf1.predict_proba(data_for_prediction)#------------------------------------------\n        data_for_prediction['predictions'] = clf1.predict(data_for_prediction)\n        data_for_prediction['predict_proba_0'] = probabilities[:, 0]\n        data_for_prediction['predict_proba_1'] = probabilities[:, 1]\n        data_for_prediction['msisdn'] = msisdns\n       \n        #matrix operations\n        matrix_path = os.path.join(ml_location,  \"matrix.csv\")\n        matrix = dd.read_csv(matrix_path)\n        data_path = os.path.join(ml_location,  \"dict.pickle\")\n        data_dict = load_picke_file(data_path)\n        \n        \n        # for key, value in data_dict.items():\n        #     data_dict[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n        \n#         for key, value in data.items():\n#             if value is None:\n#                 continue\n#             data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n            \n            \n        filtered_dict = {k: v for k, v in data_dict.items()}\n        final_df_ls=[]\n        for item, val in filtered_dict.items():\n            data = pd.read_csv(filtered_dict.get(item))\n            data_filter = data_for_prediction[data_for_prediction['msisdn'].isin(data['msisdn'])]\n            data_filter_churners = data_filter[data_filter['predict_proba_1'] > .50]\n            data_for_uplift_temp = data_for_uplift[data_for_uplift['msisdn'].isin(data_filter_churners['msisdn'])]\n\n            matrix_filter = matrix[matrix['msisdn'].isin(data['msisdn'])]\n            if matrix_filter is None or len(matrix_filter)==0:\n                continue\n            matrix_filter = matrix_filter.compute()\n            \n            msisdns_list = matrix_filter.pop(\"msisdn\")\n            matrix_filter['top_2_products'] = matrix_filter.iloc[:, 1:].apply(get_top_product_ids, axis=1)\n            matrix_filter['highest_product'] = matrix_filter['top_2_products'].apply(lambda x: x[0])\n            matrix_filter['second_highest_product'] = matrix_filter['top_2_products'].apply(lambda x: x[1])\n            print('len(matrix_filter)',len(matrix_filter))\n            print('matrix_filter[highest_product].value_counts()', matrix_filter['highest_product'].value_counts())\n            recommenting_product = matrix_filter['highest_product'].value_counts().idxmax()\n            data_filter_churners['recommended_product_id'] = recommenting_product\n            recommenting_product_name = str(getpackname(int(recommenting_product), pack_info_df))\n            data_filter_churners['recommenting_product_name'] = recommenting_product_name\n            data_filter_churners['segement_id'] = 0000\n\n            segements_data = item.split(\"-\")\n            trend = segements_data[0]\n            rfm = segements_data[1]\n            data_filter_churners['segement_name'] = item\n            data_filter_churners['trend'] = trend\n            data_filter_churners['rfm'] = rfm\n\n            print('recommenting_product is' ,recommenting_product)\n            print('recommenting_product_name is' ,recommenting_product_name)\n\n            current_arpu = data_for_uplift_temp['m1_total_revenue'].mean()\n            total_revenue = int(data_for_uplift_temp['m1_total_revenue'].sum())\n            if math.isnan(current_arpu):\n                current_arpu=0\n            recommenting_product_price = getpackprice(int(recommenting_product), pack_info_df)\n\n\n            print('recommenting_product_price is ',recommenting_product_price,'and type is ',type(recommenting_product_price))\n            \n            print('len(data_for_uplift_temp)',len(data_for_uplift_temp))\n            incremental_revenue =  recommenting_product_price * len(data_for_uplift_temp)\n\n            if incremental_revenue !=0:\n                incremental_revenue = (  60 / incremental_revenue) * 100\n            print('incremental_revenue',incremental_revenue)\n            initial_sum = data_for_uplift_temp['m1_total_revenue'].sum()\n\n            if initial_sum == 0 or incremental_revenue ==0:\n                 print('either initial_sum or incremental_revenue is 0'  )\n                 uplift =0\n            else:\n                print('initial_sum',initial_sum)\n                uplift = (incremental_revenue / initial_sum) * 100\n\n                uplift = round(uplift, 2)\n\n            print('uplift',uplift)\n            print('current_arpu',current_arpu)\n\n            predicted_arpu = int(current_arpu + ((current_arpu*uplift)/100))\n           \n           \n            print('predicted_arpu',predicted_arpu)\n           \n\n            if len(data_filter_churners) ==0:\n                continue\n\n            final_df_ls.append(data_filter_churners)\n\n            # recommended_pid = int(recommenting_product)\n            # print('recommended_pid is ',recommended_pid)\n            # print('type recommended_pid is ',type(recommended_pid))\n\n            # print('type next_purchase_date_df is ',next_purchase_date_df.dtypes)\n\n            # print('len is ', len(weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id'] == recommended_pid]))\n            # print(\"weekday is \", weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0])\n            # top_purchased_day_1 = weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0]\n            # top_purchased_day_2=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_2'].iloc[0]\n            # top_purchased_day_3=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_3'].iloc[0]\n            \n            # print('len of next is is ', len(next_purchase_date_df[next_purchase_date_df['recommended_product_id']==recommended_pid]))\n            # next_purchase_date_range=next_purchase_date_df[next_purchase_date_df['recommended_product_id']==recommended_pid]['range'].iloc[0]\n\n        \n            insert_segement_info_churn(item, dag_run_id, data_filter_churners, recommenting_product,recommenting_product_name,\n                                      incremental_revenue,uplift,current_arpu,predicted_arpu,total_revenue, db)\n        \n        final_df=pd.concat(final_df_ls)\n        final_df['dag_run_id']=str(dag_run_id)\n        final_df=final_df[['msisdn', 'segement_id', 'segement_name', 'trend', 'rfm', 'predictions',\n       'predict_proba_0', 'predict_proba_1', 'recommended_product_id',\n       'dag_run_id']]\n        \n        \n\n        APA_output_path = ml_location+'output_data/APA_output.csv'\n        APA_output=pd.read_csv(APA_output_path)\n\n\n        APA_output=pd.concat([APA_output,final_df])\n        APA_output_path = os.path.join(ml_location,  \"output_data\")\n\n        APA_output.to_csv(os.path.join(APA_output_path, \"APA_output.csv\"), header=True, index=False)\n\n        final_df.to_csv(os.path.join(ml_location,  \"churn_propensity.csv\"),\n                                   header=True, index=False)\n        ic(\"done outputting churn propensity \")\n        return final_df\n    except Exception as e:\n        print(e)\n        \n        \n\n\ndef transform(dataframe):\n    db = get_db()\n    df = churn_process(\"manual__2023-07-10T11:06:51\",db)\n    return df"
      }
    }, {
      "id": "1d706b8d-0eb5-1544-ed5f-335dc4e47cbd",
      "operation": {
        "id": "643d8706-24db-4674-b5b4-10b5129251fc",
        "name": "Transform"
      },
      "parameters": {
        "Parameters of input Transformer": {

        }
      }
    }, {
      "id": "29fc9508-6060-6ef9-5b6b-543b69d6b22d",
      "operation": {
        "id": "65240399-2987-41bd-ba7e-2944d60a3404",
        "name": "Create Custom Transformer"
      },
      "parameters": {
        "inner workflow": {
          "workflow": {
            "nodes": [{
              "id": "ea4eeaea-c74d-ef6b-437a-7c285e6ff58b",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom fastapi import Depends, FastAPI, HTTPException\nimport pathlib\nimport dask.dataframe as dd\nimport datetime\nimport dask.array as da\nfrom pathlib import Path\nfrom icecream import ic\nimport  traceback\nfrom datetime import datetime\ngeneric_dict = {\"numerical_col\": \"Revenue\"}\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n    \ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \ndef join_rfm(x):\n    return str(int(x['R_Score'])) + str(int(x['F_Score'])) + str(int(x['M_Score']))\n    \ndef segmentaion_fun1(r, f, m):\n    segments = []\n    for r, f, m in zip(r, f, m):\n        value = int(f\"{r}{f}{m}\")\n\n        if value in [555, 554, 544, 545, 454, 455, 445]:\n            segments.append(\"Champions\")\n        elif value in [543, 444, 435, 355, 354, 345, 344, 335]:\n            segments.append(\"Loyal_Customers\")\n        elif value in [553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323]:\n            segments.append(\"Potential_Loyalist\")\n        elif value in [512, 511, 422, 421, 412, 411, 311]:\n            segments.append(\"Recent_Customers\")\n        elif value in [525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313]:\n            segments.append(\"Promising_Customers\")\n        elif value in [535, 534, 443, 434, 343, 334, 325, 324]:\n            segments.append(\"Customers_needing_Attention\")\n        elif value in [331, 321, 312, 221, 213]:\n            segments.append(\"About_to_Sleep\")\n        elif value in [255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124]:\n            segments.append(\"At_Risk\")\n        elif value in [155, 154, 144, 214, 215, 115, 114, 113]:\n            segments.append(\"Cant_Loose_them\")\n        elif value in [332, 322, 231, 241, 251, 233, 232, 223, 222, 132, 123, 122, 212, 211]:\n            segments.append(\"Hibernating\")\n        else:\n            segments.append(\"Lost\")\n\n    return segments\n    \n    \ndef form_segements_purchase_rfm(ctm_class):\n    def process_duplicates(li):\n        for i in range(len(li) - 1):\n            print(li[i], i)\n            if i + 1 == len(li) - 1:\n                li[i] = ((li[i - 1] + li[i]) / 2)\n            elif li[i] == li[i + 1]:\n                li[i + 1] = ((li[i + 1] + li[i + 2]) / 2)\n        return li\n\n    # bins_recency = [-1,\n    #                 np.percentile(ctm_class[\"Recency\"], 20),\n    #                 np.percentile(ctm_class[\"Recency\"], 40),\n    #                 np.percentile(ctm_class[\"Recency\"], 60),\n    #                 np.percentile(ctm_class[\"Recency\"], 80),\n    #                 ctm_class[\"Recency\"].max().compute()]\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n    # # bins_recency = [-1,17,35,53,71, ctm_class[\"Recency\"].max().compute()]\n\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Recency']=ctm_class1['Recency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Recency').agg({'msisdn':'count'}).reset_index()\n    mean_recency = ctm_class1['Recency'].max().compute()/5\n    print(\"mean_recency\", mean_recency)\n    print(\"mean_recency *2\",mean_recency * 2)\n    print(\"mean_recency *3\",mean_recency * 3)\n    print(\"mean_recency *4\",mean_recency * 4)\n    \n    \n    bins_recency = [-1,\n                    int(mean_recency),\n                    int(mean_recency * 2),\n                    int(mean_recency * 3),\n                    int(mean_recency * 4),\n                    ctm_class[\"Recency\"].max().compute()]\n    \n    print('bins_recency is', bins_recency)\n\n\n    ctm_class['R_Score'] = ctm_class[\"Recency\"].map_partitions(pd.cut,\n                                                               bins=bins_recency,\n                                                               labels=[5, 4, 3, 2, 1]).astype(\"int\")\n\n    \n\n    # bins_frequency = [-1,\n    #                   np.percentile(ctm_class[\"Frequency\"], 20),\n    #                   np.percentile(ctm_class[\"Frequency\"], 40),\n    #                   np.percentile(ctm_class[\"Frequency\"], 60),\n    #                   np.percentile(ctm_class[\"Frequency\"], 80),\n    #                   ctm_class[\"Frequency\"].max().compute()]\n\n    # bins_frequency.sort()\n    # bins_frequency = process_duplicates(bins_frequency)\n\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Frequency']=ctm_class1['Frequency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Frequency').agg({'msisdn':'count'}).reset_index()\n    mean_frequency = ctm_class1['Frequency'].max().compute()/5\n    print(\"mean_frequency\", mean_frequency)\n    print(\"mean_frequency *2\",mean_frequency * 2)\n    print(\"mean_frequency *3\",mean_frequency * 3)\n    print(\"mean_frequency *4\",mean_frequency * 4)\n\n\n    bins_frequency = [-1,\n                    int(mean_frequency),\n                    int(mean_frequency * 2),\n                    int(mean_frequency * 3),\n                    int(mean_frequency * 4),\n                    ctm_class[\"Frequency\"].max().compute()]\n    \n    print('bins_frequency is', bins_frequency)\n    ctm_class['F_Score'] = ctm_class[\"Frequency\"].map_partitions(pd.cut,\n                                                                 bins=bins_frequency,\n                                                                 labels=[1, 2, 3, 4, 5]).astype(\"int\")\n\n    # bins_revenue = [-1,\n    #                 np.percentile(ctm_class[\"Revenue\"], 20),\n    #                 np.percentile(ctm_class[\"Revenue\"], 40),\n    #                 np.percentile(ctm_class[\"Revenue\"], 60),\n    #                 np.percentile(ctm_class[\"Revenue\"], 80),\n    #                 ctm_class[\"Revenue\"].max().compute()]\n    # bins_revenue.sort()\n    # bins_revenue = process_duplicates(bins_revenue)\n\n    # bins_revenue = [-1,189,380,571,762, ctm_class[\"Revenue\"].max().compute()]\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Revenue'] = ctm_class1['Revenue'].map_partitions(lambda series: series.apply(np.ceil))\n    ctm_class1['Revenue']=ctm_class1['Revenue'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Revenue').agg({'msisdn':'count'}).reset_index()\n    mean_revenue = ctm_class1['Revenue'].max().compute()/5\n    \n    print(\"mean_revenue\", mean_revenue)\n    print(\"mean_revenue *2\",mean_revenue * 2)\n    print(\"mean_revenue *3\",mean_revenue * 3)\n    print(\"mean_revenue *4\",mean_revenue * 4)\n    \n    bins_revenue = [-1,\n                    int(mean_revenue),\n                    int(mean_revenue * 2),\n                    int(mean_revenue * 3),\n                    int(mean_revenue * 4),\n                    ctm_class[\"Revenue\"].max().compute()]\n    \n    print('bins_revenue is', bins_revenue)\n\n\n    ctm_class['M_Score'] = ctm_class[\"Revenue\"].map_partitions(pd.cut,\n                                                               bins=bins_revenue,\n                                                               labels=[1, 2, 3, 4, 5]).astype(\"int\")\n    print(\"done with scoring\")\n    # Form RFM segment\n\n    ctm_class['RFM_Segment'] = ctm_class.apply(join_rfm, axis=1)\n    ctm_class['RFM_Segment'] = ctm_class['RFM_Segment'].astype(int)\n    print(\"formed rfm segement \")\n    ctm_class['R_Score'] = ctm_class['R_Score'].astype(int)\n    ctm_class['F_Score'] = ctm_class['F_Score'].astype(int)\n    ctm_class['M_Score'] = ctm_class['M_Score'].astype(int)\n    print(\"computing rfm\")\n    r = ctm_class['R_Score'].values.compute()\n    f = ctm_class['F_Score'].values.compute()\n    m = ctm_class['M_Score'].values.compute()\n    seg = segmentaion_fun1(r, f, m)\n    chunks = ctm_class.map_partitions(lambda x: len(x)).compute().to_numpy()\n    myarray = da.from_array(seg, chunks=tuple(chunks))\n    ctm_class['Segment'] = myarray\n    return ctm_class\n    \n    \ndef otliner_removal(df,col, per=0.97):\n    q = df[col].quantile(per)\n    print(f\"col is {col} and q is {q}\")\n    print(\"the length brfore is\", len(df))\n    outliers = df[df[col] > q]\n    df1 = df[~df['msisdn'].isin(outliers['msisdn'].compute().values)]\n    print(\"the length after is\", len(df1))\n    return df1\n    \n\ndef perform_rfm_on_purchase(df_data_rfm, period=95):\n\n    ic(\"inside perform_rfm\")\n    \n    df_data_rfm.purchase_date = dd.to_datetime(df_data_rfm.purchase_date)\n    df_data_rfm = df_data_rfm.fillna(0)\n    msisdn_name = MSISDN_COL_NAME\n    # current behaviour data\n    min_date = df_data_rfm['purchase_date'].min().compute()\n    cur_beh_data_rfm = df_data_rfm[(df_data_rfm.purchase_date <= pd.Timestamp(min_date) + pd.Timedelta(days=period))\n                                   & (df_data_rfm.purchase_date >= pd.Timestamp(min_date))].reset_index(drop=True)\n    # Get the maximum purchase date of each customer and create a dataframe with it together with the customer's id.\n    ctm_max_purchase = cur_beh_data_rfm.groupby(msisdn_name).purchase_date.max().reset_index()\n    ctm_max_purchase.columns = [msisdn_name, 'MaxPurchaseDate']\n\n    # Find the recency of each customer in days\n    ctm_max_purchase['Recency'] = (\n            ctm_max_purchase['MaxPurchaseDate'].max() - ctm_max_purchase['MaxPurchaseDate']).dt.days\n    ctm_max_purchase = ctm_max_purchase.drop(columns=['MaxPurchaseDate'])\n    print(\"done with recency \")\n    # frequency\n    ctm_frequency = cur_beh_data_rfm.groupby(msisdn_name).total_cnt.sum().reset_index()\n    ctm_frequency.columns = [msisdn_name, 'Frequency']\n    print(\"done with frequency \")\n    cur_beh_data_rfm['Revenue'] = cur_beh_data_rfm[RECHARGE_TRANSACTION_PRICE_COL_NAME]\n    ctm_revenue = cur_beh_data_rfm.groupby('msisdn').Revenue.sum().reset_index()\n    print(\"done with monitory \")\n    rfm_data_base = dd.concat([ctm_max_purchase, ctm_revenue, ctm_frequency], axis=1)\n    rfm_data_base = rfm_data_base.loc[:, ~rfm_data_base.columns.duplicated()]\n\n    #for tnm purpose start\n    #rfm_data_base=otliner_removal(rfm_data_base,'Frequency')\n    rfm_data_base=otliner_removal(rfm_data_base,'Revenue')\n    \n    #for tnm purpose end \n\n\n    return rfm_data_base\n    \n\n\ndef arpu_trend(data):\n    months = usage_no_months\n    temp_df = None\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    #for daily summarised\n\n    usage={}\n\n    for month in usage_no_months:\n        usage[month] = dd.read_csv(\n            os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n        \n    usage_3m = dd.concat(list(usage.values()))\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    temp_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n\n    print('len(temp_df) is',len(temp_df) )\n\n    #for daily summarised\n\n\n\n    for month in months:\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n        # total_voice_rev = f.Features.CUSTOMER_VOICE_REVENUE[2]\n        needed_col = [MSISDN_COL_NAME, total_revenue]\n        df = data.get(month)[needed_col]\n        df['tot_rev'] = df[total_revenue]\n        df = df.drop(total_revenue, axis=1)\n        df = df.fillna(0)\n        #df = otliner_removal(df)\n\n        df['rev_segment'] = df.tot_rev.apply(put_revenue_segement)\n        df = df.rename(columns={\"rev_segment\": \"rev_segment_\" + month})\n        df['tot_rev' + month] = df['tot_rev']\n\n        df = df.drop('tot_rev', axis=1)\n        \n\n        if temp_df is None:\n            temp_df = df\n        else:\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n            temp_df[\"rev_segment_\" + month] = temp_df[\"rev_segment_\" + month].fillna(\"Zero\")\n            temp_df[\"tot_rev\" + month] = temp_df[\"tot_rev\" + month].fillna(0)\n\n    temp_df['tot_sum'] = temp_df['tot_revm1'] + temp_df['tot_revm2'] + temp_df['tot_revm3']\n    temp_df_non = temp_df\n    print(\"len of temp_df_non  after filtering is is  \", len(temp_df_non))\n    replace_map = get_banding_confitions().get(\"common\")\n\n    df1 = temp_df_non.replace(replace_map)\n    print(type(temp_df_non))\n\n    #to convert rev_segment_cols to int \n    rev_segment_cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    df1[rev_segment_cols] = df1[rev_segment_cols].astype('int32')\n    #to convert rev_segment_cols to int \n\n    cols = df1.columns[df1.columns.str.startswith('tot_revm')].tolist()\n\n    #to find trend using segment\n    #cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    #to find trend using segment\n\n    for col in cols:\n        df1[col] = df1[col].astype('int')\n        df1[col] = df1[col]\n\n    output = df1[cols].map_partitions(lambda part: part.apply(lambda x: pandas_wrapper(x), axis=1), meta=tuple)\n    output1 = output.to_frame(name='trend')\n    op = dd.concat([df1, output1], axis=1)\n\n    return op\n    \n    \ndef get_banding_confitions():\n        return {\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\n                               6: \"high_high\"}\n        }\n        \n        \n\n\ndef put_revenue_segement(x):\n    y = 'Zero'\n    if x == 0.0000:\n        y = 'Zero'\n    elif (x > 0) & (x <= 16):\n        y = 'very_low'\n    elif (x > 16) & (x <= 36):\n        y = 'low'\n    elif (x > 36) & (x <= 61):\n        y = 'medium'\n    elif (x > 61) & (x <= 175):\n        y = 'high'\n    else:\n        y = 'high_high'\n    return y\n    \n    \n    \ndef trends(numbers):\n    if len(numbers) != 3:\n        return \"Invalid input\"\n\n    if numbers[0] == numbers[1] == numbers[2]:\n        return \"flat\"\n    elif numbers[0] > numbers[1] > numbers[2]:\n        return \"Downtrend\"\n    elif numbers[0] < numbers[1] < numbers[2]:\n        return \"Uptrend\"\n    else:\n        return \"Zigzag\"\n        \n        \n        \n        \n\n\n\ndef pandas_wrapper(row):\n    return trends([row[2], row[1], row[0]])\n    \n    \ndef rfm_process_quantile_method_purchase(dag_run_id):\n    try:\n        msisdn_name = MSISDN_COL_NAME\n        \n        \n        file_name_dict = get_file_names()\n        dtype_purchase = RECHARGE_TRANSACTION_DTYPES\n        data = {}\n        print('purchase is going to  read')\n        for month in purchase_no_months:\n            data[month] = dd.read_csv(\n                os.path.join(purchase_location, file_name_dict.get(\"purchase\").get(month)),\n                dtype=dtype_purchase)\n        print('purchase readed')\n        df_data = dd.concat(list(data.values()))\n\n        #for daily summarised\n\n        usage={}\n\n        for month in usage_no_months:\n            usage[month] = dd.read_csv(\n                os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n            \n        #usage_3m = dd.concat(list(usage.values()))\n        trend_df = arpu_trend(usage)\n        trend_df = trend_df.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n        df_data = df_data[df_data[msisdn_name].isin(trend_df[msisdn_name].compute().unique())]\n        # profile\n        dtype = {'account_type': 'float64',\n                 'current_state': 'float64',\n                 'no_of_balances': 'float64',\n                 'primary_offer_id': 'float64',\n                 'subscr_no': 'float64',\n                 'payment_type':'object'}\n        profile = dd.read_csv(os.path.join(purchase_location, file_name_dict.get(\"profile\")), dtype=dtype)\n        profile[PROFILE_DATE_ENTER_ACTIVE] = dd.to_datetime(profile[PROFILE_DATE_ENTER_ACTIVE],\n                                                                       errors='coerce')\n\n        date_threshold = datetime.strptime('2023-02-12', '%Y-%m-%d')\n\n        profile['aon'] = (date_threshold - profile[PROFILE_DATE_ENTER_ACTIVE]).dt.days\n\n        \n        profile = profile[profile['aon'] >=90]\n        profile = profile[profile['current_state'] ==2]\n        profile = profile[profile['payment_type'] =='Prepaid']\n        print('df.columns',df_data.columns)\n        # df_data = df_data.merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n        #                                                               how='inner')\n        df_data=df_data[df_data['msisdn'].isin(profile['msisdn'].compute().unique())]\n\n        #for daily summarised\n\n        df_data = df_data.fillna(0, )\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        ctm_class = perform_rfm_on_purchase(df_data, period=95)#rfm using daily summarised\n        path = os.path.join(ml_location,dag_run_id,\"rfm_before_segementation_purchase\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n        ctm_class = form_segements_purchase_rfm(ctm_class)\n        # Apply the function to the 'Segment' column\n        print('type(ctm_class)',type(ctm_class))\n\n        #ctm_class['Segment'] = ctm_class['Segment'].map(lambda s: 'purchase_rfm_' + s)\n\n        print(\"done with rfm outputing the file ongoing\")\n        path = os.path.join(ml_location,dag_run_id, \"rfm_purchase\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n\n    \n        ic(\"rfm purchase segement value counts\", ctm_class['Segment'].value_counts().compute())\n        print(\"done with rfm outputing the file done \")\n        return ctm_class.compute()\n    except Exception as e:\n        print(e)\n        \n        \n\n    \n\n\n\ndef transform(dataframe):\n    df = rfm_process_quantile_method_purchase(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }, {
              "id": "e56ff646-c454-5603-3eb2-b2c0ed91197a",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nfrom icecream import ic\r\nimport dask.dataframe as dd\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\nfrom pathlib import Path\r\nimport pickle\r\nimport math\r\nimport pyarrow as pa\r\nfrom dask.delayed import delayed\r\nimport dask\r\nfrom datetime import datetime\r\nfrom sklearn.feature_selection import SelectFromModel\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nfrom sklearn.feature_selection import SelectKBest, chi2\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.metrics import accuracy_score\r\nfrom imblearn.under_sampling import RandomUnderSampler\r\nfrom sklearn.metrics import classification_report\r\nimport vaex\r\nimport vaex.utils\r\n\r\n\r\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\r\n    'recharge_total_cntm2',\r\n    'recharge_total_cntm3',\r\n    'm1_total_voice_usage',\r\n    'm1_total_data_usage',\r\n    'm2_total_voice_usage',\r\n    'm2_total_data_usage',\r\n    'm3_total_voice_usage',\r\n    'm3_total_data_usage',\r\n    'purchase_total_cntm1',\r\n    'purchase_total_cntm2',\r\n    'purchase_total_cntm3', \r\n    'm3_total_revenue_m2_total_revenue_pct_drop',\r\n    'm3_data_revenue_m2_data_revenue_pct_drop',\r\n    'm2_voice_rev_m1_voice_rev_pct_drop',\r\n    'm2_total_revenue_m1_total_revenue_pct_drop',\r\n    'm2_data_revenue_m1_data_revenue_pct_drop',\r\n    'm3_voice_rev_m2_voice_rev_pct_drop',\r\n    'm1_m2_m3_average_voice',\r\n    'm1_m2_m3_average_data', \r\n    'm1_no_of_days',\r\n    'm2_no_of_days',\r\n    'm3_no_of_days',\r\n    'eng_index',\r\n    'consecutive_inactive_days']\r\n\r\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\r\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\r\nMSISDN_COL_NAME = 'msisdn'\r\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\r\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\r\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\r\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\r\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \r\nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\r\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\r\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\r\n\r\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\r\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\r\nTRANSACTION_PRODUCT_NAME = 'product_id'\r\n\r\nPACK_CONTI_FEATURES = []\r\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\r\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\r\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\r\nPACK_INFO_CATEGORY = 'product_type'\r\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\r\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\r\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\r\nCUSTOMER_CATEG_FEATURES = []\r\nCUSTOMER_CONTI_FEATURES = []\r\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\r\n# 0 index for inbundle 1 index for outbundled  2 index for total\r\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\r\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\r\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\r\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\r\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\r\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\r\n    'recharge_total_cntm2',\r\n    'recharge_total_cntm3',\r\n    'm1_total_voice_usage',\r\n    'm1_total_data_usage',\r\n    'm2_total_voice_usage',\r\n    'm2_total_data_usage',\r\n    'm3_total_voice_usage',\r\n    'm3_total_data_usage',\r\n    'purchase_total_cntm1',\r\n    'purchase_total_cntm2',\r\n    'purchase_total_cntm3', \r\n    'm3_total_revenue_m2_total_revenue_pct_drop',\r\n    'm3_data_revenue_m2_data_revenue_pct_drop',\r\n    'm2_voice_rev_m1_voice_rev_pct_drop',\r\n    'm2_total_revenue_m1_total_revenue_pct_drop',\r\n    'm2_data_revenue_m1_data_revenue_pct_drop',\r\n    'm3_voice_rev_m2_voice_rev_pct_drop',\r\n    'm1_m2_m3_average_voice',\r\n    'm1_m2_m3_average_data', \r\n    'm1_no_of_days',\r\n    'm2_no_of_days',\r\n    'm3_no_of_days',\r\n    'eng_index',\r\n    'consecutive_inactive_days']\r\n\r\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\r\n\r\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\r\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\r\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\r\n\r\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\r\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\r\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\r\n\r\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\r\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\r\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\r\n\r\nLABEL1 = 'downtrend'\r\nLABEL2 = 'uptrend'\r\nLABEL3 = 'zigzag'\r\nLABEL4 = 'flat'\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_features': self.TRANSACTION_FEATURES,\r\n        'usage_features': self.CUSTOMER_FEATURES,\r\n        'pack_info_features': self.ALL_PACK_FEATURES,\r\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\r\n        'usage_dtypes': self.CUSTOMER_DTYPES\r\n\r\n    }\r\n    \r\n    \r\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\r\nsegement_names = ['trend', 'rfm']\r\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\r\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\r\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\r\netl_location = \"/home/tnmops/seahorse3_bkp/\"\r\npack_info_location = '/home/tnmops/seahorse3_bkp/'\r\nml_location  ='/home/tnmops/seahorse3_bkp/'\r\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\r\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\r\n\r\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\r\n\r\nusage_no_months = ['m1', 'm2', 'm3']\r\nrecharge_no_months = ['m1', 'm2', 'm3']\r\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\r\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\r\nthreshold = 0.50\r\nnot_needed_rfm_segment = ['Lost']\r\n\r\npack_cols = ['product_id','product_type']\r\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\r\n\r\ncampaign_df= ['c4','c3']\r\ncampaign_usage= ['m4','m3']\r\n\r\ndef to_json(self):\r\n    return {\r\n        'purchase_location': self.purchase_location,\r\n        'usage_location': self.etl_location,\r\n        'pack_info_location': self.pack_info_location,\r\n        'ml_location': self.ml_location,\r\n        'recharge_location':self.recharge_location\r\n    }\r\n    \r\n    \r\n\r\n\r\ndef get_file_names():\r\n    return {\r\n        \"usage\": {\r\n            \"m1\": \"usage_march.csv\",\r\n            \"m2\": \"usage_feb.csv\",\r\n            \"m3\": \"usage_jan.csv\"\r\n        },\r\n        \"recharge\": {\r\n            \"m1\": \"recharge_march_20230809132937.csv\",\r\n            \"m2\": \"recharge_feb_20230809132937.csv\",\r\n            \"m3\": \"recharge_jan_20230809132937.csv\"\r\n\r\n        },\r\n        \"purchase\": {\r\n            \"m1\": \"purchase_march_full_df.csv\",\r\n            \"m2\": \"purchase_feb_full_df.csv\",\r\n            \"m3\": \"purchase_jan_full_df.csv\"\r\n\r\n        },\r\n        \"daily_summerized\": {\r\n            \"m1\": \"daily_summarized_march.csv\",\r\n            \"m2\": \"daily_summarized_feb.csv\",\r\n            \"m3\": \"daily_summarized_jan.csv\",\r\n        \r\n\r\n        },\r\n        \"weekwise\": \r\n        {\r\n        \r\n            \"m1\": \"weekwise_march_20230517180929.csv\",\r\n            \"m2\": \"weekwise_feb_20230517180929.csv\",\r\n            \"m3\": \"weekwise_jan_20230517180929.csv\",\r\n        \r\n        },\r\n        \"weekly_daily\": \r\n        {\r\n        \r\n            \"m1\": \"daily_weekly_avg_march_20230517163429.csv\",\r\n            \"m2\": \"daily_weekly_avg_feb_20230517163429.csv\",\r\n            \"m3\": \"daily_weekly_avg_jan_20230517163429.csv\",\r\n        \r\n        },\r\n        \"rfm_purchase\": \r\n        {\r\n\r\n            \"p1\": \"purchase_nov_full_df.csv\",\r\n            \"p2\": \"purchase_dec_full_df.csv\",\r\n            \"p3\": \"purchase_jan_full_df.csv\",\r\n            \"p4\": \"purchase_feb_full_df.csv\",\r\n            \"p5\": \"purchase_march_full_df.csv\"\r\n\r\n        },\r\n         \"campaign_data\": \r\n        {\r\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\r\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \r\n                        \r\n        },\r\n    \r\n        \"pack\":\r\n        {\r\n            \"pack\":\"pack_info.csv\"\r\n        },\r\n\r\n        \"profile\":\"profile_march_20230411085416.csv\"\r\n    \r\n\r\n    }\r\n    \r\n    \r\n\r\ndef segment_data(recharge_trend_usage_rfm, dag_run_id):\r\n    path_dict = {}\r\n    try:\r\n\r\n        print('inside segment_data')\r\n        #recharge_trend_usage_rfm['Segment'] = recharge_trend_usage_rfm['Segment'].apply(lambda x: str(x), vectorize=True)\r\n        for trend in recharge_trend_usage_rfm['trend'].unique():\r\n            print('inside for loop1')\r\n\r\n            for segement in recharge_trend_usage_rfm['Segment'].unique():\r\n                # if segement in cfg.Config.not_needed_rfm_segment:\r\n                #     continue\r\n                print('inside for loop2')\r\n                print('recharge_trend_usage_rfm.columns',recharge_trend_usage_rfm.columns)\r\n                for service_band in recharge_trend_usage_rfm['service_band'].unique():\r\n                    temp = recharge_trend_usage_rfm[\r\n                        (recharge_trend_usage_rfm['trend'] == trend) & (recharge_trend_usage_rfm['Segment'] == segement)& (recharge_trend_usage_rfm['service_band'] == service_band)]\r\n\r\n                    name = f\"{trend}-{segement}-{service_band}\"\r\n                    name = r\"\" + name\r\n                    file_name = f\"{name}.csv\"\r\n                    print(' file_name is ', file_name)\r\n\r\n                    length = len(temp)\r\n                    if length > 20:\r\n                        path = os.path.join(ml_location,dag_run_id,  file_name)\r\n                        print(f\"the length is suff {length} file name {name} \")\r\n                        # print('path_dict before is' ,path_dict)\r\n\r\n                        path_dict[str(name)] = str(path)\r\n                        # print('path_dict after  is' ,path_dict)\r\n\r\n   \r\n                        temp.to_csv(r\"\" + path,index=False)\r\n                        print('file_exported')\r\n\r\n                    else:\r\n                        print(f\"the length is  insuff {length} file name {name} \")\r\n        # print('here')\r\n        path_d = os.path.join(ml_location, dag_run_id,\"dict.pickle\")\r\n        print('path_d is', path_d)\r\n        print('path_dict is', path_dict)\r\n        with open(path_d, 'wb') as handle:\r\n            print('opened')\r\n            pickle.dump(path_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n            print('path_dict dumped ')\r\n\r\n    except Exception as e:\r\n        print(\"error occoured in segment_data\")\r\n        traceback.print_exc()\r\n        raise Exception(e)\r\n        \r\n        \r\ndef no_of_purchase_days_count(dag_run_id):\r\n    \r\n    msisdn_name =  MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    purchase_no_months = ['m1', 'm2', 'm3']\r\n    cdr_date = RECHARGE_TRANSACTION_PURCHASE_DATE_NAME\r\n    \r\n    print(file_name_dict)\r\n    print(\"finding no_of_purchase_days_count ongoing \")\r\n    purchase = {}\r\n    \r\n    for month in purchase_no_months:\r\n        purchase[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"purchase\").get(month)),)\r\n\r\n    purchase_3m = dd.concat(list(purchase.values()))\r\n    \r\n    purchase_3m_msisdn = purchase_3m[msisdn_name].unique().compute()\r\n    print('len(purchase_3m_msisdn) is',len(purchase_3m_msisdn) )\r\n\r\n    purchase_dict = {msisdn_name: purchase_3m_msisdn}\r\n    #print(usage_3m_dict)\r\n\r\n    print('len(usage_3m_msisdn) is',len(purchase_3m_msisdn) )\r\n        \r\n    purchase_base_df = dd.from_pandas(pd.DataFrame(purchase_dict), npartitions=1)\r\n    \r\n    \r\n    lis_df  = [purchase['m1'],purchase['m2'],purchase['m3']]\r\n    result = []\r\n    weekdays = []\r\n\r\n    def extract_day_of_week(series):\r\n            return series.dt.day_name().unique()\r\n\r\n    for i, df in enumerate(lis_df):\r\n        df[cdr_date] = dd.to_datetime(df[cdr_date])\r\n        df = df.groupby(msisdn_name)[cdr_date].apply(extract_day_of_week).reset_index()\r\n        weekdays.append(df)\r\n\r\n\r\n    for i, df in enumerate(lis_df, start =1):\r\n        new_column = f\"m{i}_no_of_days\"\r\n        df[cdr_date] = dd.to_datetime(df[cdr_date])\r\n        resultss = df.groupby(msisdn_name)[cdr_date].nunique().rename(new_column).reset_index()\r\n        result.append(resultss)\r\n    \r\n    df = purchase_base_df[[msisdn_name]].merge(result[0], on=msisdn_name, how='left').merge(result[1], on=msisdn_name, how='left').merge(result[2], on=msisdn_name, how='left').merge(weekdays[0], on=msisdn_name, how='left').merge(weekdays[1], on=msisdn_name, how='left').merge(weekdays[2], on=msisdn_name, how='left')\r\n    df = df.rename(columns={'cdr_date_x':'m1_weekdays', 'cdr_date_y': 'm2_weekdays','cdr_date': 'm3_weekdays'})\r\n    column = ['m1_weekdays','m2_weekdays','m3_weekdays']\r\n    \r\n    for col in column:\r\n        df[col] = df[col].apply(lambda x: ','.join(str(i) for i in x) if isinstance(x, list) else str(x))\r\n        df[col] = df[col].fillna(np.nan).replace('nan', '', regex=True)\r\n        \r\n    df =df.fillna(0)\r\n    \r\n    def assign_label(column):\r\n            labels = np.empty(len(column), dtype='object')\r\n            labels[(column == 0)] = 'A_[0]'\r\n            labels[(column > 0) & (column <= 5)] = 'B_[0_5]'  \r\n            labels[(column > 5 ) & (column <= 10)] = 'C_[5_10]'  \r\n            labels[(column > 10) & (column <= 15)] = 'D_[10_15]'  \r\n            labels[(column > 15) & (column <= 20)] = 'E_[15_20]'\r\n            labels[(column > 20) & (column <= 25)] = 'F_[20_25]'\r\n            labels[column >  25] = 'G_[25 +]'\r\n            return labels\r\n\r\n    columns = ['m1_no_of_days','m2_no_of_days','m3_no_of_days']\r\n    \r\n    for column in columns:\r\n        new_column = column + '_bands'\r\n        df[new_column] = df[column].map_partitions(assign_label, meta='object')\r\n    df = df.reset_index(drop =True)\r\n    \r\n    no_of_purchase_days_count_path = os.path.join(ml_location,dag_run_id, \"no_of_purchase_days_count\")\r\n    Path(no_of_purchase_days_count_path).mkdir(parents=True, exist_ok=True)\r\n    print(\"no_of_purchase_days_count   file output is ongoing \")\r\n    print('df.dtypes',df.dtypes)\r\n    df.to_parquet(no_of_purchase_days_count_path)\r\n    print(\"no_of_purchase_days_count   file output is completed \")\r\n    \r\n\r\n\r\ndef calculate_pct_drop_daily_weekly(dag_run_id):\r\n    \r\n    msisdn_name =  MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    weekly_daily_no_months = ['m1', 'm2', 'm3']\r\n    print(file_name_dict)\r\n    print(\"finding calculate_pct_drop ongoing \")\r\n    weekly_daily = {}\r\n    for month in weekly_daily_no_months:\r\n        weekly_daily[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"weekly_daily\").get(month)))\r\n                                  \r\n\r\n    usage_3m = dd.concat(list(weekly_daily.values()))\r\n    \r\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n\r\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\r\n    #print(usage_3m_dict)\r\n\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n        \r\n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\r\n    print(usage_base_df,'usage_base_df')\r\n    \r\n    m1 = weekly_daily['m1']\r\n    m2 = weekly_daily['m2']\r\n    m3 = weekly_daily['m3']\r\n   \r\n    \r\n    \r\n\r\n    etl_3_df = usage_base_df.merge(m1, on=msisdn_name, how='left').merge(m2, on=msisdn_name, how='left').merge(m3, on=msisdn_name, how='left')\r\n    print(etl_3_df)\r\n    etl_3_df = etl_3_df.fillna(0)\r\n    etl_3_df = dd.merge(dd.merge(m1, m2, on=msisdn_name), m3, on=msisdn_name)\r\n    new_column_names = {}\r\n    for col_name in etl_3_df.columns:\r\n        if col_name == msisdn_name:\r\n            new_col_name = col_name\r\n        elif col_name.endswith('_x'):\r\n            new_col_name = 'm1_' + col_name[:-2]\r\n        elif col_name.endswith('_y'):\r\n            new_col_name = 'm2_' + col_name[:-2]\r\n        else:\r\n            new_col_name = 'm3_' + col_name\r\n        new_column_names[col_name] = new_col_name\r\n\r\n    etl_3_df = etl_3_df.rename(columns=new_column_names)\r\n    etl_3_df = etl_3_df[['msisdn', 'm1_weekly_avg_voice_usage', 'm1_weekly_avg_data_usage','m1_daily_avg_voice_usage','m1_daily_avg_data_usage',\r\n                         'm2_weekly_avg_voice_usage', 'm2_weekly_avg_data_usage','m2_daily_avg_voice_usage','m2_daily_avg_data_usage',\r\n                         'm3_weekly_avg_voice_usage', 'm3_weekly_avg_data_usage','m3_daily_avg_voice_usage','m3_daily_avg_data_usage']]\r\n    df = etl_3_df.copy()\r\n    revenue_types = ['weekly_avg_voice_usage', 'weekly_avg_data_usage', 'daily_avg_voice_usage','daily_avg_data_usage']\r\n    for i in range(1, 3):\r\n        for rt in revenue_types:\r\n            col1 = f'm{i+1}_{rt}'\r\n            col2 = f'm{i}_{rt}'\r\n            pct_drop_col = f'{col1}_{col2}_pt_drop'\r\n            df[pct_drop_col] = (df[col1] - df[col2]) / df[col1] * 100\r\n            df[pct_drop_col] = df[pct_drop_col].replace([np.inf, -np.inf], np.nan)\r\n            df[pct_drop_col] = df[pct_drop_col].fillna(0)\r\n            \r\n    columns_to_select = ['msisdn'] + [col for col in df.columns if 'pt_drop' in col]\r\n    df = df.loc[:, columns_to_select]\r\n    \r\n        \r\n    def assign_label(column):\r\n        labels = np.empty(len(column), dtype='object')\r\n        labels[column == 0] = 'd)0'  \r\n        labels[(column > 0) & (column <= 50)] = 'e)0_50'  \r\n        labels[(column > 50) & (column <= 100)] = 'f)50_100'  \r\n        labels[column > 100] = 'g)>100'\r\n        labels[(column < 0) & (column >= -50)] = 'c)<0_-50' \r\n        labels[(column < -50) & (column >= -100)] = 'b)-50_-100'  \r\n        labels[column < -100] = 'a)>-100'\r\n        return labels\r\n\r\n\r\n    columns = ['m2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop','m2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop',\r\n    'm2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop','m2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop','m3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop','m3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop','m3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop','m3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop']\r\n        \r\n\r\n    for column in columns:\r\n        new_column = column + '_band'\r\n        df[new_column] = df[column].map_partitions(assign_label, meta='object')\r\n    \r\n    \r\n    daily_weekly_pct_drop = os.path.join(ml_location,dag_run_id,\"daily_weekly_pct_drop\")\r\n    Path(daily_weekly_pct_drop).mkdir(parents=True, exist_ok=True)\r\n    print(\"daily_weekly_pct_drop  file output is going on \")\r\n\r\n    df.to_parquet(daily_weekly_pct_drop)\r\n    \r\n    \r\n\r\ndef delta_calculation_weekwise(dag_run_id):\r\n    \r\n    msisdn_name =  MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    weekwise_no_months = ['m1', 'm2', 'm3']\r\n    print(file_name_dict)\r\n    print(\"finding calculate_pct_drop ongoing \")\r\n    weekwise = {}\r\n    for month in weekwise_no_months:\r\n        weekwise[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"weekwise\").get(month)))\r\n\r\n    usage_3m = dd.concat(list(weekwise.values()))\r\n    \r\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n\r\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\r\n   \r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n        \r\n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\r\n   \r\n    \r\n    \r\n\r\n    datasets = [ weekwise['m1'],weekwise['m2'],weekwise['m3']]\r\n    merged_dict = {}\r\n  \r\n    for i, df in enumerate(datasets, start=1):\r\n        dataset_prefix = f'm{i}'\r\n        voice_usage_columns = [col for col in df.columns if 'total_voice_usage' in col]\r\n        data_usage_columns = [col for col in df.columns if 'data_usage' in col]\r\n\r\n        for j in range(1, len(voice_usage_columns)):\r\n            if j != 2:  \r\n                prev_col = voice_usage_columns[j-1]\r\n                col = voice_usage_columns[j]\r\n                print('prev_col', prev_col)\r\n                print('col', col)\r\n                pct_drop_col = f'{prev_col}_{col}_pct_drop_{dataset_prefix}'\r\n                df[pct_drop_col] = ((df[prev_col] - df[col]) / df[prev_col]) * 100\r\n                df[pct_drop_col] = df[pct_drop_col].replace([np.inf, -np.inf], np.nan).fillna(0).astype(float)\r\n\r\n                prev_data_col = data_usage_columns[j-1]\r\n                data_col = data_usage_columns[j]\r\n                data_pct_drop_col = f'{prev_data_col}_{data_col}_pct_drop_{dataset_prefix}'\r\n                df[data_pct_drop_col] = ((df[prev_data_col] - df[data_col]) / df[prev_data_col]) * 100\r\n                df[data_pct_drop_col] = df[data_pct_drop_col].replace([np.inf, -np.inf], np.nan).fillna(0).astype(float)\r\n\r\n        w3_voice_pct_drop_col = f'w3_total_voice_usage_w4_total_voice_usage_pct_drop_{dataset_prefix}'\r\n        w3_data_pct_drop_col = f'w3_data_usage_w4_data_usage_pct_drop_{dataset_prefix}'\r\n\r\n        df[w3_voice_pct_drop_col] = ((df['w3_total_voice_usage'] - df['w4_total_voice_usage']) / df['w3_total_voice_usage']) * 100\r\n        df[w3_data_pct_drop_col] = ((df['w3_data_usage'] - df['w4_data_usage']) / df['w3_data_usage']) * 100\r\n\r\n        df = df.map_partitions(lambda df: df.replace([np.inf, -np.inf], np.nan).fillna(0).astype(float))\r\n\r\n       \r\n        merged_dict[dataset_prefix] = df\r\n\r\n   \r\n    merged_df = None\r\n    for key, value in merged_dict.items():\r\n        if merged_df is None:\r\n            merged_df = value\r\n        else:\r\n            merged_df = merged_df.merge(value, on=msisdn_name, suffixes=('', f'_{dataset_prefix}'))\r\n            \r\n    merged_df  =  usage_base_df[[msisdn_name]].merge(merged_df, on=msisdn_name, how='left')\r\n    merged_df = merged_df.fillna(0)\r\n    columns_to_select = ['msisdn'] + [col for col in merged_df.columns if 'pct_drop' in col]\r\n\r\n    merged_df = merged_df.loc[:, columns_to_select]\r\n    \r\n    def assign_label(column):\r\n        labels = np.empty(len(column), dtype='object')\r\n        labels[column == 0] = 'd)0'  # if 0\r\n        labels[(column > 0) & (column <= 50)] = 'e)0_50'  # 0 (exclusive) to 50 (inclusive)\r\n        labels[(column > 50) & (column <= 100)] = 'f)50_100'  # 50 (exclusive) to 100 (inclusive)\r\n        labels[column > 100] = 'g)>100'\r\n        labels[(column < 0) & (column >= -50)] = 'c)<0_-50'  # less than 0\r\n        labels[(column < -50) & (column >= -100)] = 'b)-50_-100'  # -50 (inclusive) to -100 (exclusive)\r\n        labels[column < -100] = 'a)>-100'\r\n        return labels\r\n\r\n    \r\n\r\n    \r\n    columns = ['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1',\r\n    'w1_data_usage_w2_data_usage_pct_drop_m1',\r\n    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1',\r\n    'w3_data_usage_w4_data_usage_pct_drop_m1',\r\n    'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2',\r\n    'w1_data_usage_w2_data_usage_pct_drop_m2',\r\n    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2',\r\n    'w3_data_usage_w4_data_usage_pct_drop_m2',\r\n    'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3',\r\n    'w1_data_usage_w2_data_usage_pct_drop_m3',\r\n    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3',\r\n    'w3_data_usage_w4_data_usage_pct_drop_m3']\r\n\r\n    for column in columns:\r\n        new_column = column + '_banded'\r\n        merged_df[new_column] = merged_df[column].map_partitions(assign_label, meta='object')\r\n\r\n    \r\n    weekwise_pct_drop = os.path.join(ml_location,dag_run_id,\"weekwise_pct_drop\")\r\n    Path( weekwise_pct_drop).mkdir(parents=True, exist_ok=True)\r\n    print(\" weekwise_pct_drop  file output is going on \")\r\n\r\n    merged_df.to_parquet(weekwise_pct_drop)\r\n    \r\n    \r\n\r\n\r\ndef inactive_days_band(dag_run_id):\r\n    \r\n    \r\n    date_name  =   DAILY_TRANSACTION_PURCHASE_DATE_NAME  \r\n\r\n    msisdn_name = MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    daily_summerized_month =  ['m1','m2','m3']\r\n    daily_summerized_dict = {}\r\n    for month in daily_summerized_month:\r\n        daily_summerized_dict[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"daily_summerized\").get(month)))\r\n        \r\n    m1 = daily_summerized_dict['m1']\r\n    m2 = daily_summerized_dict['m2']\r\n    m3 = daily_summerized_dict['m3']\r\n    \r\n\r\n    df = dd.concat([m1, m2, m3], axis=0)\r\n    \r\n    df[date_name] = dd.to_datetime(df[date_name])\r\n    current_date = df[date_name].max().compute()\r\n    last_active_date = df.groupby(msisdn_name)[date_name].max()\r\n    consecutive_inactive_days = (current_date - last_active_date).dt.days\r\n    inactive_days_df = consecutive_inactive_days.reset_index()\r\n    inactive_days_df.columns = ['msisdn', 'consecutive_inactive_days']\r\n    inactive_days_df = inactive_days_df[inactive_days_df['consecutive_inactive_days'] > 0]\r\n    bins = [-1, 5, 10, 15, 30, 90, float('inf')]\r\n    labels = ['a)0-5', 'b)5-10', 'c)10-15', 'd)15-30', 'e)30-90', 'f)90+']\r\n    inactive_days_df['inactive_days_band'] = inactive_days_df['consecutive_inactive_days'].map_partitions(lambda s: pd.cut(s, bins=bins, labels=labels).astype(str))\r\n    inactive_days_df['inactive_days_band'] = inactive_days_df['inactive_days_band'].astype('category')\r\n    \r\n    inactive_days_banding_path = os.path.join(ml_location,dag_run_id, \"inactive_days_banding_path\")\r\n    Path(inactive_days_banding_path).mkdir(parents=True, exist_ok=True)\r\n    print(\"inactive_days_banding_path bands  file output is going on \")\r\n\r\n    inactive_days_df.to_parquet(inactive_days_banding_path)\r\n    \r\n    \r\ndef three_month_engagement_index(dag_run_id):\r\n    msisdn_name = MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    daily_summerized_month =  ['m1','m2','m3']\r\n    daily_summerized_dict = {}\r\n    for month in daily_summerized_month:\r\n        daily_summerized_dict[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"daily_summerized\").get(month)))\r\n        \r\n    m1 = daily_summerized_dict['m1']\r\n    m2 = daily_summerized_dict['m2']\r\n    m3 = daily_summerized_dict['m3']\r\n    \r\n\r\n    df = dd.concat([m1, m2, m3], axis=0)\r\n    daily_summ_col = DAILY_TRANSACTION_PURCHASE_DATE_NAME\r\n    m1[daily_summ_col] = dd.to_datetime(m1[daily_summ_col])\r\n    m2[daily_summ_col] = dd.to_datetime(m2[daily_summ_col])\r\n    m3[daily_summ_col] = dd.to_datetime(m3[daily_summ_col])    \r\n    \r\n    \r\n    date_sum = m1[daily_summ_col].dt.day.max() + m2[daily_summ_col].dt.day.max() + m3[daily_summ_col].dt.day.max()\r\n\r\n    df_final = df[df['total_revenue'] > 0]\r\n    result = df_final.groupby(msisdn_name).fct_dt.count().reset_index()\r\n    result.columns = ['msisdn', 'fct_days']\r\n    result['eng_index'] = result['fct_days'] / date_sum\r\n    result['eng_index'] = result['eng_index'] * 100\r\n\r\n    df1 = df[df['total_revenue'] == 0]    \r\n    df1 = df1.merge(df_final[[msisdn_name]], how='left', indicator=True)\r\n    df1 = df1[df1['_merge'] == 'left_only'].drop('_merge', axis=1)\r\n\r\n\r\n    df1 = dd.from_pandas(pd.DataFrame(df1[msisdn_name].unique()), npartitions=1).reset_index(drop=True).rename(columns={0: 'msisdn'})\r\n    df1['eng_index'] = 0.0\r\n\r\n    result = result.drop('fct_days', axis=1)\r\n    f_result = dd.concat([result, df1], axis=0)\r\n    bins = [-1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 101]\r\n    labels = ['a)0_10', 'b)10_20', 'c)20_30', 'd)30_40', 'e)40_50',\r\n          'f)50_60', 'g)60_70', 'h)70_80', 'i)80_90', 'j)90_100']\r\n\r\n    f_result['eng_index_band'] = dd.from_dask_array(f_result['eng_index'].to_dask_array(), columns=['eng_index_band']).map_partitions(\r\n        lambda df: pd.cut(df['eng_index_band'], bins=bins, labels=labels))\r\n    \r\n    engagment_index_banding_path = os.path.join(ml_location, dag_run_id,\"engagment_index_banding_path\")\r\n    Path(engagment_index_banding_path).mkdir(parents=True, exist_ok=True)\r\n    print(\"engagment_index bands  file output is going on \")\r\n    f_result.to_parquet(engagment_index_banding_path)\r\n    \r\n    \r\ndef usage_banding_process(dag_run_id):\r\n\r\n    msisdn_name = MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    print(\"finding calculate_pct_drop ongoing \")\r\n    usage = {}\r\n    for month in usage_no_months:\r\n        usage[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\r\n                                    dtype=CUSTOMER_DTYPES,usecols =CUSTOMER_USAGE_COLUMNS )\r\n\r\n    usage_3m = dd.concat(list(usage.values()))\r\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n\r\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\r\n\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n        \r\n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\r\n\r\n\r\n    m1 = usage['m1']\r\n    m2 = usage['m2']\r\n    m3 = usage['m3']\r\n\r\n    \r\n    etl_3_df = usage_base_df.merge(m1, on=msisdn_name, how='left').merge(m2, on=msisdn_name, how='left').merge(m3, on=msisdn_name, how='left')\r\n    \r\n    etl_3_df = etl_3_df.fillna(0)\r\n    new_column_names = {}\r\n    for col_name in etl_3_df.columns:\r\n        if col_name == 'msisdn':\r\n            new_col_name = col_name\r\n        elif col_name.endswith('_x'):\r\n            new_col_name = 'm1_' + col_name[:-2]\r\n        elif col_name.endswith('_y'):\r\n            new_col_name = 'm2_' + col_name[:-2]\r\n        else:\r\n            new_col_name = 'm3_' + col_name\r\n        new_column_names[col_name] = new_col_name\r\n    etl_3_df = etl_3_df.rename(columns=new_column_names)\r\n    etl_3_df = etl_3_df[['msisdn', 'm1_total_voice_usage', 'm1_total_data_usage',\r\n                         'm2_total_voice_usage', 'm2_total_data_usage',\r\n                         'm3_total_voice_usage', 'm3_total_data_usage']]\r\n    df = etl_3_df.copy()\r\n    df['m1_m2_m3_average_voice'] = df[['m1_total_voice_usage', 'm2_total_voice_usage', 'm3_total_voice_usage']].mean(axis=1)\r\n    df['m1_m2_m3_average_data'] = df[['m1_total_data_usage', 'm2_total_data_usage', 'm3_total_data_usage']].mean(axis=1)\r\n    return df \r\n    \r\n    \r\n    \r\ndef usage_category(df, column_name, band_function):\r\n    df[column_name + '_band'] = df[column_name].map_partitions(lambda x: x.apply(band_function), meta=('object'))\r\n    return df[['msisdn', column_name + '_band',column_name]]\r\n\r\n\r\n\r\ndef voice_band(val):\r\n    \r\n    if val == 0.0:\r\n        return 'Zero'\r\n    elif (val > 0) and (val <= 119):\r\n        return 'low'\r\n    elif (val > 119) and (val <= 238):\r\n        return 'medium'\r\n    elif (val > 238) and (val <= 357):\r\n        return 'high'\r\n    elif val > 357:\r\n        return 'very_high'\r\n    else:\r\n        return None\r\n        \r\n        \r\ndef data_band(val):\r\n    \r\n    if val == 0.0:\r\n        return 'Zero'\r\n    elif (val > 0) and (val <= 1542.):\r\n        return 'low'\r\n    elif (val > 1542.) and (val <= 3084.):\r\n        return 'medium'\r\n    elif (val > 3084.) and (val <= 4626.):\r\n        return 'high'\r\n    elif val > 4626.:\r\n        return 'very_high'\r\n    else:\r\n        return None\r\n        \r\n        \r\n\r\ndef data_voice_usage_banding(dag_run_id):\r\n    usage_process_df = usage_banding_process(dag_run_id)\r\n    df_voice_band = usage_category(usage_process_df, 'm1_m2_m3_average_voice', voice_band)\r\n    df_data_band = usage_category(usage_process_df, 'm1_m2_m3_average_data', data_band)\r\n    usage_band_df = dd.merge(df_voice_band, df_data_band, on='msisdn')\r\n\r\n    voice_data_usage_banding_path = os.path.join(ml_location,dag_run_id, \"voice_data_usage_banding\")\r\n    Path(voice_data_usage_banding_path).mkdir(parents=True, exist_ok=True)\r\n    print(\"usage bands  file output is ongoing \")\r\n    usage_band_df.to_parquet(voice_data_usage_banding_path)\r\n    \r\n\r\ndef calculate_pct_drop(dag_run_id):\r\n\r\n    \r\n\r\n    def assign_label(column):\r\n        \r\n        labels = np.empty(len(column), dtype='object')\r\n        labels[column == 0] = 'd)0'  \r\n        labels[(column > 0) & (column <= 50)] = 'e)0_50'  \r\n        labels[(column > 50) & (column <= 100)] = 'f)50_100'  \r\n        labels[column > 100] = 'g)>100'\r\n        labels[(column < 0) & (column >= -50)] = 'c)<0_-50'  \r\n        labels[(column < -50) & (column >= -100)] = 'b)-50_-100' \r\n        labels[column < -100] = 'a)>-100'\r\n        return labels\r\n\r\n    msisdn_name = MSISDN_COL_NAME\r\n    file_name_dict = get_file_names()\r\n    print(\"finding calculate_pct_drop ongoing \")\r\n    usage = {}\r\n    for month in usage_no_months:\r\n        usage[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\r\n                                    dtype= CUSTOMER_DTYPES,usecols = CUSTOMER_DROP_COLUMNS )\r\n\r\n    usage_3m = dd.concat(list(usage.values()))\r\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n\r\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\r\n\r\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\r\n        \r\n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\r\n\r\n\r\n    m1 = usage['m1']\r\n    m2 = usage['m2']\r\n    m3 = usage['m3']\r\n   \r\n    \r\n    columns = ['m2_total_revenue_m1_total_revenue_pct_drop', 'm2_voice_rev_m1_voice_rev_pct_drop', 'm2_data_revenue_m1_data_revenue_pct_drop', 'm3_total_revenue_m2_total_revenue_pct_drop', 'm3_voice_rev_m2_voice_rev_pct_drop', 'm3_data_revenue_m2_data_revenue_pct_drop']\r\n\r\n    etl_3_df = usage_base_df.merge(m1, on=msisdn_name, how='left').merge(m2, on=msisdn_name, how='left').merge(m3, on=msisdn_name, how='left')\r\n    print(etl_3_df)\r\n    etl_3_df = etl_3_df.fillna(0)\r\n    new_column_names = {}\r\n    for col_name in etl_3_df.columns:\r\n        if col_name == 'msisdn':\r\n            new_col_name = col_name\r\n        elif col_name.endswith('_x'):\r\n            new_col_name = 'm1_' + col_name[:-2]\r\n        elif col_name.endswith('_y'):\r\n            new_col_name = 'm2_' + col_name[:-2]\r\n        else:\r\n            new_col_name = 'm3_' + col_name\r\n        new_column_names[col_name] = new_col_name\r\n\r\n\r\n    etl_3_df = etl_3_df.rename(columns=new_column_names)\r\n    etl_3_df = etl_3_df[['msisdn', 'm1_total_revenue', 'm1_voice_rev', 'm1_data_revenue',\r\n                        'm2_total_revenue', 'm2_voice_rev', 'm2_data_revenue',\r\n                        'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue']]\r\n    df = etl_3_df.copy()\r\n    revenue_types = ['total_revenue', 'voice_rev', 'data_revenue']\r\n    for i in range(1, 3):\r\n        for rt in revenue_types:\r\n            col1 = f'm{i+1}_{rt}'\r\n            col2 = f'm{i}_{rt}'\r\n            pct_drop_col = f'{col1}_{col2}_pct_drop'\r\n            df[pct_drop_col] = (df[col1] - df[col2]) / df[col1] * 100\r\n            df[pct_drop_col] = df[pct_drop_col].replace([np.inf, -np.inf], np.nan)\r\n            df[pct_drop_col] = df[pct_drop_col].fillna(0)\r\n        df[pct_drop_col] = df[pct_drop_col].astype('float64')\r\n\r\n        \r\n\r\n    for column in columns:\r\n        new_column = column + '_banded'\r\n        df[new_column] = df[column].map_partitions(assign_label, meta='object')\r\n    \r\n    usage_rev_drop_path = os.path.join(ml_location,dag_run_id, \"usage_rev_drop\")\r\n    Path(usage_rev_drop_path).mkdir(parents=True, exist_ok=True)\r\n    print(\"revenue drop   file output is ongoing \")\r\n\r\n    \r\n    \r\n    expected_schema = pa.schema([\r\n    ('msisdn', pa.int64()),\r\n    ('m1_total_revenue', pa.float32()),\r\n    ('m1_voice_rev', pa.float64()),\r\n    ('m1_data_revenue', pa.float64()),\r\n    ('m2_total_revenue', pa.float32()),\r\n    ('m2_voice_rev', pa.float64()),\r\n    ('m2_data_revenue', pa.float64()),\r\n    ('m3_total_revenue', pa.float32()),\r\n    ('m3_voice_rev', pa.float64()),\r\n    ('m3_data_revenue', pa.float64()),\r\n    ('m2_total_revenue_m1_total_revenue_pct_drop', pa.float64()),\r\n    ('m2_voice_rev_m1_voice_rev_pct_drop', pa.float64()),\r\n    ('m2_data_revenue_m1_data_revenue_pct_drop', pa.float64()),\r\n    ('m3_total_revenue_m2_total_revenue_pct_drop', pa.float64()),\r\n    ('m3_voice_rev_m2_voice_rev_pct_drop', pa.float64()),\r\n    ('m3_data_revenue_m2_data_revenue_pct_drop', pa.float64()),\r\n    ('m2_total_revenue_m1_total_revenue_pct_drop_banded', pa.string()),\r\n    ('m2_voice_rev_m1_voice_rev_pct_drop_banded', pa.string()),\r\n    ('m2_data_revenue_m1_data_revenue_pct_drop_banded', pa.string()),\r\n    ('m3_total_revenue_m2_total_revenue_pct_drop_banded', pa.string()),\r\n    ('m3_voice_rev_m2_voice_rev_pct_drop_banded', pa.string()),\r\n    ('m3_data_revenue_m2_data_revenue_pct_drop_banded', pa.string()),\r\n    ('__null_dask_index__', pa.int64())\r\n    ])\r\n    \r\n    \r\n\r\n    df.to_parquet(usage_rev_drop_path, schema=expected_schema)\r\n    print(\"-------need to perquet\")\r\n    \r\n    \r\n\r\n\r\ndef get_banding_confitions():\r\n        return {\r\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\r\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\r\n                               6: \"high_high\"}\r\n        }\r\n        \r\n        \r\n\r\ndef segementation(dag_run_id):\r\n    try:\r\n        recharge_path = os.path.join(ml_location,dag_run_id,  \"recharge_band\")\r\n        recharge = vaex.open(recharge_path, )\r\n        print(\"loaded recharge\")\r\n        recharge=recharge[['msisdn','recharge_total_cntm1','recharge_total_cntm2','recharge_total_cntm3']]\r\n\r\n        path_pur = os.path.join(ml_location,dag_run_id,  \"purchase_all_months\")\r\n        pur_all_months = dd.read_parquet(path_pur)\r\n        print(\"purchase 3 months loaded\")\r\n\r\n        purchase_path = os.path.join(ml_location,dag_run_id, \"purchase_band\")\r\n        purchase = vaex.open(purchase_path)\r\n        print(\"loaded purchase\")\r\n        purchase=purchase[['msisdn','purchase_total_cntm1','purchase_total_cntm2','purchase_total_cntm3']]\r\n\r\n        path_recharge = os.path.join(ml_location,dag_run_id, \"recharge_all_months\")\r\n        recharge_all_months = dd.read_parquet(path_recharge)\r\n        print(\"recharge 3 months loaded\")\r\n\r\n        trend_path = os.path.join(ml_location,dag_run_id, \"trend_filtered\")\r\n        trend = vaex.open(trend_path)\r\n        print(\"loaded trend\")\r\n        #trend = trend[['msisdn', 'tot_revm1', 'tot_revm2', 'tot_revm3']]\r\n        trend = trend[['msisdn', 'rev_segment_m1', 'rev_segment_m2', 'rev_segment_m3', 'trend']]\r\n        replace_map = get_banding_confitions().get(\"common_reverse\")\r\n\r\n        trend['rev_segment_m1'] = trend.rev_segment_m1.map(replace_map)\r\n        trend['rev_segment_m2'] = trend.rev_segment_m2.map(replace_map)\r\n        trend['rev_segment_m3'] = trend.rev_segment_m3.map(replace_map)\r\n\r\n        usage_path = os.path.join(ml_location, dag_run_id, \"usage_bands\")\r\n        usage = vaex.open(usage_path)\r\n        usage=usage[['msisdn','m1_total_voice_usage','m1_total_data_usage','m2_total_voice_usage', 'm2_total_data_usage','m3_total_voice_usage', 'm3_total_data_usage']]\r\n        print(\"loaded usage\")\r\n\r\n        rfm_path = os.path.join(ml_location,dag_run_id, \"rfm\")\r\n        rfm = vaex.open(rfm_path)\r\n        rfm = rfm[['msisdn', 'Segment']]\r\n        print(\"loaded rfm\")\r\n        # ------------nnew logic----------------#\r\n        # trend_rfm = trend.join(rfm, on='msisdn', how=\"inner\")\r\n        trend_rfm = trend.copy()\r\n        trend_rfm=trend_rfm[['msisdn','trend']]\r\n        trend_rfm_recharge = trend_rfm.join(recharge, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge['recharge_total_cntm1'] = trend_rfm_recharge['recharge_total_cntm1'].fillna(0)\r\n        trend_rfm_recharge['recharge_total_cntm2'] = trend_rfm_recharge['recharge_total_cntm2'].fillna(0)\r\n        trend_rfm_recharge['recharge_total_cntm3'] = trend_rfm_recharge['recharge_total_cntm3'].fillna(0)\r\n        trend_rfm_recharge_usage = trend_rfm_recharge.join(usage, on='msisdn', how=\"left\")\r\n\r\n\r\n         #to fill null values if there is in data and voice band \r\n        trend_rfm_recharge_usage['m1_total_voice_usage'] = trend_rfm_recharge_usage['m1_total_voice_usage'].fillna(0)\r\n        trend_rfm_recharge_usage['m1_total_data_usage'] = trend_rfm_recharge_usage['m1_total_data_usage'].fillna(0)\r\n        trend_rfm_recharge_usage['m2_total_voice_usage'] = trend_rfm_recharge_usage['m1_total_voice_usage'].fillna(0)\r\n        trend_rfm_recharge_usage['m2_total_data_usage'] = trend_rfm_recharge_usage['m2_total_data_usage'].fillna(0)\r\n        trend_rfm_recharge_usage['m3_total_voice_usage'] = trend_rfm_recharge_usage['m1_total_voice_usage'].fillna(0)\r\n        trend_rfm_recharge_usage['m3_total_data_usage'] = trend_rfm_recharge_usage['m3_total_data_usage'].fillna(0)\r\n\r\n        #to fill null values if there is in data and voice band \r\n\r\n\r\n\r\n        trend_rfm_recharge_usage_purchase = trend_rfm_recharge_usage.join(purchase, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase['purchase_total_cntm1'] = trend_rfm_recharge_usage_purchase[\r\n            'purchase_total_cntm1'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase['purchase_total_cntm2'] = trend_rfm_recharge_usage_purchase[\r\n            'purchase_total_cntm2'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase['purchase_total_cntm3'] = trend_rfm_recharge_usage_purchase[\r\n            'purchase_total_cntm3'].fillna(0)\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase.join(rfm, on='msisdn', how=\"inner\")\r\n\r\n        #joining usage_drop_percentage\r\n        calculate_pct_drop(dag_run_id)\r\n        usage_rev_drop_path = os.path.join(ml_location,dag_run_id, \"usage_rev_drop\")\r\n        usage_drop_percentage = vaex.open(usage_rev_drop_path)\r\n        usage_drop_percentage = usage_drop_percentage[['msisdn','m3_total_revenue_m2_total_revenue_pct_drop', \r\n                                                       'm3_data_revenue_m2_data_revenue_pct_drop', \r\n                                                       'm2_voice_rev_m1_voice_rev_pct_drop', \r\n                                                       'm2_total_revenue_m1_total_revenue_pct_drop', \r\n                                                       'm2_data_revenue_m1_data_revenue_pct_drop', \r\n                                                       'm3_voice_rev_m2_voice_rev_pct_drop']]\r\n\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.join(usage_drop_percentage, on='msisdn', how=\"left\")\r\n       \r\n        #joining usage_drop_percentage\r\n\r\n        #joining usage bands\r\n        data_voice_usage_banding(dag_run_id)\r\n        voice_data_usage_banding_path = os.path.join(ml_location,dag_run_id, \"voice_data_usage_banding\")\r\n        voice_data_usage_banding = vaex.open(voice_data_usage_banding_path)\r\n        voice_data_usage_banding=voice_data_usage_banding[['msisdn','m1_m2_m3_average_voice','m1_m2_m3_average_data']]\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.join(voice_data_usage_banding, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_voice']=trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_voice'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_data']=trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_data'].fillna(0)\r\n        #joining usage bands\r\n\r\n\r\n        #joining engagement index \r\n        three_month_engagement_index(dag_run_id)\r\n        engagment_index_banding_path = os.path.join(ml_location,dag_run_id, \"engagment_index_banding_path\")\r\n        engagment_index_banding = vaex.open(engagment_index_banding_path)\r\n\r\n        engagment_index_banding=engagment_index_banding[['msisdn','eng_index']]\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.join(engagment_index_banding, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['eng_index']=trend_rfm_recharge_usage_purchase_rfm['eng_index'].fillna(0)\r\n\r\n\r\n        #joining engagement index \r\n\r\n\r\n        #joining purchase rfm\r\n\r\n        rfm__purchase_path = os.path.join(ml_location,dag_run_id, \"rfm_purchase\")\r\n        rfm_purchase = vaex.open(rfm__purchase_path)\r\n        print('type(rfm_purchase)',type(rfm_purchase))\r\n        rfm_purchase.rename('RFM_Segment', 'rfm_purchase_segment', unique=False)\r\n        rfm_purchase = rfm_purchase[['msisdn', 'rfm_purchase_segment']]\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(rfm_purchase, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['rfm_purchase_segment']=trend_rfm_recharge_usage_purchase_rfm['rfm_purchase_segment'].fillna(000)\r\n\r\n        \r\n        #joining purchase rfm\r\n\r\n\r\n         #joining inactivity \r\n\r\n        inactive_days_band(dag_run_id)\r\n        inactive_days_banding_path = os.path.join(ml_location,dag_run_id,\"inactive_days_banding_path\")\r\n        inactivity_days_df = vaex.open(inactive_days_banding_path)\r\n        inactivity_days_df=inactivity_days_df[['msisdn', 'consecutive_inactive_days']]\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(inactivity_days_df, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['consecutive_inactive_days']=trend_rfm_recharge_usage_purchase_rfm['consecutive_inactive_days'].fillna(0)\r\n\r\n\r\n        #joining inactivity \r\n\r\n        #joining weekly delta change\r\n        delta_calculation_weekwise(dag_run_id)\r\n        weekwise_pct_drop = os.path.join(ml_location,dag_run_id,\"weekwise_pct_drop\")\r\n        weekwise_pct_drop_df = vaex.open(weekwise_pct_drop)\r\n        weekwise_pct_drop_df=weekwise_pct_drop_df[['msisdn',\r\n            'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3', \r\n            'w1_data_usage_w2_data_usage_pct_drop_m3',\r\n             'w3_data_usage_w4_data_usage_pct_drop_m2',\r\n              'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3',\r\n               'w1_data_usage_w2_data_usage_pct_drop_m1',\r\n                'w1_data_usage_w2_data_usage_pct_drop_m2',\r\n                 'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1',\r\n                  'w3_data_usage_w4_data_usage_pct_drop_m1',\r\n                   'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1',\r\n                    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2',\r\n                      'w3_data_usage_w4_data_usage_pct_drop_m3',\r\n                        'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2']]\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(weekwise_pct_drop_df, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m3'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m2'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m1'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m2'].fillna(0)        \r\n        trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1'].fillna(0)        \r\n        trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m1'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m3'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2'].fillna(0)\r\n\r\n        #joining weekly delta change\r\n\r\n\r\n        #joining weekly-daily delta change\r\n\r\n\r\n        calculate_pct_drop_daily_weekly(dag_run_id)\r\n\r\n        daily_weekly_pct_drop = os.path.join(ml_location,dag_run_id,\"daily_weekly_pct_drop\")\r\n        daily_weekly_pct_drop_df = vaex.open(daily_weekly_pct_drop)\r\n        daily_weekly_pct_drop_df=daily_weekly_pct_drop_df[['msisdn',\r\n           'm2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop',\r\n             'm2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop',\r\n               'm3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop',\r\n                 'm3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop',\r\n                   'm2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop',\r\n                     'm3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop',\r\n                       'm2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop',\r\n                         'm3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop']]\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(daily_weekly_pct_drop_df, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop'].fillna(0)       \r\n        trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop'].fillna(0)\r\n\r\n\r\n        #joining weekly-daily delta change\r\n\r\n\r\n        #joining purchase number of days \r\n        no_of_purchase_days_count(dag_run_id)\r\n        no_of_purchase_days_count_path = os.path.join(ml_location, dag_run_id,\"no_of_purchase_days_count\")\r\n        print('opening no_of_purchase_days_count file'  )\r\n        no_of_purchase_days_count_df = vaex.open(no_of_purchase_days_count_path)\r\n        no_of_purchase_days_count_df=no_of_purchase_days_count_df[['msisdn', 'm1_no_of_days',\r\n       'm2_no_of_days', 'm3_no_of_days']]\r\n        \r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(no_of_purchase_days_count_df, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['m1_no_of_days']= trend_rfm_recharge_usage_purchase_rfm['m1_no_of_days'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m2_no_of_days']= trend_rfm_recharge_usage_purchase_rfm['m2_no_of_days'].fillna(0)\r\n        trend_rfm_recharge_usage_purchase_rfm['m3_no_of_days']= trend_rfm_recharge_usage_purchase_rfm['m3_no_of_days'].fillna(0)\r\n        #joining purchase number of days \r\n\r\n        #adding usage service\r\n        service_path = os.path.join(ml_location,dag_run_id,\"favorite_Service.csv\")\r\n        print('opening favorite_Service file'  )\r\n        usage_service = vaex.open(service_path)\r\n        usage_service=usage_service[['msisdn','service_band']]\r\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(usage_service, on='msisdn', how=\"left\")\r\n        trend_rfm_recharge_usage_purchase_rfm['service_band']=trend_rfm_recharge_usage_purchase_rfm['service_band'].fillna('no_usage')\r\n       \r\n\r\n\r\n\r\n         #adding usage service\r\n        \r\n        #exporting and adding the kpi segments to db\r\n        print('APA_kpi_segments_analysis file going to export'  )\r\n        print('trend_rfm_recharge_usage_purchase_rfm.columns',trend_rfm_recharge_usage_purchase_rfm.columns)\r\n        print('trend_rfm_recharge_usage_purchase_rfm.dtypes',trend_rfm_recharge_usage_purchase_rfm.dtypes)\r\n        trend_rfm_recharge_usage_purchase_rfm.export_csv(os.path.join(ml_location, dag_run_id,\"APA_kpi_segments_analysis.csv\"))\r\n        print('APA_kpi_segments_analysis file exported '  )\r\n        # for df_chunk in pd.read_csv(os.path.join(ml_location, dag_run_id,\"APA_kpi_segments_analysis.csv\"),\r\n        #                             chunksize=5000):\r\n        #     # insert each chunk into database table\r\n        #     df_chunk['dag_run_id']=str(dag_run_id)\r\n        #     df_chunk.to_sql('APA_kpi_segments_analysis_num', engine, if_exists='append', index=False)\r\n\r\n        \r\n\r\n\r\n       \r\n        # trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.extract()\r\n\r\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.to_pandas_df()\r\n        print('type(trend_rfm_recharge_usage_purchase_rfm)',type(trend_rfm_recharge_usage_purchase_rfm))\r\n        segment_data(trend_rfm_recharge_usage_purchase_rfm, dag_run_id)\r\n\r\n        ic(\"mergeing rechare and trend  and rfm \", trend_rfm_recharge_usage_purchase_rfm['msisdn'].nunique())\r\n\r\n      \r\n\r\n        df = trend_rfm_recharge_usage_purchase_rfm.groupby([\"trend\", 'Segment']).agg({\"msisdn\": \"count\"})\r\n        df.to_csv(os.path.join(ml_location,dag_run_id, \"trend_segment.csv\"),index=False)\r\n        # ------------------------inner join logic  end-------------------------------#\r\n        # df = trend_rfm_recharge_usage_purchase.groupby([\"trend\", 'Segment']).agg({\"msisdn\": \"count\"})\r\n        # df.export_csv(os.path.join(cfg.Config.ml_location, dag_run_id, \"trend_segment.csv\"))\r\n        print(\"done with segmentation\")\r\n        return df\r\n\r\n        # segment_data_with_rfm(trend_rfm_recharge_usage_purchase, recharge_all_months.copy(), dag_run_id)\r\n    except Exception as e:\r\n        print(e)\r\n        \r\n        \r\n    \r\n\r\n\r\n\r\ndef transform(dataframe):\r\n    d=segementation(\"manual__2023-07-10T11:06:51\")\r\n    return d"
              }
            }, {
              "id": "59f3a59d-1812-cf4a-50c4-ffd4bea1d34d",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nimport numpy as np\nimport os\nfrom fastapi import Depends, FastAPI, HTTPException\nimport pathlib\nimport dask.dataframe as dd\nimport datetime\nimport dask.array as da\nfrom pathlib import Path\nfrom icecream import ic\nimport  traceback\nfrom datetime import datetime\ngeneric_dict = {\"numerical_col\": \"Revenue\"}\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n\n\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_feb.csv\",\n            \"m2\": \"purchase_m1_20230206203137.csv\",\n            \"m3\": \"purchase_m2_20230206203137.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n\n\ndef join_rfm(x):\n    return str(int(x['R_Score'])) + str(int(x['F_Score'])) + str(int(x['M_Score']))\n    \n\n\ndef segmentaion_fun1(r, f, m):\n    segments = []\n    for r, f, m in zip(r, f, m):\n        value = int(f\"{r}{f}{m}\")\n\n        if value in [555, 554, 544, 545, 454, 455, 445]:\n            segments.append(\"Champions\")\n        elif value in [543, 444, 435, 355, 354, 345, 344, 335]:\n            segments.append(\"Loyal_Customers\")\n        elif value in [553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323]:\n            segments.append(\"Potential_Loyalist\")\n        elif value in [512, 511, 422, 421, 412, 411, 311]:\n            segments.append(\"Recent_Customers\")\n        elif value in [525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313]:\n            segments.append(\"Promising_Customers\")\n        elif value in [535, 534, 443, 434, 343, 334, 325, 324]:\n            segments.append(\"Customers_needing_Attention\")\n        elif value in [331, 321, 312, 221, 213]:\n            segments.append(\"About_to_Sleep\")\n        elif value in [255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124]:\n            segments.append(\"At_Risk\")\n        elif value in [155, 154, 144, 214, 215, 115, 114, 113]:\n            segments.append(\"Cant_Loose_them\")\n        elif value in [332, 322, 231, 241, 251, 233, 232, 223, 222, 132, 123, 122, 212, 211]:\n            segments.append(\"Hibernating\")\n        else:\n            segments.append(\"Lost\")\n\n    return segments\n    \n\n\n\ndef form_segements(ctm_class):\n    def process_duplicates(li):\n        for i in range(len(li) - 1):\n            print(li[i], i)\n            if i + 1 == len(li) - 1:\n                li[i] = ((li[i - 1] + li[i]) / 2)\n            elif li[i] == li[i + 1]:\n                li[i + 1] = ((li[i + 1] + li[i + 2]) / 2)\n        return li\n\n    # bins_recency = [-1,\n    #                 np.percentile(ctm_class[\"Recency\"], 20),\n    #                 np.percentile(ctm_class[\"Recency\"], 40),\n    #                 np.percentile(ctm_class[\"Recency\"], 60),\n    #                 np.percentile(ctm_class[\"Recency\"], 80),\n    #                 ctm_class[\"Recency\"].max().compute()]\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n    # bins_recency = [-1,17,35,53,71, ctm_class[\"Recency\"].max().compute()]\n\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Recency']=ctm_class1['Recency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Recency').agg({'msisdn':'count'}).reset_index()\n    mean_recency = ctm_class1['Recency'].max().compute()/5\n    print(\"mean_recency\", mean_recency)\n    print(\"mean_recency *2\",mean_recency * 2)\n    print(\"mean_recency *3\",mean_recency * 3)\n    print(\"mean_recency *4\",mean_recency * 4)\n    \n    \n    bins_recency = [-1,\n                    int(mean_recency),\n                    int(mean_recency * 2),\n                    int(mean_recency * 3),\n                    int(mean_recency * 4),\n                    ctm_class[\"Recency\"].max().compute()]\n    \n    print('bins_recency is', bins_recency)\n    \n\n    ctm_class['R_Score'] = ctm_class[\"Recency\"].map_partitions(pd.cut,\n                                                               bins=bins_recency,\n                                                               labels=[5, 4, 3, 2, 1]).astype(\"int\")\n\n    \n\n    # bins_frequency = [-1,\n    #                   np.percentile(ctm_class[\"Frequency\"], 20),\n    #                   np.percentile(ctm_class[\"Frequency\"], 40),\n    #                   np.percentile(ctm_class[\"Frequency\"], 60),\n    #                   np.percentile(ctm_class[\"Frequency\"], 80),\n    #                   ctm_class[\"Frequency\"].max().compute()]\n\n    # bins_frequency.sort()\n    # bins_frequency = process_duplicates(bins_frequency)\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Frequency']=ctm_class1['Frequency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Frequency').agg({'msisdn':'count'}).reset_index()\n    mean_frequency = ctm_class1['Frequency'].max().compute()/5\n    print(\"mean_frequency\", mean_frequency)\n    print(\"mean_frequency *2\",mean_frequency * 2)\n    print(\"mean_frequency *3\",mean_frequency * 3)\n    print(\"mean_frequency *4\",mean_frequency * 4)\n\n\n    bins_frequency = [-1,\n                    int(mean_frequency),\n                    int(mean_frequency * 2),\n                    int(mean_frequency * 3),\n                    int(mean_frequency * 4),\n                    ctm_class[\"Frequency\"].max().compute()]\n    \n    print('bins_frequency is', bins_frequency)\n    ctm_class['F_Score'] = ctm_class[\"Frequency\"].map_partitions(pd.cut,\n                                                                 bins=bins_frequency,\n                                                                 labels=[1, 2, 3, 4, 5]).astype(\"int\")\n\n    \n    \n    # bins_revenue = [-1,\n    #                 np.percentile(ctm_class[\"Revenue\"], 20),\n    #                 np.percentile(ctm_class[\"Revenue\"], 40),\n    #                 np.percentile(ctm_class[\"Revenue\"], 60),\n    #                 np.percentile(ctm_class[\"Revenue\"], 80),\n    #                 ctm_class[\"Revenue\"].max().compute()]\n    # bins_revenue.sort()\n    # bins_revenue = process_duplicates(bins_revenue)\n\n    #bins_revenue = [-1,189,380,571,762, ctm_class[\"Revenue\"].max().compute()]\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Revenue'] = ctm_class1['Revenue'].map_partitions(lambda series: series.apply(np.ceil))\n    ctm_class1['Revenue']=ctm_class1['Revenue'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Revenue').agg({'msisdn':'count'}).reset_index()\n    mean_revenue = ctm_class1['Revenue'].max().compute()/5\n    \n    print(\"mean_revenue\", mean_revenue)\n    print(\"mean_revenue *2\",mean_revenue * 2)\n    print(\"mean_revenue *3\",mean_revenue * 3)\n    print(\"mean_revenue *4\",mean_revenue * 4)\n    \n    bins_revenue = [-1,\n                    int(mean_revenue),\n                    int(mean_revenue * 2),\n                    int(mean_revenue * 3),\n                    int(mean_revenue * 4),\n                    ctm_class[\"Revenue\"].max().compute()]\n    \n    print('bins_revenue is', bins_revenue)\n\n    ctm_class['M_Score'] = ctm_class[\"Revenue\"].map_partitions(pd.cut,\n                                                               bins=bins_revenue,\n                                                               labels=[1, 2, 3, 4, 5]).astype(\"int\")\n    print(\"done with scoring\")\n    # Form RFM segment\n\n    ctm_class['RFM_Segment'] = ctm_class.apply(join_rfm, axis=1)\n    ctm_class['RFM_Segment'] = ctm_class['RFM_Segment'].astype(int)\n    print(\"formed rfm segement \")\n    ctm_class['R_Score'] = ctm_class['R_Score'].astype(int)\n    ctm_class['F_Score'] = ctm_class['F_Score'].astype(int)\n    ctm_class['M_Score'] = ctm_class['M_Score'].astype(int)\n    print(\"computing rfm\")\n    r = ctm_class['R_Score'].values.compute()\n    f = ctm_class['F_Score'].values.compute()\n    m = ctm_class['M_Score'].values.compute()\n    seg = segmentaion_fun1(r, f, m)\n    chunks = ctm_class.map_partitions(lambda x: len(x)).compute().to_numpy()\n    myarray = da.from_array(seg, chunks=tuple(chunks))\n    ctm_class['Segment'] = myarray\n    return ctm_class\n    \n    \n\ndef otliner_removal(df,col, per=0.97):\n    q = df[col].quantile(per)\n    print(f\"col is {col} and q is {q}\")\n    print(\"the length brfore is\", len(df))\n    outliers = df[df[col] > q]\n    df1 = df[~df['msisdn'].isin(outliers['msisdn'].compute().values)]\n    print(\"the length after is\", len(df1))\n    return df1\n    \n    \ndef get_banding_confitions():\n        return {\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\n                               6: \"high_high\"}\n        }\n        \n        \n\n\ndef arpu_trend(data):\n    months = usage_no_months\n    temp_df = None\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    #for daily summarised\n\n    usage={}\n\n    for month in usage_no_months:\n        usage[month] = dd.read_csv(\n            os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n        \n    usage_3m = dd.concat(list(usage.values()))\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    temp_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n\n    print('len(temp_df) is',len(temp_df) )\n\n    #for daily summarised\n\n\n\n    for month in months:\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n        # total_voice_rev = f.Features.CUSTOMER_VOICE_REVENUE[2]\n        needed_col = [MSISDN_COL_NAME, total_revenue]\n        df = data.get(month)[needed_col]\n        df['tot_rev'] = df[total_revenue]\n        df = df.drop(total_revenue, axis=1)\n        df = df.fillna(0)\n        #df = otliner_removal(df)\n\n        df['rev_segment'] = df.tot_rev.apply(put_revenue_segement)\n        df = df.rename(columns={\"rev_segment\": \"rev_segment_\" + month})\n        df['tot_rev' + month] = df['tot_rev']\n\n        df = df.drop('tot_rev', axis=1)\n        \n\n        if temp_df is None:\n            temp_df = df\n        else:\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n            temp_df[\"rev_segment_\" + month] = temp_df[\"rev_segment_\" + month].fillna(\"Zero\")\n            temp_df[\"tot_rev\" + month] = temp_df[\"tot_rev\" + month].fillna(0)\n\n    temp_df['tot_sum'] = temp_df['tot_revm1'] + temp_df['tot_revm2'] + temp_df['tot_revm3']\n    temp_df_non = temp_df\n    print(\"len of temp_df_non  after filtering is is  \", len(temp_df_non))\n    replace_map = get_banding_confitions().get(\"common\")\n\n    df1 = temp_df_non.replace(replace_map)\n    print(type(temp_df_non))\n\n    #to convert rev_segment_cols to int \n    rev_segment_cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    df1[rev_segment_cols] = df1[rev_segment_cols].astype('int32')\n    #to convert rev_segment_cols to int \n\n    cols = df1.columns[df1.columns.str.startswith('tot_revm')].tolist()\n\n    #to find trend using segment\n    #cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    #to find trend using segment\n\n    for col in cols:\n        df1[col] = df1[col].astype('int')\n        df1[col] = df1[col]\n\n    output = df1[cols].map_partitions(lambda part: part.apply(lambda x: pandas_wrapper(x), axis=1), meta=tuple)\n    output1 = output.to_frame(name='trend')\n    op = dd.concat([df1, output1], axis=1)\n\n    return op\n    \n    \ndef perform_rfm_on_daily_summ(df_data_rfm, period=95):\n\n    ic(\"inside perform_rfm in daily summarised\")\n\n    df_data_rfm.purchase_date = dd.to_datetime(df_data_rfm.purchase_date)\n    df_data_rfm = df_data_rfm.fillna(0)\n    msisdn_name = MSISDN_COL_NAME\n    # current behaviour data\n    min_date = df_data_rfm['purchase_date'].min().compute()\n    cur_beh_data_rfm = df_data_rfm[(df_data_rfm.purchase_date <= pd.Timestamp(min_date) + pd.Timedelta(days=period))\n                                   & (df_data_rfm.purchase_date >= pd.Timestamp(min_date))].reset_index(drop=True)\n    # Get the maximum purchase date of each customer and create a dataframe with it together with the customer's id.\n    ctm_max_purchase = cur_beh_data_rfm.groupby(msisdn_name).purchase_date.max().reset_index()\n    ctm_max_purchase.columns = [msisdn_name, 'MaxPurchaseDate']\n\n    # Find the recency of each customer in days\n    ctm_max_purchase['Recency'] = (\n            ctm_max_purchase['MaxPurchaseDate'].max() - ctm_max_purchase['MaxPurchaseDate']).dt.days\n    ctm_max_purchase = ctm_max_purchase.drop(columns=['MaxPurchaseDate'])\n    print(\"done with recency \")\n    # frequency\n    ctm_frequency = cur_beh_data_rfm.groupby(msisdn_name).size().reset_index()\n    ctm_frequency.columns = [msisdn_name, 'Frequency']\n    print(\"done with frequency \")\n    cur_beh_data_rfm['Revenue'] = cur_beh_data_rfm[DAILY_TRANSACTION_PRICE_COL_NAME]\n    ctm_revenue = cur_beh_data_rfm.groupby('msisdn').Revenue.sum().reset_index()\n    print(\"done with monitory \")\n    rfm_data_base = dd.concat([ctm_max_purchase, ctm_revenue, ctm_frequency], axis=1)\n    rfm_data_base = rfm_data_base.loc[:, ~rfm_data_base.columns.duplicated()]\n\n    #for btc purpose start\n    #rfm_data_base=otliner_removal(rfm_data_base,'Frequency')\n    rfm_data_base=otliner_removal(rfm_data_base,'Revenue')\n    \n    #for btc purpose end \n\n\n    return rfm_data_base\n    \n\n\ndef put_revenue_segement(x):\n    y = 'Zero'\n    if x == 0.0000:\n        y = 'Zero'\n    elif (x > 0) & (x <= 16):\n        y = 'very_low'\n    elif (x > 16) & (x <= 36):\n        y = 'low'\n    elif (x > 36) & (x <= 61):\n        y = 'medium'\n    elif (x > 61) & (x <= 175):\n        y = 'high'\n    else:\n        y = 'high_high'\n    return y\n    \n\n\ndef trends(numbers):\n    if len(numbers) != 3:\n        return \"Invalid input\"\n\n    if numbers[0] == numbers[1] == numbers[2]:\n        return \"flat\"\n    elif numbers[0] > numbers[1] > numbers[2]:\n        return \"Downtrend\"\n    elif numbers[0] < numbers[1] < numbers[2]:\n        return \"Uptrend\"\n    else:\n        return \"Zigzag\"\n        \n\n\ndef pandas_wrapper(row):\n    return trends([row[2], row[1], row[0]])\n    \n\n\ndef rfm_process_quantile_method_daily_summerized(dag_run_id):\n    try:\n        msisdn_name = MSISDN_COL_NAME\n        \n        \n        file_name_dict = get_file_names()\n        dtype_purchase = DAILY_TRANSACTION_DTYPES\n        data = {}\n        print('purchase is going to  read')\n        for month in purchase_no_months:\n            data[month] = dd.read_csv(\n                os.path.join(purchase_location, file_name_dict.get(\"daily_summerized\").get(month)),\n                dtype=dtype_purchase)\n        print('purchase readed')\n        df_data = dd.concat(list(data.values()))\n        \n        #for daily summarised\n\n        usage={}\n\n        for month in usage_no_months:\n            usage[month] = dd.read_csv(\n                os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n            \n        #usage_3m = dd.concat(list(usage.values()))\n        trend_df = arpu_trend(usage)\n        trend_df = trend_df.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n        df_data = df_data[df_data[msisdn_name].isin(trend_df[msisdn_name].compute().unique())]\n        print(\"usage filtered---\")\n        \n        # profile\n        dtype = {'account_type': 'float64',\n                 'current_state': 'float64',\n                 'no_of_balances': 'float64',\n                 'primary_offer_id': 'float64',\n                 'subscr_no': 'float64',\n                 'payment_type':'object'}\n        profile = dd.read_csv(os.path.join(purchase_location, file_name_dict.get(\"profile\")), dtype=dtype)\n        profile[PROFILE_DATE_ENTER_ACTIVE] = dd.to_datetime(profile[PROFILE_DATE_ENTER_ACTIVE],\n                                                                       errors='coerce')\n\n        date_threshold = datetime.strptime('2023-02-12', '%Y-%m-%d')\n\n        profile['aon'] = (date_threshold - profile[PROFILE_DATE_ENTER_ACTIVE]).dt.days\n\n        \n        profile = profile[profile['aon'] >=90]\n        profile = profile[profile['current_state'] ==2]\n        profile = profile[profile['payment_type'] =='Prepaid']\n        print('df.columns',df_data.columns)\n        # df_data = df_data.merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n        #                                                               how='inner')\n        df_data=df_data[df_data['msisdn'].isin(profile['msisdn'].compute().unique())]\n        df_data=df_data[['msisdn', 'fct_dt', 'total_revenue', 'avg_total_revenue']]\n\n\n        #for daily summarised\n\n        df_data = df_data.fillna(0 )\n        df_data = df_data.rename(columns={DAILY_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        ctm_class = perform_rfm_on_daily_summ(df_data, period=95)#rfm using daily summarised\n        path = os.path.join(ml_location,dag_run_id, \"rfm_before_segementation\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n        ctm_class = form_segements(ctm_class)\n\n        print(\"done with rfm outputing the file ongoing\")\n        path = os.path.join(ml_location,dag_run_id, \"rfm\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(path)\n        ctm_class.to_parquet(path)\n\n        ic(\"rfm segement value counts\", ctm_class['Segment'].value_counts().compute())\n        print(\"done with rfm outputing the file done \")\n\n    except Exception as e:\n        print(e)\n        \n\ndef transform(dataframe):\n    rfm_process_quantile_method_daily_summerized(\"manual__2023-07-10T11:06:51\")\n    return dataframe"
              }
            }, {
              "id": "425b2c9d-f25d-9107-49d7-ef723ce53449",
              "operation": {
                "id": "f94b04d7-ec34-42f7-8100-93fe235c89f8",
                "name": "Source"
              },
              "parameters": {

              }
            }, {
              "id": "b28ed4f1-4d37-610f-ed65-7a4a5d27b920",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n    \n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n    \n    \ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_march_20230809132937.csv\",\n            \"m2\": \"recharge_feb_20230809132937.csv\",\n            \"m3\": \"recharge_jan_20230809132937.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n\n\nclass RechargeBanding(object):\n    def __init__(self, data):\n        self.data = data\n        # self.categorize()\n\n    def count_category(self, x, m):\n\n        if x == 1:\n            resp = 'b)1'\n        elif 1 < x <= 4:\n            resp = 'c)1-4'\n        elif 4 < x <= 10:\n            resp = 'd)4-10'\n        elif 10 < x <= 30:\n            resp = 'e)10-30'\n        elif x > 30:\n            resp = 'f)30 above'\n        else:\n            resp = 'a)zero'\n        return m + \"_\" + resp\n\n    def categorize(self):\n\n        months = recharge_no_months\n        recharge_count_col_name = RECHARGE_COUNT_COL_NAME\n        msisdn_name = MSISDN_COL_NAME\n        temp_df = None\n        for month in months:\n            needed_col = RECHARGE_NEEDED_COL\n            dataset = self.data.get(month)[needed_col]\n            dataset = dataset.groupby([msisdn_name]).agg({recharge_count_col_name: 'sum'})\n            dataset['count_pattern'] = dataset[recharge_count_col_name].apply(self.count_category, args=(month,))\n            dataset['recharge_count_pattern_' + month] = dataset['count_pattern']\n            dataset['recharge_' + recharge_count_col_name + month] = dataset[recharge_count_col_name]\n            dataset = dataset.drop(recharge_count_col_name, axis=1)\n            dataset = dataset.drop('count_pattern', axis=1)\n            if temp_df is None:\n                temp_df = dataset\n            else:\n                temp_df = temp_df.merge(dataset, on=msisdn_name, how=\"left\")\n                temp_df[\"recharge_count_pattern_\" + month] = temp_df[\"recharge_count_pattern_\" + month].fillna(month+\"_a)zero\")\n                temp_df['recharge_' +recharge_count_col_name + month]=temp_df['recharge_' +recharge_count_col_name + month].fillna(0)\n\n        return temp_df.reset_index()\n        \n        \n\n\n\ndef recharge_process(dag_run_id):\n    try:\n        file_name_dict = get_file_names()\n        print(\"rechare preprocess  ongoing \")\n        data = {}\n        dtype_recharge = RECHARGE_DTYPES\n        for month in recharge_no_months:\n            data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"recharge\").get(month)),\n                                      dtype= RECHARGE_DTYPES)\n\n        # ic(\"the length of m1 in recharge \", len(data['m1']))\n        # ic(\"the length of m2 in recharge \", len(data['m2']))\n        # ic(\"the length of m3 in recharge \", len(data['m3']))\n        #\n        # ic(\"the length of unique msisdn m3 in recharge \", data['m1']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in recharge \", data['m2']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in recharge \", data['m3']['msisdn'].nunique().compute())\n        df_data = dd.concat(list(data.values()))\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        path_recharge = os.path.join(ml_location, dag_run_id, \"recharge_all_months\")\n        Path(path_recharge).mkdir(parents=True, exist_ok=True)\n        df_data.to_parquet(path_recharge)\n        rb = RechargeBanding(data)\n        print(\"recharge categorizing ongoing \")\n        df = rb.categorize()\n        print(\"recharge categorizing done \")\n        path = os.path.join(ml_location, dag_run_id, \"recharge_band\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(\"recharge categorizing  to file op ongoing \")\n        df.to_parquet(path)\n        print(\"recharge categorizing  to file op done  \")\n        return df.compute()\n    except Exception as e:\n        print(e)\n        \n        \n\n\ndef transform(dataframe):\n    df = recharge_process(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }, {
              "id": "d167bcbc-3263-32cc-0d23-251d79ec464c",
              "operation": {
                "id": "e652238f-7415-4da6-95c6-ee33808561b2",
                "name": "Sink"
              },
              "parameters": {

              }
            }, {
              "id": "72233ecf-f3a6-49f7-8e53-58404a070956",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n    \n    \ndef status_process(dag_run_id):\n    try:\n        print(\" inside status_process\")\n        file_name_dict = get_file_names()\n        trend_path = os.path.join(ml_location,dag_run_id,  \"trend\")\n        trend_filtered_path = os.path.join(ml_location, dag_run_id,  \"trend_filtered\")\n        trend_profie_active_inactive_trend_eda_path= os.path.join(ml_location,dag_run_id,   \"trend_profie_active_inactive_trend_eda\")\n        rfm_path = os.path.join(ml_location, dag_run_id, \"rfm\")\n        print(\"reading perquet\")\n        trend = dd.read_parquet(trend_path)\n        print(\"reading perquet cmpt\")\n\n        print('len of trend is ',len(trend))\n        trend_filter = trend.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n        print('len of trend_filter after filter either 3 month total rev grater that 0  is ',len(trend_filter))\n\n\n        # profile\n        dtype = {'account_type': 'float64',\n                 'current_state': 'float64',\n                 'no_of_balances': 'float64',\n                 'primary_offer_id': 'float64',\n                 'subscr_no': 'float64',\n                 'payment_type':'object'}\n        profile = dd.read_csv(os.path.join(purchase_location, file_name_dict.get(\"profile\")), dtype=dtype)\n        profile[PROFILE_DATE_ENTER_ACTIVE] = dd.to_datetime(profile[PROFILE_DATE_ENTER_ACTIVE],\n                                                                       errors='coerce')\n\n        date_threshold = datetime.strptime('2023-07-30', '%Y-%m-%d')\n\n        profile['aon'] = (date_threshold - profile[PROFILE_DATE_ENTER_ACTIVE]).dt.days\n        # trend = trend.merge(trend_filter, on='msisdn', how='left', indicator=True)\n        # trend['active_inactive'] = trend['_merge'].apply(lambda x: 'active' if x == 'both' else 'inactive',\n        #                                                  meta=('str'))\n        # trend = trend.drop(columns=['_merge'])\n\n        # trend_active_aon = trend[['msisdn', 'active_inactive']].merge(profile[['msisdn', 'aon']], on='msisdn',\n        #                                                               how='left')\n        # trend_active_aon = trend_active_aon.fillna(0)\n\n        # print(' head is going to add')\n\n        # trend_active_aon1 = trend_active_aon.head(0)\n        # print('trend_active_aon1 is' ,trend_active_aon1 )\n        # print('type of trend_active_aon1 is' ,type(trend_active_aon1 ))\n        # #trend_active_aon1=trend_active_aon1.compute()\n        # print('trend_active_aon1 is' ,trend_active_aon1 )\n        # trend_active_aon['dag_run_id']=str(dag_run_id)\n\n        # trend_active_aon.head(0).to_sql(\"trend_active_aon\", engine, if_exists=\"append\", index=False)\n        # print(' head added')\n        # @delayed\n        # def insert_partition(partition):\n        #     partition.to_sql(\"trend_active_aon\", engine, if_exists=\"append\", index=False)\n\n        # insertions = [insert_partition(partition) for partition in trend_active_aon.to_delayed()]\n        # dask.compute(*insertions)\n        print(' testing 1  ')\n\n\n        trend_active_aon_f = trend_filter.merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n                                                                      how='inner')\n\n        trend_active_aon_f = trend_active_aon_f.fillna(0)\n        trend_active_aon_f = trend_active_aon_f[trend_active_aon_f['aon'] >=90]\n        trend_active_aon_f = trend_active_aon_f[trend_active_aon_f['current_state'] ==2]\n        trend_active_aon_f = trend_active_aon_f[trend_active_aon_f['payment_type'] =='Prepaid']\n\n        print('len of trend_active_aon_f after aon,current_state and payment_type filter is ',len(trend_active_aon_f))\n        \n\n        #trend_active_aon_f_pd = trend_active_aon_f.to_pandas_df()\n\n        # create a dask dataframe from the pandas dataframe\n        #trend_active_aon_f_dd = trend_active_aon_f.compute()\n\n        profile = profile.compute()\n        trend=trend.compute()\n\n        print(' testing 2 ')\n\n        trend_profie = trend[['msisdn','trend']].merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n                                                                      how='left')\n        \n        trend_profie[[ 'aon','current_state']]=trend_profie[[ 'aon','current_state']].fillna(0)\n        trend_profie['payment_type']=trend_profie['payment_type'].fillna('nil')\n\n\n\n       \n\n        #rend_active_aon_f_msisdn=trend_active_aon_f['msisdn'].tolist()\n\n        trend_profie_active = trend_profie[(trend_profie['payment_type'] == 'Prepaid') & (trend_profie['current_state'] == 2)]\n        trend_profie_active= trend_profie_active[trend_profie_active['msisdn'].isin(trend_filter['msisdn'].compute())]\n        trend_profie_inactive = trend_profie[~trend_profie['msisdn'].isin(trend_profie_active['msisdn'])]\n\n        trend_profie_active['active_inactive'] = 'active'\n        \n        trend_profie_inactive['active_inactive'] = 'inactive'\n   \n        trend_profie_active = dd.from_pandas(trend_profie_active, npartitions=2)\n        trend_profie_inactive = dd.from_pandas(trend_profie_inactive, npartitions=2)\n\n        trend_profie_active_inactive = dd.concat([trend_profie_active,trend_profie_inactive])\n        \n       \n        trend_profie_active_inactive['dag_run_id']=str(dag_run_id)\n\n\n       \n        \n\n        print('type(trend_profie_active_inactive',type(trend_profie_active_inactive))\n  \n#         print(' head added')\n#         @delayed\n#         def insert_partition(partition):\n#             partition.to_sql(\"APA_trend_profie_active_inactive\", engine, if_exists=\"append\", index=False)\n\n#         insertions = [insert_partition(partition) for partition in trend_profie_active_inactive.to_delayed()]\n#         dask.compute(*insertions)\n#         print(' trend_profie_active_inactive added to sql table ')\n\n        rfm = dd.read_parquet(rfm_path)\n\n        trend_profie_active_inactive_trend_rfm = trend_profie_active_inactive.merge(rfm[['msisdn', 'Segment']], on='msisdn',\n                                                                      how='left')\n        trend_profie_active_inactive_trend_rfm['Segment']=trend_profie_active_inactive_trend_rfm['Segment'].fillna('nil')\n\n        trend_profie_active_inactive_trend_rfm.to_parquet(trend_profie_active_inactive_trend_eda_path)\n        \n        trend_active_aon_f.to_parquet(trend_filtered_path)\n\n        # ic(\"the length of trend df \", len(trend))\n        # ic(\"the unique msisdn in trend df \", trend['msisdn'].nunique().compute())\n\n        # ic(\"the length of trend df  after all 3 months active \", len(trend_filter))\n        # ic(\"the unique msisdn in trend df  all 3 months active \", trend_filter['msisdn'].nunique().compute())\n        return trend_active_aon_f.compute()\n        pass\n    except Exception as e:\n        print(e)\n        \n\ndef transform(dataframe):\n    df = status_process(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }, {
              "id": "b75153f5-e16b-7e96-e9a4-77f36e4d2117",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_march_20230809132937.csv\",\n            \"m2\": \"recharge_feb_20230809132937.csv\",\n            \"m3\": \"recharge_jan_20230809132937.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n\n\nclass purchaseBanding(object):\n    def __init__(self, data):\n        self.data = data\n        # self.categorize()\n\n    def count_category(self, x, m):\n\n        if x == 1:\n            resp = 'b)1'\n        elif 1 < x <= 4:\n            resp = 'c)1-4'\n        elif 4 < x <= 10:\n            resp = 'd)4-10'\n        elif 10 < x <= 30:\n            resp = 'e)10-30'\n        elif x > 30:\n            resp = 'f)30 above'\n        else:\n            resp = 'a)zero'\n        return m + \"_\" + resp\n\n    def categorize(self):\n\n        months = purchase_no_months_seg\n        purchase_count_col_name = TRANSACTION_COUNT_COL_NAME\n        msisdn_name = MSISDN_COL_NAME\n        temp_df = None\n        for month in months:\n            needed_col = TRANSACTION_NEEDED_COL\n            dataset = self.data.get(month)[needed_col]\n            dataset = dataset.groupby([msisdn_name]).agg({purchase_count_col_name: 'sum'})\n            dataset['count_pattern'] = dataset[purchase_count_col_name].apply(self.count_category, args=(month,))\n            dataset['purchase_count_pattern_' + month] = dataset['count_pattern']\n            dataset['purchase_'+purchase_count_col_name + month] = dataset[purchase_count_col_name]\n            dataset = dataset.drop(purchase_count_col_name, axis=1)\n            dataset = dataset.drop('count_pattern', axis=1)\n            if temp_df is None:\n                temp_df = dataset\n            else:\n                temp_df = temp_df.merge(dataset, on=msisdn_name, how=\"left\")\n                temp_df[\"purchase_count_pattern_\" + month] = temp_df[\"purchase_count_pattern_\" + month].fillna(month+\"_a)zero\")\n                temp_df['purchase_'+purchase_count_col_name + month]=temp_df['purchase_'+purchase_count_col_name + month].fillna(0)\n        return temp_df.reset_index()\n        \n\n\n\ndef purchase_process(dag_run_id):\n    try:\n        file_name_dict = get_file_names()\n        print(\"purchase preprocess  ongoing \")\n        data = {}\n        dtype_purchase = RECHARGE_TRANSACTION_DTYPES\n        for month in purchase_no_months_seg:\n            data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"purchase\").get(month)),\n                                      dtype= RECHARGE_TRANSACTION_DTYPES)\n\n        df_data = dd.concat(list(data.values()))\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        path_pur = os.path.join(ml_location, dag_run_id,\"purchase_all_months\")\n        Path(path_pur).mkdir(parents=True, exist_ok=True)\n        df_data.to_parquet(path_pur)\n        # ic(\"the length of m1 in purchase \", len(data['m1']))\n        # ic(\"the length of m2 in purchase \", len(data['m2']))\n        # ic(\"the length of m3 in purchase \", len(data['m3']))\n        #\n        # ic(\"the length of unique msisdn m3 in purchase \", data['m1']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in purchase \", data['m2']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in purchase \", data['m3']['msisdn'].nunique().compute())\n        rb = purchaseBanding(data)\n        print(\"purchase categorizing ongoing \")\n        df = rb.categorize()\n        print(\"purchase categorizing done \")\n        path = os.path.join(ml_location, dag_run_id,\"purchase_band\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        print(\"purchase categorizing  to file op ongoing \")\n        df.to_parquet(path)\n        print(\"purchase categorizing  to file op done  \")\n        return df.compute()\n    except Exception as e:\n        print(e)\n        \n\n\n\n\n\ndef transform(dataframe):\n    df  = purchase_process(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }, {
              "id": "d0377de7-a1e4-02f5-56fb-1317a2dc0f8f",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_feb.csv\",\n            \"m2\": \"purchase_m1_20230206203137.csv\",\n            \"m3\": \"purchase_m2_20230206203137.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n    \nclass UsageCategory(object):\n    def __init__(self, data):\n        self.data = data\n        self.current_iteration_month = None\n        # self.categorize()\n\n    def voice_band(self, x, m):\n        \n        if x == 0.0000:\n            y = 'a)Zero'\n        elif (x > 0) & (x <= 8):\n            y = 'b)<=8'\n        elif (x > 8) & (x <= 20):\n            y = 'c)8 - 20'\n        elif (x > 20) & (x <= 40):\n            y = 'd)20 - 40'\n        elif (x > 40) & (x <= 100):\n            y = 'e)40 - 100'\n        elif (x > 100) & (x <= 200):\n            y = 'f)100 - 200'\n        elif (x > 200) & (x <= 500):\n            y = 'g)200 - 500'\n        elif (x > 500) & (x <= 1000):\n            y = 'h)500 - 1000'\n        else:\n            y = 'i)1000+'\n        return m + \"_\" + y\n\n    def data_band(self, x, m):\n        \n        if x == 0.0000:\n            y = 'a)Zero'\n        elif (x > 0) & (x <= 50):\n            y = 'b)0-50 MB'\n        elif (x > 50) & (x <= 100):\n            y = 'c)50-100 MB'\n        elif (x > 100) & (x <= 250):\n            y = 'd)100-250 MB'\n        elif (x > 250) & (x <= 512):\n            y = 'e)250-512 MB'\n        elif (x > 512) & (x <= 1536):\n            y = 'f)512 MB-1.5 GB'\n        elif (x > 1536) & (x <= 3072):\n            y = 'g)1.5-3 GB'\n        else:\n            y = 'h)3 GB +'\n        return m + \"_\" + y\n\n    def categorize(self):\n        months = ['m1', 'm2', 'm3']\n        temp_df = None\n        for month in months:\n            self.current_iteration_month = month\n\n            needed_col = CUSTOMER_CATEG_NEEDED_COL\n            dataset = self.data.get(month)[needed_col]\n            dataset = dataset.fillna(0)\n            # 0 index for inbundle 1 index for outbundled  2 index for total\n            voice_col = CUSTOMER_VOICE_COL_NAME[0]\n            data_col = CUSTOMER_DATA_COL_NAME[0]\n            dataset[month + '_voice_band'] = dataset[voice_col].apply(self.voice_band, args=(month,))\n            dataset[month + '_data_band'] = dataset[data_col].apply(self.data_band, args=(month,))\n            dataset[month +'_'+ voice_col] = dataset[voice_col]\n            dataset[month +'_'+ data_col] = dataset[data_col]\n            dataset = dataset.drop(columns=[voice_col, data_col])\n            if temp_df is None:\n                temp_df = dataset\n            else:\n                temp_df = temp_df.merge(dataset, on='msisdn', how=\"left\")\n                temp_df[month + '_voice_band'] = temp_df[month + '_voice_band'].fillna(month + \"_\"'a)Zero')\n                temp_df[month + '_data_band'] = temp_df[month + '_data_band'].fillna(month + \"_\"'a)Zero')\n                dataset[month +'_'+ voice_col] = dataset[month +'_'+ voice_col].fillna(0)\n                dataset[month +'_'+ data_col] = dataset[month +'_'+ data_col].fillna(0)\n\n        return temp_df\n        \n        \n\n\ndef arpu_trend(data):\n    months = usage_no_months\n    temp_df = None\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    #for daily summarised\n\n    usage={}\n\n    for month in usage_no_months:\n        usage[month] = dd.read_csv(\n            os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n        \n    usage_3m = dd.concat(list(usage.values()))\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    temp_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n\n    print('len(temp_df) is',len(temp_df) )\n\n    #for daily summarised\n\n\n\n    for month in months:\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n        # total_voice_rev = CUSTOMER_VOICE_REVENUE[2]\n        needed_col = [MSISDN_COL_NAME, total_revenue]\n        df = data.get(month)[needed_col]\n        df['tot_rev'] = df[total_revenue]\n        df = df.drop(total_revenue, axis=1)\n        df = df.fillna(0)\n        #df = otliner_removal(df)\n\n        df['rev_segment'] = df.tot_rev.apply(put_revenue_segement)\n        df = df.rename(columns={\"rev_segment\": \"rev_segment_\" + month})\n        df['tot_rev' + month] = df['tot_rev']\n\n        df = df.drop('tot_rev', axis=1)\n        \n\n        if temp_df is None:\n            temp_df = df\n        else:\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n            temp_df[\"rev_segment_\" + month] = temp_df[\"rev_segment_\" + month].fillna(\"Zero\")\n            temp_df[\"tot_rev\" + month] = temp_df[\"tot_rev\" + month].fillna(0)\n\n    temp_df['tot_sum'] = temp_df['tot_revm1'] + temp_df['tot_revm2'] + temp_df['tot_revm3']\n    temp_df_non = temp_df\n    print(\"len of temp_df_non  after filtering is is  \", len(temp_df_non))\n    replace_map = get_banding_confitions().get(\"common\")\n\n    df1 = temp_df_non.replace(replace_map)\n    print(type(temp_df_non))\n\n    #to convert rev_segment_cols to int \n    rev_segment_cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    df1[rev_segment_cols] = df1[rev_segment_cols].astype('int32')\n    #to convert rev_segment_cols to int \n\n    cols = df1.columns[df1.columns.str.startswith('tot_revm')].tolist()\n\n    #to find trend using segment\n    #cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    #to find trend using segment\n\n    for col in cols:\n        df1[col] = df1[col].astype('int')\n        df1[col] = df1[col]\n\n    output = df1[cols].map_partitions(lambda part: part.apply(lambda x: pandas_wrapper(x), axis=1), meta=tuple)\n    output1 = output.to_frame(name='trend')\n    op = dd.concat([df1, output1], axis=1)\n\n    return op\n    \n    \n\n\ndef get_banding_confitions():\n        return {\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\n                               6: \"high_high\"}\n        }\n        \n        \n        \n\n\n\n\ndef put_revenue_segement(x):\n    y = 'Zero'\n    if x == 0.0000:\n        y = 'Zero'\n    elif (x > 0) & (x <= 16):\n        y = 'very_low'\n    elif (x > 16) & (x <= 36):\n        y = 'low'\n    elif (x > 36) & (x <= 61):\n        y = 'medium'\n    elif (x > 61) & (x <= 175):\n        y = 'high'\n    else:\n        y = 'high_high'\n    return y\n    \n    \ndef trends(numbers):\n    if len(numbers) != 3:\n        return \"Invalid input\"\n\n    if numbers[0] == numbers[1] == numbers[2]:\n        return \"flat\"\n    elif numbers[0] > numbers[1] > numbers[2]:\n        return \"Downtrend\"\n    elif numbers[0] < numbers[1] < numbers[2]:\n        return \"Uptrend\"\n    else:\n        return \"Zigzag\"\n        \n        \n\n\ndef pandas_wrapper(row):\n    return trends([row[2], row[1], row[0]])\n    \n    \n\ndef usage_process(dag_run_id):\n    try:\n\n\n\n        file_name_dict = get_file_names()\n        print(\"finding trend ongoing \")\n        data = {}\n        for month in usage_no_months:\n            data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\n                                      dtype= CUSTOMER_DTYPES)\n\n        # ic(\"the length of m1 in usage \", len(data['m1']))\n        # ic(\"the length of m2 in usage \", len(data['m2']))\n        # ic(\"the length of m3 in usage \", len(data['m3']))\n        # ic(\"the length of unique msisdn m3 in usage \", data['m1']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in usage \", data['m2'\n        #                                                     '']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in usage \", data['m3']['msisdn'].nunique().compute())\n\n        trend_df = arpu_trend(data)\n\n        # ic(\"the trend value counts is \", trend_df['trend'].value_counts().compute())\n\n        path = os.path.join(ml_location,dag_run_id, \"trend\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        #trend_df = trend_df.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n\n        trend_df.to_parquet(path)\n        uc = UsageCategory(data)\n        path = os.path.join(ml_location, dag_run_id, \"usage_bands\")\n        print(\"finding usage band ongoing \")\n        df = uc.categorize()\n\n        #to fill null values if there is in data and voice band \n        # df['m1_voice_band'] = df['m1_voice_band'].fill('m1_Zero')\n        # df['m1_data_band '] = df['m1_data_band'].fill('m1_Zero')\n        # df['m2_voice_band'] = df['m2_voice_band'].fill('m2_Zero')\n        # df['m2_data_band '] = df['m2_data_band'].fill('m2_Zero')\n        # df['m3_voice_band'] = df['m3_voice_band'].fill('m3_Zero')\n        # df['m3_data_band '] = df['m3_data_band'].fill('m3_Zero')\n\n        #to fill null values if there is in data and voice band \n\n\n\n\n        df.to_parquet(path)\n        print(\"finding usage band done \")\n        return df.compute()\n    except Exception as e:\n        print(e)\n        \n        \n\n\n\n\n\n\n\ndef transform(dataframe):\n    df = usage_process(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }],
            "connections": [{
              "from": {
                "nodeId": "b28ed4f1-4d37-610f-ed65-7a4a5d27b920",
                "portIndex": 0
              },
              "to": {
                "nodeId": "b75153f5-e16b-7e96-e9a4-77f36e4d2117",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "b75153f5-e16b-7e96-e9a4-77f36e4d2117",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e56ff646-c454-5603-3eb2-b2c0ed91197a",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "d0377de7-a1e4-02f5-56fb-1317a2dc0f8f",
                "portIndex": 0
              },
              "to": {
                "nodeId": "72233ecf-f3a6-49f7-8e53-58404a070956",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "ea4eeaea-c74d-ef6b-437a-7c285e6ff58b",
                "portIndex": 0
              },
              "to": {
                "nodeId": "d0377de7-a1e4-02f5-56fb-1317a2dc0f8f",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "72233ecf-f3a6-49f7-8e53-58404a070956",
                "portIndex": 0
              },
              "to": {
                "nodeId": "b28ed4f1-4d37-610f-ed65-7a4a5d27b920",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "425b2c9d-f25d-9107-49d7-ef723ce53449",
                "portIndex": 0
              },
              "to": {
                "nodeId": "59f3a59d-1812-cf4a-50c4-ffd4bea1d34d",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "e56ff646-c454-5603-3eb2-b2c0ed91197a",
                "portIndex": 0
              },
              "to": {
                "nodeId": "d167bcbc-3263-32cc-0d23-251d79ec464c",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "59f3a59d-1812-cf4a-50c4-ffd4bea1d34d",
                "portIndex": 0
              },
              "to": {
                "nodeId": "ea4eeaea-c74d-ef6b-437a-7c285e6ff58b",
                "portIndex": 0
              }
            }]
          },
          "thirdPartyData": {
            "gui": {
              "name": "Inner workflow of custom transformer",
              "nodes": {
                "ea4eeaea-c74d-ef6b-437a-7c285e6ff58b": {
                  "uiName": "rfm_purchase",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4519,
                    "y": 4746
                  }
                },
                "e56ff646-c454-5603-3eb2-b2c0ed91197a": {
                  "uiName": "segmentation",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5570,
                    "y": 4836
                  }
                },
                "59f3a59d-1812-cf4a-50c4-ffd4bea1d34d": {
                  "uiName": "rfm_daily_summerised",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4299,
                    "y": 4724
                  }
                },
                "72233ecf-f3a6-49f7-8e53-58404a070956": {
                  "uiName": "active_inactive",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4946,
                    "y": 4780
                  }
                },
                "425b2c9d-f25d-9107-49d7-ef723ce53449": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 4322,
                    "y": 4615
                  }
                },
                "b28ed4f1-4d37-610f-ed65-7a4a5d27b920": {
                  "uiName": "recharge_process",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5156,
                    "y": 4799
                  }
                },
                "d167bcbc-3263-32cc-0d23-251d79ec464c": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5776,
                    "y": 4939
                  }
                },
                "d0377de7-a1e4-02f5-56fb-1317a2dc0f8f": {
                  "uiName": "usage_process",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4730,
                    "y": 4762
                  }
                },
                "b75153f5-e16b-7e96-e9a4-77f36e4d2117": {
                  "uiName": "purchase_process",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5363,
                    "y": 4815
                  }
                }
              }
            }
          },
          "publicParams": []
        }
      }
    }, {
      "id": "428ec8be-3805-61e3-4b04-7c5d29922ad2",
      "operation": {
        "id": "643d8706-24db-4674-b5b4-10b5129251fc",
        "name": "Transform"
      },
      "parameters": {
        "Parameters of input Transformer": {

        }
      }
    }, {
      "id": "60f149ca-2650-9efb-dc53-09b00343bf36",
      "operation": {
        "id": "65240399-2987-41bd-ba7e-2944d60a3404",
        "name": "Create Custom Transformer"
      },
      "parameters": {
        "inner workflow": {
          "workflow": {
            "nodes": [{
              "id": "c2f0a205-ed28-04e7-ed27-0d0a9b49d7fa",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nimport os\nimport pickle\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nimport traceback\n\nfile_path = '/home/tnmops/seahorse3_bkp/'\ndag_run_id = \"manual__2023-07-10T11:06:51\"    \nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nmsisdn_name = 'msisdn'\nTRANSACTION_PRODUCT_NAME = 'product_id'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\npack_name = \"product_name\"\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nrule_path = os.path.join(file_path, \"rule.csv\")\ndb = pd.read_csv(rule_path)\n\n    \ndef load_picke_file(filename):\n    with open(filename, 'rb') as handle:\n        data = pickle.load(handle)\n    return data\n    \n    \nclass Fpgrowth(object):\n    def __init__(self, purchase, dag_run_id, item):\n        self.purchase = purchase\n        self.frequent_itemsets = None\n        self.results = None\n        self.results_processed = None\n        self.item_set_max_length = 4\n        self.item_set_min_length = 1\n        self.dag_run_id = dag_run_id\n        self.item = item\n\n        status, msg = self.form_associations()\n        print(msg)\n\n        if not status:\n            return None\n        self.process_association()\n\n        self.results_processed.to_csv(os.path.join(file_path, \"association.csv\"), header=True,\n                                      index=False)\n        self.results_processed.to_csv(os.path.join(file_path, str(item) + \"_association.csv\"),\n                                      header=True,\n                                      index=False)\n\n    def form_associations(self):\n        print('len of purchase before fropping na ', len(self.purchase))\n        self.purchase.dropna(inplace=True)\n        print('len of purchase ', len(self.purchase))\n\n        basket = (self.purchase.groupby([msisdn_name, TRANSACTION_PRODUCT_NAME])[\n                      TRANSACTION_PRODUCT_NAME]\n                  .count().unstack().reset_index().fillna(0)\n                  .set_index(msisdn_name))\n\n        print('basket created')\n        print('len of basket ', len(basket))\n\n        basket_sets = basket.applymap(encode_units)\n        basket_sets_filter = basket_sets[(basket_sets > 0).sum(axis=1) >= 2]\n        frequent_itemsets = fpgrowth(basket_sets_filter, min_support=0.03, use_colnames=True)\n        print('frequent_itemsets created')\n        print('len of frequent_itemsets ', len(frequent_itemsets))\n        if frequent_itemsets is None or len(frequent_itemsets) == 0:\n            # retun none so that the next cluster\n            return False, \"the result does not have any lenth the lenfth is \" + str(len(frequent_itemsets))\n        frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n\n        self.frequent_itemsets = frequent_itemsets[(frequent_itemsets['length'] >= self.item_set_min_length) & (\n                frequent_itemsets['length'] <= self.item_set_max_length)]\n        print('frequent_itemsets filtered')\n        self.results = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=2).sort_values('lift',\n                                                                                                        ascending=False).reset_index(\n            drop=True)\n        print('association_rules created')\n        print('self.results is ', self.results)\n        print(' len self.results is ', len(self.results))\n        print(' type self.results is ', type(self.results))\n        if len(self.results) > 2:\n            return True, \"got the asociaitons\"\n        return False, \"the result does not have any lenth the lenfth is \" + str(len(self.results))\n\n    def process_association(self):\n        print(' inside process_association')\n        results = self.results\n        results['antecedents_length'] = results['antecedents'].apply(lambda x: len(x))\n        results['consequents_length'] = results['consequents'].apply(lambda x: len(x))\n        # antecedents1 is a set converted from frozen set antecedents\n        results['antecedents1'] = results['antecedents'].apply(set)\n        results['consequents1'] = results['consequents'].apply(set)\n\n        results.drop(['antecedents', 'consequents', 'leverage', 'conviction'], inplace=True, axis=1, errors='ignore')\n\n        self.results_processed = results\n\n\n\n\nclass UpsellCrossell(object):\n    def __init__(self, consequents_length=1, exclude_types=None,\n                 dag_run_id=None, db=None, pack_info=None,\n                 result=None, cluster_number=None, segement_name=None):\n        self.segement_name_list = None\n        if exclude_types is None:\n            exclude_types = ['SMS']\n        self.dag_run_id = dag_run_id\n        self.exclude_types = exclude_types\n        self.associations_df = result\n        self.db = db\n        self.pack_info = pack_info\n        self.df_cross_df = None\n        self.df_upsell_df = None\n        self.consequents_length = consequents_length\n\n        # self.read_fiels()\n        self.check_files()\n\n    def read_fiels(self):\n        try:\n            self.pack_info = pd.read_csv(os.path.join(file_path, 'packinfo.csv'))\n            self.associations_df = pd.read_csv(os.path.join(file_path, self.dag_run_id, 'association.csv'))\n        except Exception as e:\n            raise ValueError(e)\n\n    def check_files(self):\n        if self.pack_info is None or self.associations_df is None:\n            print(f\"the packinfo or association is null\")\n            raise ValueError(f\"the packinfo or association is null\")\n\n    def determine_service_type(self):\n        try:\n            print('self.associations_df is ', self.associations_df)\n            print('self.associations_df.columns is ', self.associations_df.columns)\n            if len(self.associations_df) <= 2:\n                return\n            self.associations_df = self.associations_df[self.associations_df['consequents_length'] == 1]\n            df = self.associations_df.apply(self.determine_service, axis=1)\n            if len(df) == 0:\n                return\n            for type_info in self.exclude_types:\n                df = df[~df['type_service'].str.contains(type_info)]\n\n            self.df_upsell_df = df[df['service'] == 'upsell']\n            self.df_cross_df = df[df['service'] != 'upsell']\n\n\n        except Exception as e:\n            print(\"error occoured in determine service\" + str(e))\n            raise ValueError(e)\n\n    def determine_service(self, x):\n        print('x is', x)\n\n        group_type = PACK_INFO_SUB_CATEGORY\n\n        def get_pack_type(pack_name):\n            type_pack = None\n            print(\"isnide get_pack_type\")\n            print(\"pack_name is \", pack_name)\n            print(\"group_type is \", group_type)\n            # print(\"f.Features.PACK_INFO_PACK_COLUMN_NAME is \", f.Features.PACK_INFO_PACK_COLUMN_NAME)\n            # print(\"self.pack_info is \", self.pack_info)\n            print(\"len self.pack_info is \", len(self.pack_info))\n\n            data = self.pack_info[self.pack_info[PACK_INFO_PACK_COLUMN_NAME] == pack_name][group_type]\n            print(\"data is \", data)\n            if len(data) > 0:\n                type_pack = data.values[0]\n                print(\"type_pack is \", type_pack)\n            return type_pack\n\n        x['antecedents1'] = str(x['antecedents1'])\n        x['consequents1'] = str(x['consequents1'])\n\n        antecedents_list = list(eval(x['antecedents1']))\n        print('antecedents_list is ', antecedents_list)\n        consequents_list = list(eval(x['consequents1']))\n        print('consequents_list is ', consequents_list)\n        antecedents_length = len(antecedents_list)\n\n        i = 0\n        service = \"upsell\"\n        type_service = None\n        temp_set = set()\n        try:\n            # since for each antecident we need to find eg: antecident length 1 ,2 ,3\n            while i < antecedents_length:\n                print('i is', i)\n                temp_set.add(get_pack_type(antecedents_list[i]))\n                print('temp_set is', temp_set)\n                i = i + 1\n                print('i is', i)\n            temp_set.add(get_pack_type(consequents_list[0]))\n            print('temp_set 1  is', temp_set)\n\n            if len(temp_set) > 1:\n                service = \"corsssell\"\n                type_service = ','.join(temp_set)\n            elif len(temp_set) == 1:\n                type_service = next(iter(temp_set))\n\n            x['service'] = service\n            x['type_service'] = type_service\n            return x\n        except Exception as e:\n            print(\"error occoured in determine_service\", e)\n            raise ValueError(e)\n\n    \n    \n    \n    def find_crossell(self, segement_name, cluster_number):\n        # try:\n            segement_name = f\"{segement_name}-{str(int(cluster_number))}\"\n            # self.segement_name_list = segement_name.split(\"|\")\n\n            print(\"self.df_cross_df is \", self.df_cross_df)\n            df = self.df_cross_df.apply(self.cross_sell_parser, axis=1)\n\n            if len(df) > 0:\n                df = df.sort_values(by=\"confidence\", ascending=False)\n                rule_path = os.path.join(file_path, \"rule.csv\")\n                \n                \n                db = pd.read_csv(rule_path)\n                \n                \n                segements = findByAutoPilotIdAndSegementName(db, _id=self.dag_run_id,segement_name=segement_name,cluster_number=int(cluster_number))\n                \n                \n                if segements is None:\n                    return\n                db1 = self.insert_segementinfo(segements, 1, df, \"crossell\")\n                \n            \n            #     # SegementRepo.deleteById(self.db, [segements.id])\n            # # # ------------------------adding to db -------------------------------#\n            for index, row in df.iterrows():\n                # print(row['confidence'])\n                # info = pd.read_csv(rule_path)\n                # info.service_type = str(row['type_service'])\n                # info.type_info = str(row['val'])\n                # info.dag_run_id = self.dag_run_id\n                # info.support = round(float(row['support']), 2)\n                # info.confidence = round(float(row['confidence']), 2)\n                # info.lift = round(float(row['lift']), 2)\n                # print(\"row['conci'] is \", row['conci'])\n\n                # info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\n                # info.recommended_pack_id = str(row['conci'])\n\n                # info.number_of_current_packs = int(row['antecedents_length'])\n                # info.current_pack = \"|\".join(\n                #     [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                # info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n\n                # info.segement_name = segements.segment_name\n                # current_pack_types = \"|\".join(\n                #     [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                # info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n                \n                db1['service_type'] = str(row['type_service'])\n                db1['type_info'] = str(row['val'])\n                db1['dag_run_id'] = self.dag_run_id\n                db1['support'] = round(float(row['support']), 2)\n                db1['confidence'] = round(float(row['confidence']), 2)\n                db1['lift'] = round(float(row['lift']), 2)\n                print(\"row['conci'] is \", row['conci'])\n                \n                db1['recommended_pack'] = str(getpackname(row['conci'], self.pack_info))\n                db1['recommended_pack_id'] = str(row['conci'])\n                \n                db1['number_of_current_packs'] = int(row['antecedents_length'])\n                db1['current_pack'] = \"|\".join(\n                    [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                db1['current_pack_ids'] = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n                db1['segement_name'] = segements['segment_name']\n                \n                current_pack_types = \"|\".join(\n                    [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                db1['recommendation_type'] = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n            \n                # db2 = db1.append(db, ignore_index=True)\n                # db = pd.concat(db_list1,db)\n            # db = pd.DataFrame(db)\n                db = pd.concat([db, db1], ignore_index=True)\n\n            db.to_csv(rule_path, index = False, header = True)\n\n            #     print(\"added crossel info to db\")\n            #     AssociationRepo.create(db=self.db, info=info)\n            # # # ------------------------adding to db -------------------------------#\n\n        # except Exception as e:\n        #     traceback.print_exc()\n        #     raise ValueError(e)\n\n    def cross_sell_parser(self, x):\n        cross_sell = False\n        try:\n            print('x is', x)\n            anti = list(eval(x['antecedents1']))\n            conci = list(eval(x['consequents1']))\n            print('here 2 ')\n            print(f\"anti {anti} conci {conci}\")\n            if len(anti) == 1:\n                anti_catagory = self.pack_info[self.pack_info[pack_name] == anti[0]]['bundle_type'].values\n                anti_class = self.pack_info[self.pack_info[pack_name] == anti[0]]['bundle_type'].values\n                if len(anti_catagory) > 0:\n                    anti_catagory = anti_catagory[0]\n                    anti_class = anti_class[0]\n                conci_catagory = self.pack_info[self.pack_info[pack_name] == conci[0]]['bundle_type'].values\n                conci_class = self.pack_info[self.pack_info[pack_name] == conci[0]]['bundle_type'].values\n                if len(conci_catagory) > 0:\n                    conci_catagory = conci_catagory[0]\n                    conci_class = conci_class[0]\n                print(f\"the anti c  is {anti_catagory}  and conci cata is  {conci_catagory}\")\n                if ((anti_catagory != conci_catagory) and (anti_class != conci_class)):\n                    cross_sell = True\n            anti_name = None\n            if len(anti) == 2:\n                cross_sell = True\n                for anti_item in anti:\n                    print(\"the anti item \", anti_item)\n                    anti_catagory = self.pack_info[self.pack_info[pack_name] == anti_item]['bundle_type'].values\n                    conci_catagory = self.pack_info[self.pack_info[pack_name] == conci[0]]['bundle_type'].values\n                    print(f\"the anticent name {anti_item} conci name {conci[0]}\")\n                    print(f\"the anticent catagory {anti_catagory} conci cata {conci_catagory}\")\n                    if (anti_catagory == conci_catagory):\n                        cross_sell = False\n                        anti_name = None\n                    else:\n                        anti_name = anti_item\n\n            if cross_sell:\n                x['val'] = \"crossell\"\n                if len(anti) == 2:\n                    x['anti'] = anti_name\n                else:\n                    x['anti'] = anti[0]\n\n                x['conci'] = conci[0]\n                return x\n            x['val'] = \"not_valid_crossell\"\n            x['anti'] = anti[0]\n            x['conci'] = conci[0]\n            return x\n        except Exception as e:\n            print('the error occoured in find crosssell', e)\n            raise ValueError(e)\n\n    def find_upsell(self, type_service, anticendent_number, segement_name, cluster_number):\n        # try:\n            is_upsell = 1\n            if type_service != 'upsell':\n                is_upsell = 0\n\n            rule_path = os.path.join(file_path, \"rule.csv\")\n            db = pd.read_csv(rule_path)\n                \n                \n            segements = findByAutoPilotIdAndSegementName(db, _id=self.dag_run_id,\n                                                                          segement_name= f\"{segement_name}-{str(int(cluster_number))}\",\n                                                                          cluster_number=int(cluster_number))\n            if segements is None:\n                return\n            \n            if anticendent_number == 1:\n                print(\"inside antecedent 1 \")\n                df = self.df_upsell_df[self.df_upsell_df['antecedents_length'] == 1]\n                data = df.sort_values(by='confidence', ascending=False)\n                print(len(data.drop_duplicates(subset=['consequents1'])))\n                print(\"data.columns inside antecedent 1 is \", data.columns)\n                data1 = data.apply(self.check_upsell, anticendent_number=1, axis=1)\n                print('check_upsell completed')\n                # ------------------- insert db ------------------------------\n                db1 = self.insert_segementinfo(segements, anticendent_number, data1, \"upsell\")\n\n                # ------------------------adding to db -------------------------------#\n                for index, row in data1.iterrows():\n                #     print(row['confidence'])\n                #     info = schemas.AssociationInfo()\n                    # info.service_type = str(row['type_service'])\n                    # info.type_info = str(row['upsell_case'])\n                    # info.dag_run_id = self.dag_run_id\n                    # info.support = round(float(row['support']), 2)\n                    # info.confidence = round(float(row['confidence']), 2)\n                    # info.lift = round(float(row['lift']), 2)\n                    # print(\"row['conci'] is \", row['conci'])\n                    # info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\n                    # info.recommended_pack_id = str(row['conci'])\n\n                    # info.number_of_current_packs = int(row['antecedents_length'])\n                    # info.current_pack = \"|\".join(\n                    #     [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    # info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n                    # info.segement_name = segements.segment_name\n                    # print('info.number_of_current_packs is', info.number_of_current_packs)\n                    # print('info.current_pack is', info.current_pack)\n                    # print('info.recommended_pack is', info.recommended_pack)\n\n                    # print('info is', info)\n                    # current_pack_types = \"|\".join(\n                    #     [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    # info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n                #     AssociationRepo.create(db=self.db, info=info)\n                #     print('all  inserted')\n                    \n                    db1['service_type'] = str(row['type_service'])\n                    db1['type_info'] = str(row['upsell_case'])\n                    db1['dag_run_id'] = self.dag_run_id\n                    db1['support'] = round(float(row['support']), 2)\n                    db1['confidence'] = round(float(row['confidence']), 2)\n                    db1['lift'] = round(float(row['lift']), 2)\n                    print(\"row['conci'] is \", row['conci'])\n                    \n                    db1['recommended_pack'] = str(getpackname(row['conci'], self.pack_info))\n                    db1['recommended_pack_id'] = str(row['conci'])\n                    \n                    db1['number_of_current_packs'] = int(row['antecedents_length'])\n                    db1['current_pack'] = \"|\".join(\n                        [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    db1['current_pack_ids'] = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n                    db1['segement_name'] = segements['segment_name']\n                    \n                    current_pack_types = \"|\".join(\n                        [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    db1['recommendation_type'] = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n                \n                \n                db = pd.concat([db, db1], ignore_index=True)\n\n                # db = db.append(db_list1, ignore_index=True)\n                db.to_csv(rule_path , index = False, header = True)\n                # # ------------------------adding to db -------------------------------#\n            # ---------------------ipo venda -----------------------------------------#\n\n            elif anticendent_number == 2:\n                print(\"inside antecedent 2 \")\n                df = self.df_upsell_df[self.df_upsell_df['antecedents_length'] == 2]\n                if len(df) == 0:\n                    return None\n                data = df.sort_values(by='confidence', ascending=False)\n                print(len(data.drop_duplicates(subset=['consequents1'])))\n                print(\"data.columns inside antecedent 2  is \", data.columns)\n                data1 = data.apply(self.check_upsell, anticendent_number=2, axis=1)\n                # ------------------------adding to db -------------------------------#\n                \n                db1 = self.insert_segementinfo(segements, anticendent_number, data1, \"upsell\")\n\n                for index, row in data1.iterrows():\n                #     print(row['confidence'])\n                #     info = schemas.AssociationInfo()\n                #     info.service_type = str(row['type_service'])\n                #     info.type_info = str(row['upsell_case'])\n                #     info.dag_run_id = self.dag_run_id\n                #     info.support = round(float(row['support']), 2)\n                #     info.confidence = round(float(row['confidence']), 2)\n                #     info.lift = round(float(row['lift']), 2)\n                #     print(\"row['conci'] is \", row['conci'])\n\n                #     info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\n                #     info.recommended_pack_id = str(row['conci'])\n\n                #     info.segement_name = segements.segment_name\n                #     info.number_of_current_packs = int(row['antecedents_length'])\n                #     info.current_pack = \"|\".join(\n                #         [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                #     info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n                   \n                   \n                #     current_pack_types = \"|\".join(\n                #         [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                #     info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n                #     AssociationRepo.create(db=self.db, info=info)\n                # ------------------------adding to db -------------------------------#\n                    \n                    db1['service_type'] = str(row['type_service'])\n                    db1['type_info'] = str(row['upsell_case'])\n                    db1['dag_run_id'] = self.dag_run_id\n                    db1['support'] = round(float(row['support']), 2)\n                    db1['confidence'] = round(float(row['confidence']), 2)\n                    db1['lift'] = round(float(row['lift']), 2)\n                    print(\"row['conci'] is \", row['conci'])\n                    \n                    db1['recommended_pack'] = str(getpackname(row['conci'], self.pack_info))\n                    db1['recommended_pack_id'] = str(row['conci'])\n                    \n                    db1['number_of_current_packs'] = int(row['antecedents_length'])\n                    db1['current_pack'] = \"|\".join(\n                        [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    db1['current_pack_ids'] = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n                    db1['segement_name'] = segements['segment_name']\n                    \n                    current_pack_types = \"|\".join(\n                        [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    db1['recommendation_type'] = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n                \n                \n                db = pd.concat([db, db1], ignore_index=True)                \n                \n                \n                # db = db.append(db_list1, ignore_index=True)\n                db.to_csv(rule_path, index = False, header = True)              \n                # # ---------------------ipo venda -----------------------------------------#\n\n            elif anticendent_number == 3:\n                print(\"inside antecedent 3 \")\n                df = self.df_upsell_df[self.df_upsell_df['antecedents_length'] == 3]\n                if len(df) == 0:\n                    return None\n\n                data = df.sort_values(by='confidence', ascending=False)\n                print(len(data.drop_duplicates(subset=['consequents1'])))\n                print(\"data.columns inside antecedent 3 is \", data.columns)\n                data1 = data.apply(self.check_upsell, anticendent_number=3, axis=1)\n                # ------------------------adding to db -------------------------------#\n                db1 = self.insert_segementinfo(segements, anticendent_number, data1, \"upsell\")\n\n                for index, row in data1.iterrows():\n                #     print(row['confidence'])\n                #     info = schemas.AssociationInfo()\n                #     info.service_type = str(row['type_service'])\n                #     info.type_info = str(row['upsell_case'])\n                #     info.dag_run_id = self.dag_run_id\n                #     info.support = round(float(row['support']), 2)\n                #     info.confidence = round(float(row['confidence']), 2)\n                #     info.lift = round(float(row['lift']), 2)\n                #     print(\"row['conci'] is \", row['conci'])\n\n                #     info.recommended_pack = str(getpackname(row['conci'], self.pack_info))\n                #     info.recommended_pack_id = str(row['conci'])\n\n                #     info.segement_name = segements.segment_name\n                #     info.number_of_current_packs = int(row['antecedents_length'])\n                #     info.current_pack = \"|\".join(\n                #         [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                #     info.current_pack_ids = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n                #     print('info.number_of_current_packs is', info.number_of_current_packs)\n                #     print('info.current_pack is', info.current_pack)\n                #     print('info.recommended_pack is', info.recommended_pack)\n                #     current_pack_types = \"|\".join(\n                #         [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                #     info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n                #     AssociationRepo.create(db=self.db, info=info)\n                # ------------------------adding to db -------------------------------#\n                \n                    db1['service_type'] = str(row['type_service'])\n                    db1['type_info'] = str(row['upsell_case'])\n                    db1['dag_run_id'] = self.dag_run_id\n                    db1['support'] = round(float(row['support']), 2)\n                    db1['confidence'] = round(float(row['confidence']), 2)\n                    db1['lift'] = round(float(row['lift']), 2)\n                    print(\"row['conci'] is \", row['conci'])\n                    \n                    db1['recommended_pack'] = str(getpackname(row['conci'], self.pack_info))\n                    db1['recommended_pack_id'] = str(row['conci'])\n                    \n                    db1['number_of_current_packs'] = int(row['antecedents_length'])\n                    db1['current_pack'] = \"|\".join(\n                        [str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    db1['current_pack_ids'] = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n                    db1['segement_name'] = segements['segment_name']\n                    \n                    current_pack_types = \"|\".join(\n                        [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n                    db1['recommendation_type'] = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n                \n                \n                db = pd.concat([db, db1], ignore_index=True)                \n                \n                # db = db.append(db_list1, ignore_index=True)\n                db.to_csv(rule_path, index = False, header = True)\n            \n            # SegementRepo.deleteById(self.db, [segements.id])\n            \n            # ---------------------ipo venda -----------------------------------------#\n        # except Exception as e:\n        #     print(\"error ocoured in output service\", e)\n        #     raise ValueError(e)\n    \n    \n    def check_upsell(self, x, anticendent_number):\n        print('Inside check_upsell')\n\n        col = PACK_INFO_PACK_PRICE_COLUMN_NAME\n\n        def get_price(pack_name):\n            price = None\n            data = self.pack_info[self.pack_info[PACK_INFO_PACK_COLUMN_NAME] == pack_name][col]\n            if (len(data) > 0):\n                price = data.values[0]\n            return price\n\n        anti = None\n        conci = None\n        print('x is', x)\n        print(\"x['consequents1'] is\", x['consequents1'])\n        print(\"anticendent_number is\", anticendent_number)\n        x['is_upsell'] = 1\n        x['upsell_case'] = \"upsell\"\n\n        try:\n            conci = list(eval(x['consequents1']))[0]\n            print(\"teh conci is \", conci)\n            conci_price = get_price(conci)\n            print(f\"the conci price is {conci_price}\")\n            x['conci'] = conci\n\n            if anticendent_number == 1:\n                anti = list(eval(x['antecedents1']))[0]\n                anti_price = get_price(anti)\n\n                if conci_price < anti_price:\n                    x['is_upsell'] = 0\n                    x['upsell_case'] = \"not_a_valid_upsell\"\n                    x['conci'] = conci\n            else:\n                amti_list = list(eval(x['antecedents1']))\n                for anti_obj in amti_list:\n\n                    anti_price = get_price(anti_obj)\n                    if conci_price <= anti_price:\n                        x['is_upsell'] = 0\n                        x['upsell_case'] = \"not_a_valid_upsell\"\n                        x['conci'] = conci\n\n            return x\n        except Exception as e:\n            print(\" the error occoured in check_upsell \", e)\n            raise ValueError(e)\n            \n    def update_info(self, anticident_name, type_of_service, segement_name, cluster_number):\n        segements = SegementRepo.findByAutoPilotIdAndSegementName(self.db, segement_name=segement_name,\n                                                                  cluster_number=cluster_number)\n        pass\n    \n    \n    def insert_segementinfo(self, segements, anticendent_number, data1, type_of_service):\n        db_list = []\n        if data1 is None or len(data1) == 0:\n            ic(f\"the segement {segements.segment_name} has no upsell 1 data1 \")\n            return\n        if type_of_service == \"upsell\":\n            data1 = data1[data1['is_upsell'] == 1]\n            if data1 is None or len(data1) == 0:\n                # ic(f\"the segement {segements.segment_name}  has no upsell 1\")\n                return\n        data2 = data1.sort_values(by='confidence', ascending=False)\n        data2 = data2.head(5)\n        segments_list = []\n        \n        \n        for index, row in data2.iterrows():\n            get_rule_query(segements)\n            \n           \n            # info = SegementInfo()\n            # info.segment_name = segements.segment_name\n            # info.segment_name = None\n            # info.cluster_description=segements.cluster_description\n            # info.dag_run_id = self.dag_run_id\n            # current_product_id = \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))])\n            # info.current_product = current_product_id\n            # info.current_products_names = \"|\".join([str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n           \n            # rec_product_id = str(row['conci'])\n            \n            # info.recommended_product_id = rec_product_id\n            # info.recommended_product_name = str(getpackname(row['conci'], self.pack_info))\n            # info.predicted_arpu = None\n            # info.current_arpu = None\n            # info.segment_length = segements.segment_length\n            # info.rule = None\n            # info.actual_rule = None\n            # info.uplift = None\n            # info.incremental_revenue = None\n            # info.campaign_type = type_of_service\n            # info.campaign_name = None\n            # info.action_key = None\n            # info.robox_id = None\n            # info.samples = segements.samples\n            # info.segment_name = segements.segment_name\n            # info.current_ARPU_band = None\n            # info.current_revenue_impact = None\n            # info.customer_status = segements.customer_status\n            # info.query = segements.query\n            # info.cluster_no = segements.cluster_no\n            # info.confidence = round(float(row['confidence']), 2)\n            \n            current_pack_types = \"|\".join(\n                [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n            \n            # info.recommendation_type = f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n\n\n            current_pack_types = \"|\".join(\n                [str(getpacktype(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))])\n            d = {\n                'segment_name': segements['segment_name'],\n                'cluster_description': segements['cluster_description'],\n                'dag_run_id': self.dag_run_id,\n                'current_product': \"|\".join([str(i) for i in list(eval(str(row['antecedents1'])))]),\n                'current_products_names': \"|\".join([str(getpackname(i, self.pack_info)) for i in list(eval(str(row['antecedents1'])))]),\n                'recommended_product_id': str(row['conci']),\n                'recommended_product_name': str(getpackname(row['conci'], self.pack_info)),\n                'predicted_arpu': None,\n                'current_arpu': None,\n                'segment_length': segements['segment_length'],\n                'rule': None,\n                'actual_rule': None,\n                'uplift': None,\n                'incremental_revenue': None,\n                'campaign_type': type_of_service,\n                'campaign_name': None,\n                'action_key': None,\n                'robox_id': None,\n                'samples': segements['samples'],\n                'current_ARPU_band': None,\n                'current_revenue_impact': None,\n                'customer_status': segements['customer_status'],\n                'cluster_no': segements['cluster_no'],\n                'confidence': round(float(row['confidence']), 2),\n                'recommendation_type': f\"{current_pack_types} - {getpacktype(row['conci'], self.pack_info)}\"\n            }\n            \n            df = pd.DataFrame([d])\n        \n        return df\n        #     segments_list.append(info)\n\n        # for segment in segments_list:\n        #     SegementRepo.create(self.db, segment)\n\n\n\n\ndef findByAutoPilotIdAndSegementName(data, _id, segement_name, cluster_number):\n    df = pd.DataFrame(data)\n    result = df[\n        (df['dag_run_id'] == _id) &\n        (df['segment_name'] == segement_name) &\n        (df['cluster_no'] == cluster_number)\n    ].head(1)\n    \n    # Return the first matching row as a dictionary\n    return result.to_dict(orient='records')[0] if not result.empty else None\n\n\ndef getpacktype(product_id, packinfo_df):\n    product_name = \"No product name\"\n    try:\n        if (packinfo_df is not None):\n            if (product_id in packinfo_df['product_id'].values):\n\n                product_name = str(packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"type\"])\n                print(\"the product name of \", product_id, \" is \", product_name)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackname \", e)\n\n    return product_name\n\n\n    \ndef encode_units(x):\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\n        \n        \n        \ndef getpackname(product_id, packinfo_df):\n    product_name = \"No product name\"\n    try:\n        if (packinfo_df is not None):\n            if (product_id in packinfo_df['product_id'].values):\n\n                product_name = str(packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"product_name\"])\n                print(\"the product name of \", product_id, \" is \", product_name)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackname \", e)\n\n    return product_name\n    \ndef get_rule_query(segements):\n\n    return None    \n    \n    \ndef transform(dataframe):\n    dataframe = dataframe.toPandas()\n    file_path = '/home/tnmops/seahorse3_bkp/'\n    dag_run_id = \"manual__2023-07-10T11:06:51\"    \n    PACK_INFO_SUB_CATEGORY = 'bundle_type'\n    msisdn_name = 'msisdn'\n    TRANSACTION_PRODUCT_NAME = 'product_id'\n    PACK_INFO_PACK_COLUMN_NAME = 'product_id'\n    pack_name = \"product_name\"\n    PACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\n\n    \n    \n    \n    result_dict_path = os.path.join(file_path, 'purchased_for_association', 'dict.pickle')\n    data_path = os.path.join(file_path, \"dict.pickle\")\n    data_dict = load_picke_file(data_path)\n    pack_info = os.path.join(file_path, 'pack_info.csv')\n    pack_info_df = pd.read_csv(pack_info)\n    data = load_picke_file(result_dict_path)\n\n    filtered_dict = {k: v for k, v in data.items()}\n    filtered_data__dict = {k: v for k, v in data_dict.items()}\n    purchase_list=[]\n\n\n    for item, val in filtered_dict.items():\n            \n            if val is None:\n                continue\n            data = pd.read_csv(filtered_data__dict.get(item))\n            \n            purchase = pd.read_csv(val)\n            for cluster in data['label'].unique():\n                data_temp = data[data['label'] == cluster]\n                purchase_filtered = purchase[purchase[msisdn_name].isin(data_temp[msisdn_name])]\n                pruchase_grp_df=purchase_filtered.groupby([TRANSACTION_PRODUCT_NAME]).agg({msisdn_name:'nunique'}).reset_index()\n                pruchase_grp_df['segment_name'] = f\"{item}-{str(cluster)}\"\n\n                purchase_list.append(pruchase_grp_df)\n\n                fp = Fpgrowth(purchase_filtered, dag_run_id, item=item)\n\n\n                if fp.results is None or len(fp.results) == 0:\n                    continue\n\n                uc = UpsellCrossell(dag_run_id=dag_run_id, pack_info=pack_info_df, result=fp.results)\n                dd = uc.determine_service_type()\n                if len(uc.associations_df) <= 2:\n                    continue\n                if uc.df_cross_df is not None and len(uc.df_cross_df) > 0:\n                    uc.find_crossell(segement_name=item, cluster_number=cluster)\n                if uc.df_upsell_df is None or len(uc.df_upsell_df) == 0:\n                    continue\n                for number in [1, 2, 3]:\n                    uc.find_upsell(type_service='upsell', anticendent_number=number, segement_name=item,\n                                  cluster_number=cluster)\n\n    purchase_list_df = pd.concat(purchase_list)\n    # purchase_list_df['dag_run_id']=str(dag_run_id)\n    # purchase_list_df.to_csv(os.path.join(file_path, \"total_purchased_products.csv\"), header=True,\n    #                       index=False)\n\n    \n    return purchase_list_df"
              }
            }, {
              "id": "85dd0b8f-b63a-351c-ecc8-a9930ba70944",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import os.path\nfrom pathlib import Path\nimport dask.dataframe as dd\nimport pandas as pd\nimport traceback\nimport numpy as np\nimport os\nimport pathlib\nimport dask.array as da\nfrom dask.dataframe import merge\nimport datetime\nimport dask.array as da\nfrom pathlib import Path\nimport  traceback\nimport pickle\nfrom datetime import datetime, timedelta,date\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nimport xgboost as xgb\nfrom functools import reduce\n\n\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\nfile_path = '/home/tnmops/seahorse3_bkp/'\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nMSISDN_COL_NAME = 'msisdn'\ndag_run_id = 'manual__2023-07-11T10:20:22'\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\npack_cols = ['product_id','product_type']\n\n\ndef get_file_names():\n    return  { \n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"rfm_purchase\":   {\n        \"p1\": \"purchase_nov_full_df.csv\",\n        \"p2\": \"purchase_dec_full_df.csv\",\n        \"p3\": \"purchase_jan_full_df.csv\",\n        \"p4\": \"purchase_feb_full_df.csv\",\n        \"p5\": \"purchase_march_full_df.csv\" },\n        \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        }\n        }\n\ndef findByAutoPilotId(df, _id):\n    return df[df['dag_run_id'] == _id]\n\ndef transform(dataframe):\n    # dataframe = dataframe.to_pandas()\n    # dask_df = dd.from_pandas(dataframe, npartitions=4)  # 'npartitions' can be adjusted based on your data size\n\n        \n    file_path = '/home/tnmops/seahorse3_bkp/'\n    file_name_dict = get_file_names()\n        \n        # fin = dd.read_csv(os.path.join(file_path, \"APA_output.csv\"))\n        # li = fin['recommended_product_id'].unique().tolist()\n\n    purchase = {}\n    for month in purchase_no_months_for_next_purchase:\n        purchase[month] = dd.read_csv(os.path.join(file_path, file_name_dict.get(\"rfm_purchase\").get(month)))\n    \n    df1 = purchase.get('p2')\n    for key, value in purchase.items():\n        value = value.reset_index(drop=True)    \n        \n        \n    source = dd.concat([purchase['p1'], purchase['p2'], purchase['p3']], axis=0, ignore_index=True, verify_integrity=False)\n    target = dd.concat([purchase['p4']])    \n        \n        \n    pack_info = os.path.join(file_path, 'pack_info.csv')\n    pack = dd.read_csv(pack_info, usecols=['product_id','product_type'])        \n        \n    f = merge(source,pack,how='inner',on='product_id')\n    f1 = merge(target,pack,how='inner',on='product_id')\n        \n    p_type = ['DATA','VOICE']\n        \n    for i in p_type:\n        print('the product type is------------ ',i)\n            \n        source = f[f['product_type']==i]\n        target = f1[f1['product_type']==i]\n        \n        final = nextpurchase(source,target,i+\"_model\",dag_run_id)        \n        xgmodel(final,i, dag_run_id)\n\n    nextthreemonth(purchase, dag_run_id)\n    recommended_id_prediction(dag_run_id)\n    \n    \n    weekday_product_purchase_count_path = os.path.join(file_path, \"weekday_product_purchase_count.csv\")\n    weekday_product_purchase_count(dag_run_id)\n    weekday_product_purchase_count_df = pd.read_csv(weekday_product_purchase_count_path)\n        \n        \n        \n    next_purchase_date_path = os.path.join(file_path, 'recommended_id_prediction.csv')   \n    next_purchase_date_df = pd.read_csv(next_purchase_date_path)\n    next_purchase_date_df['recommended_product_id']=next_purchase_date_df['recommended_product_id'].astype(int)\n\n    \n    rule_path = os.path.join(file_path, \"rule.csv\")\n    db = pd.read_csv(rule_path)\n    \n    # segements_two = findByAutoPilotId(db, dag_run_id)\n    # for seg in db:\n    #     if seg.recommended_product_id is  None or seg.recommended_product_id ==0:\n    #         # ic(\"deleteing segement if recommended_product_id is none \")\n    #         print(\"seg.id 1  is \",seg.id)\n    #         # SegementRepo.deleteById(db=db, _ids=[seg.id])\n    \n    return next_purchase_date_df\n\n\n\n\n\ndef nextpurchase(source, target,filename, dag_run_id):    \n    \n    \n    source['cdr_date'] = dd.to_datetime(source['cdr_date'])\n    target['cdr_date'] = dd.to_datetime(target['cdr_date'])\n\n    tx_user = source['msisdn'].unique().to_frame(name='msisdn')\n\n    # Last month\n    tx_next_first_purchase = target.groupby('msisdn').cdr_date.min().reset_index()\n    tx_next_first_purchase.columns = ['msisdn', 'MinPurchaseDate']\n\n    # Two-month data\n    tx_last_purchase = source.groupby('msisdn').cdr_date.max().reset_index()\n    tx_last_purchase.columns = ['msisdn', 'MaxPurchaseDate']\n\n    # Merge two data\n    tx_purchase_dates = merge(tx_last_purchase, tx_next_first_purchase, on='msisdn', how='left')\n\n    # Calculating difference\n    tx_purchase_dates['NextPurchaseDay'] = (tx_purchase_dates['MinPurchaseDate'] - tx_purchase_dates['MaxPurchaseDate']).dt.days\n\n    # Merge with tx_user\n    tx_user = merge(tx_user, tx_purchase_dates[['msisdn', 'NextPurchaseDay']], on='msisdn', how='left')\n    tx_user = tx_user.fillna(999)\n\n    # Create a Dask dataframe with msisdn and cdr_date\n    tx_day_order = source[['msisdn', 'cdr_date']].compute()\n\n    # Convert cdr_date to day\n    tx_day_order['InvoiceDay'] = tx_day_order['cdr_date'].dt.date\n    tx_day_order = tx_day_order.sort_values(['msisdn', 'cdr_date'])\n\n    # Drop duplicates\n    tx_day_order = tx_day_order.drop_duplicates(subset=['msisdn', 'cdr_date'], keep='first')\n\n    # Shifting last 3 purchase dates\n    tx_day_order['PrevInvoiceDate'] = tx_day_order.groupby('msisdn')['InvoiceDay'].shift(1)\n    tx_day_order['T2InvoiceDate'] = tx_day_order.groupby('msisdn')['InvoiceDay'].shift(2)\n    tx_day_order['T3InvoiceDate'] = tx_day_order.groupby('msisdn')['InvoiceDay'].shift(3)\n\n    tx_day_order['DayDiff'] = (tx_day_order['InvoiceDay'] - tx_day_order['PrevInvoiceDate']).dt.days\n    tx_day_order['DayDiff2'] = (tx_day_order['InvoiceDay'] - tx_day_order['T2InvoiceDate']).dt.days\n    tx_day_order['DayDiff3'] = (tx_day_order['InvoiceDay'] - tx_day_order['T3InvoiceDate']).dt.days\n    \n    \n\n    tx_day_diff = tx_day_order.groupby('msisdn').agg({'DayDiff': ['mean', 'std']}).reset_index()\n    tx_day_diff.columns = ['msisdn', 'DayDiffMean', 'DayDiffStd']\n\n    tx_day_order_last = tx_day_order.drop_duplicates(subset=['msisdn'], keep='last')\n    tx_day_order_last = tx_day_order_last.fillna(0)\n\n    tx_day_order_last = merge(tx_day_order_last, tx_day_diff, on='msisdn')\n    tx_user = merge(tx_user, tx_day_order_last[['msisdn','DayDiff', 'DayDiff2','DayDiff3','DayDiffMean', 'DayDiffStd']], on='msisdn')\n    \n    \n    \n    #create tx_class as a copy of tx_user before applying get_dummies\n    tx_class = tx_user.copy()\n\n    tx_class = NextPurchaseDayRange(tx_class)   \n\n    \n    rfm = rfm_process_quantile_method_purchase(source)\n\n    rfm = rfm.compute()\n    final= merge(rfm,tx_class,how='inner',on='msisdn')    \n    \n    final = final.categorize(columns=['Segment'])\n    final = dd.get_dummies(final, columns=['Segment'])\n    \n    path = os.path.join(file_path, filename)   \n    Path(path).mkdir(parents=True, exist_ok=True)\n    final.to_csv(path)\n    \n    \n    return final.compute()\n\n\ndef NextPurchaseDayRange(tx_class):\n    \n    tx_class['NextPurchaseDayRange'] = 0\n\n    # Use Dask function 'where' to perform conditional assignments\n    tx_class['NextPurchaseDayRange'] = tx_class['NextPurchaseDayRange'].where(\n        ~((tx_class['NextPurchaseDay'] > 7) & (tx_class['NextPurchaseDay'] <= 14)), 1\n    )\n    tx_class['NextPurchaseDayRange'] = tx_class['NextPurchaseDayRange'].where(\n        ~((tx_class['NextPurchaseDay'] > 14) & (tx_class['NextPurchaseDay'] <= 30)), 2\n    )\n    tx_class['NextPurchaseDayRange'] = tx_class['NextPurchaseDayRange'].where(\n        ~(tx_class['NextPurchaseDay'] > 30), 3\n    )\n\n    tx_class = tx_class.fillna(0)\n    \n    return tx_class\n\n\ndef rfm_process_quantile_method_purchase(df_data):\n    try: \n        \n        df_data = df_data.fillna(0, )\n\n        print(len(df_data))\n        df_data = df_data.rename(columns={RECHARGE_TRANSACTION_PURCHASE_DATE_NAME: \"purchase_date\"})\n        ctm_class = perform_rfm_on_purchase(df_data, period=95)\n\n        # # print(len(ctm_class))\n        \n        ctm_class = form_segements_purchase_rfm(ctm_class)\n\n        # print(len(ctm_class))\n        \n        return ctm_class\n\n    except Exception as e:\n        print(e) \n\n\n\n\n\n\n\ndef perform_rfm_on_purchase(df_data_rfm, period=95):\n\n    # ic(\"inside perform_rfm\")\n    \n    df_data_rfm.purchase_date = dd.to_datetime(df_data_rfm.purchase_date)\n    df_data_rfm = df_data_rfm.fillna(0)\n    msisdn_name = MSISDN_COL_NAME\n    \n    \n    \n    # current behaviour data\n    min_date = df_data_rfm['purchase_date'].min().compute()\n    cur_beh_data_rfm = df_data_rfm[(df_data_rfm.purchase_date <= pd.Timestamp(min_date) + pd.Timedelta(days=period))\n                                  & (df_data_rfm.purchase_date >= pd.Timestamp(min_date))].reset_index(drop=True)\n    # Get the maximum purchase date of each customer and create a dataframe with it together with the customer's id.\n    ctm_max_purchase = cur_beh_data_rfm.groupby(msisdn_name).purchase_date.max().reset_index()\n    ctm_max_purchase.columns = [msisdn_name, 'MaxPurchaseDate']\n\n    # Find the recency of each customer in days\n    ctm_max_purchase['Recency'] = (\n            ctm_max_purchase['MaxPurchaseDate'].max() - ctm_max_purchase['MaxPurchaseDate']).dt.days\n    ctm_max_purchase = ctm_max_purchase.drop(columns=['MaxPurchaseDate'])\n    \n    \n    \n    print(\"done with recency \")\n    # frequency\n    ctm_frequency = cur_beh_data_rfm.groupby(msisdn_name).total_cnt.sum().reset_index()\n    ctm_frequency.columns = [msisdn_name, 'Frequency']\n    \n    \n    \n    print(\"done with frequency \")\n    cur_beh_data_rfm['Revenue'] = cur_beh_data_rfm[RECHARGE_TRANSACTION_PRICE_COL_NAME]\n    ctm_revenue = cur_beh_data_rfm.groupby('msisdn').Revenue.sum().reset_index()\n    \n    \n    print(\"done with monitory \")\n    rfm_data_base = dd.concat([ctm_max_purchase, ctm_revenue, ctm_frequency], axis=1)\n    rfm_data_base = rfm_data_base.loc[:, ~rfm_data_base.columns.duplicated()]\n\n    #for tnm purpose start\n    #rfm_data_base=otliner_removal(rfm_data_base,'Frequency')\n    rfm_data_base=otliner_removal(rfm_data_base,'Revenue')\n    \n    #for tnm purpose end \n    return rfm_data_base\n    \ndef otliner_removal(df,col, per=0.97):\n    q = df[col].quantile(per)\n    print(f\"col is {col} and q is {q}\")\n    print(\"the length brfore is\", len(df))\n    outliers = df[df[col] > q]\n    df1 = df[~df['msisdn'].isin(outliers['msisdn'].compute().values)]\n    print(\"the length after is\", len(df1))\n    return df1    \n\n\n\n\n\n\n\n\n\n\ndef form_segements_purchase_rfm(ctm_class):\n    def process_duplicates(li):\n        for i in range(len(li) - 1):\n            print(li[i], i)\n            if i + 1 == len(li) - 1:\n                li[i] = ((li[i - 1] + li[i]) / 2)\n            elif li[i] == li[i + 1]:\n                li[i + 1] = ((li[i + 1] + li[i + 2]) / 2)\n        return li\n\n    # bins_recency = [-1,\n    #                 np.percentile(ctm_class[\"Recency\"], 20),\n    #                 np.percentile(ctm_class[\"Recency\"], 40),\n    #                 np.percentile(ctm_class[\"Recency\"], 60),\n    #                 np.percentile(ctm_class[\"Recency\"], 80),\n    #                 ctm_class[\"Recency\"].max().compute()]\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n    # # bins_recency = [-1,17,35,53,71, ctm_class[\"Recency\"].max().compute()]\n\n    # bins_recency.sort()\n    # bins_recency = process_duplicates(bins_recency)\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Recency']=ctm_class1['Recency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Recency').agg({'msisdn':'count'}).reset_index()\n    mean_recency = ctm_class1['Recency'].max().compute()/5\n    print(\"mean_recency\", mean_recency)\n    print(\"mean_recency *2\",mean_recency * 2)\n    print(\"mean_recency *3\",mean_recency * 3)\n    print(\"mean_recency *4\",mean_recency * 4)\n    \n    \n    bins_recency = [-1,\n                    int(mean_recency),\n                    int(mean_recency * 2),\n                    int(mean_recency * 3),\n                    int(mean_recency * 4),\n                    ctm_class[\"Recency\"].max().compute()]\n    \n    print('bins_recency is', bins_recency)\n\n\n    ctm_class['R_Score'] = ctm_class[\"Recency\"].map_partitions(pd.cut,\n                                                               bins=bins_recency,\n                                                               labels=[5, 4, 3, 2, 1]).astype(\"int\")\n\n    \n\n    # bins_frequency = [-1,\n    #                   np.percentile(ctm_class[\"Frequency\"], 20),\n    #                   np.percentile(ctm_class[\"Frequency\"], 40),\n    #                   np.percentile(ctm_class[\"Frequency\"], 60),\n    #                   np.percentile(ctm_class[\"Frequency\"], 80),\n    #                   ctm_class[\"Frequency\"].max().compute()]\n\n    # bins_frequency.sort()\n    # bins_frequency = process_duplicates(bins_frequency)\n\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Frequency']=ctm_class1['Frequency'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Frequency').agg({'msisdn':'count'}).reset_index()\n    mean_frequency = ctm_class1['Frequency'].max().compute()/5\n    print(\"mean_frequency\", mean_frequency)\n    print(\"mean_frequency *2\",mean_frequency * 2)\n    print(\"mean_frequency *3\",mean_frequency * 3)\n    print(\"mean_frequency *4\",mean_frequency * 4)\n\n\n    bins_frequency = [-1,\n                    int(mean_frequency),\n                    int(mean_frequency * 2),\n                    int(mean_frequency * 3),\n                    int(mean_frequency * 4),\n                    ctm_class[\"Frequency\"].max().compute()]\n    \n    print('bins_frequency is', bins_frequency)\n    ctm_class['F_Score'] = ctm_class[\"Frequency\"].map_partitions(pd.cut,\n                                                                 bins=bins_frequency,\n                                                                 labels=[1, 2, 3, 4, 5]).astype(\"int\")\n\n    # bins_revenue = [-1,\n    #                 np.percentile(ctm_class[\"Revenue\"], 20),\n    #                 np.percentile(ctm_class[\"Revenue\"], 40),\n    #                 np.percentile(ctm_class[\"Revenue\"], 60),\n    #                 np.percentile(ctm_class[\"Revenue\"], 80),\n    #                 ctm_class[\"Revenue\"].max().compute()]\n    # bins_revenue.sort()\n    # bins_revenue = process_duplicates(bins_revenue)\n\n    # bins_revenue = [-1,189,380,571,762, ctm_class[\"Revenue\"].max().compute()]\n\n    ctm_class1=ctm_class.copy()\n    ctm_class1['Revenue'] = ctm_class1['Revenue'].map_partitions(lambda series: series.apply(np.ceil))\n    ctm_class1['Revenue']=ctm_class1['Revenue'].astype('int')\n    ctm_class1 = ctm_class1.groupby('Revenue').agg({'msisdn':'count'}).reset_index()\n    mean_revenue = ctm_class1['Revenue'].max().compute()/5\n    \n    print(\"mean_revenue\", mean_revenue)\n    print(\"mean_revenue *2\",mean_revenue * 2)\n    print(\"mean_revenue *3\",mean_revenue * 3)\n    print(\"mean_revenue *4\",mean_revenue * 4)\n    \n    bins_revenue = [-1,\n                    int(mean_revenue),\n                    int(mean_revenue * 2),\n                    int(mean_revenue * 3),\n                    int(mean_revenue * 4),\n                    ctm_class[\"Revenue\"].max().compute()]\n    \n    print('bins_revenue is', bins_revenue)\n\n\n    ctm_class['M_Score'] = ctm_class[\"Revenue\"].map_partitions(pd.cut,\n                                                              bins=bins_revenue,\n                                                              labels=[1, 2, 3, 4, 5]).astype(\"int\")\n    print(\"done with scoring\")\n    # Form RFM segment\n\n    ctm_class['RFM_Segment'] = ctm_class.apply(join_rfm, axis=1)\n    ctm_class['RFM_Segment'] = ctm_class['RFM_Segment'].astype(int)\n    print(\"formed rfm segement \")\n    ctm_class['R_Score'] = ctm_class['R_Score'].astype(int)\n    ctm_class['F_Score'] = ctm_class['F_Score'].astype(int)\n    ctm_class['M_Score'] = ctm_class['M_Score'].astype(int)\n    print(\"computing rfm\")\n    r = ctm_class['R_Score'].values.compute()\n    f = ctm_class['F_Score'].values.compute()\n    m = ctm_class['M_Score'].values.compute()\n    seg = segmentaion_fun1(r, f, m)\n    chunks = ctm_class.map_partitions(lambda x: len(x)).compute().to_numpy()\n    myarray = da.from_array(seg, chunks=tuple(chunks))\n    ctm_class['Segment'] = myarray\n    return ctm_class\n    \n    \n    \n\n\ndef join_rfm(x):\n    return str(int(x['R_Score'])) + str(int(x['F_Score'])) + str(int(x['M_Score']))\n\n\ndef segmentaion_fun1(r, f, m):\n    segments = []\n    for r, f, m in zip(r, f, m):\n        value = int(f\"{r}{f}{m}\")\n\n        if value in [555, 554, 544, 545, 454, 455, 445]:\n            segments.append(\"Champions\")\n        elif value in [543, 444, 435, 355, 354, 345, 344, 335]:\n            segments.append(\"Loyal_Customers\")\n        elif value in [553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323]:\n            segments.append(\"Potential_Loyalist\")\n        elif value in [512, 511, 422, 421, 412, 411, 311]:\n            segments.append(\"Recent_Customers\")\n        elif value in [525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313]:\n            segments.append(\"Promising_Customers\")\n        elif value in [535, 534, 443, 434, 343, 334, 325, 324]:\n            segments.append(\"Customers_needing_Attention\")\n        elif value in [331, 321, 312, 221, 213]:\n            segments.append(\"About_to_Sleep\")\n        elif value in [255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124]:\n            segments.append(\"At_Risk\")\n        elif value in [155, 154, 144, 214, 215, 115, 114, 113]:\n            segments.append(\"Cant_Loose_them\")\n        elif value in [332, 322, 231, 241, 251, 233, 232, 223, 222, 132, 123, 122, 212, 211]:\n            segments.append(\"Hibernating\")\n        else:\n            segments.append(\"Lost\")\n\n    return segments\n    \n    \n    \n    \n    \n    \n    \n    \n    \ndef xgmodel(final, name, dag_run_id):\n\n\n\n    X, y = final.drop(['NextPurchaseDayRange','msisdn','NextPurchaseDay'],axis=1), final.NextPurchaseDayRange\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)\n    \n    \n    path = os.path.join(file_path, name+\"_feature.pkl\")\n    pickle.dump(X.columns.tolist(), open(path, 'wb'))\n        \n    \n    rus = RandomUnderSampler(random_state=42)\n    X_train, y_train = rus.fit_resample(X, y)\n    \n\n    xgb_model = xgb.XGBClassifier().fit(X_train, y_train)\n    \n    # print('Accuracy of XGB classifier on training set: {:.2f}'\n    #       .format(xgb_model.score(X_train, y_train)))\n    # print('Accuracy of XGB classifier on test set: {:.2f}'\n    #       .format(xgb_model.score(X_test[X_train.columns], y_test)))\n\n\n    y_pred = xgb_model.predict(X_test)\n    # print(classification_report(y_test, y_pred))\n \n    \n    path = os.path.join(file_path, name+\".pkl\")    \n    pickle.dump(xgb_model, open(path, 'wb'))\n    # print('model created ')\n    \n    \n    \n    \n    \n    \ndef nextthreemonth(purchase, dag_run_id):\n\n\n    fin = dd.read_csv(os.path.join(file_path, \"APA_output.csv\"))\n    li = fin['recommended_product_id'].unique().compute().tolist()\n\n    file_name_dict = get_file_names()\n\n    pack = dd.read_csv(os.path.join(file_path, file_name_dict.get(\"pack\").get('pack')),usecols= pack_cols)#packinfo loading\n    \n    source1 = dd.concat([purchase['p2'], purchase['p3'], purchase['p4']], axis=0, ignore_index=True, verify_integrity=False)\n    target1 = dd.concat([purchase['p5']])\n    \n    \n    result = pd.DataFrame({'msisdn': [], 'PredictedValues': [],'recommended_product_id': [], 'product_type': []})\n    result = dd.from_pandas(result, npartitions=2)\n\n    for i in li:\n        \n        msi = fin[fin['recommended_product_id'] == i][['msisdn']]                \n        \n        source =  merge(source1,msi,how='inner',on='msisdn').drop('Unnamed: 0', axis=1)\n        target =  merge(target1,msi,how='inner',on='msisdn').drop('Unnamed: 0', axis=1)\n        \n        if len(source)<50   or len(target)<50:\n            continue     \n            \n    # #     print('len source ',len(source))\n    # #     print('source.column ',source.columns)\n    # #     print('len target ',len(target))\n    # #     print('target.column ',target.columns)\n        \n        final = nextpurchase(source,target,'file',dag_run_id)\n\n        x, x1 = final.drop(['NextPurchaseDayRange', 'msisdn', 'NextPurchaseDay'], axis=1), final['msisdn']\n        x, x1 = final.drop(['NextPurchaseDayRange', 'msisdn', 'NextPurchaseDay'], axis=1), final['msisdn']   \n        \n        # x = final.drop(['NextPurchaseDayRange', 'msisdn', 'NextPurchaseDay'], axis=1)\n             \n\n        c = pack[pack['product_id'] == i]['product_type'].compute().tolist()           \n            \n            \n        feature_path = os.path.join(file_path, c[0]+\"_feature.pkl\")  \n        columns = pickle.load(open(feature_path, 'rb'))\n            \n        output_list = [j for j in columns if j not in x.columns]\n            \n        for k in output_list:\n            x[k]=0 \n            \n        pickel_path = os.path.join(file_path ,c[0]+\".pkl\")\n        loaded_model = pickle.load(open(pickel_path, 'rb'))\n        feature_names = loaded_model.get_booster().feature_names\n        x=x[feature_names]\n            \n        y = loaded_model.predict(x)    \n        y_df = dd.from_array(y, columns=['PredictedValues'])\n\n        predict = dd.concat([x1, y_df], axis=1)\n        predict['recommended_product_id'] = i\n        predict['product_type'] = c[0]\n\n        result = dd.concat([result, predict], axis=0)            \n    \n    result1 = result.compute()  \n    path = os.path.join(file_path, 'predicted_result1.csv') \n    result1.to_csv(path)\n    \n    return result1\n    \n\ndef recommended_id_prediction(dag_run_id):\n    \n    path = os.path.join(file_path, 'predicted_result1.csv') \n    s = pd.read_csv(path)\n    \n    def transform_value(value):\n        \n        if value ==0:\n            return '0-7'\n        elif value==1:\n            return '7-14'\n        elif value ==2:\n            return '14-30'\n        else:\n            return '30+'\n\n    # Apply the transformation using the apply function\n    s['PredictedValues'] = s['PredictedValues'].fillna(0)\n    s['PredictedValues'] = s['PredictedValues'].astype(int)\n    s['range'] = s['PredictedValues'].apply(transform_value)\n    \n    result = s.groupby(['recommended_product_id', 'PredictedValues']).size().reset_index(name='PredictedValues_counts')\n    max_counts = result.groupby('recommended_product_id')['PredictedValues_counts'].idxmax()\n    result = result.loc[max_counts]\n    result['range'] = result['PredictedValues'].apply(transform_value)\n\n    path = os.path.join(file_path, 'recommended_id_prediction.csv')   \n    result.to_csv(path,index=False,header=True)    \n    return result\n    \n\n\ndef weekday_product_purchase_count(dag_run_id):\n    \n    msisdn_name =  MSISDN_COL_NAME\n    purchase_no_months = ['m1', 'm2', 'm3']\n    file_name_dict = get_file_names()\n    print(file_name_dict)\n    print(\"finding weekday_puchase_count ongoing \")\n    purchase = {}\n    \n    for month in purchase_no_months:\n        purchase[month] = dd.read_csv(os.path.join(file_path, file_name_dict.get(\"purchase\").get(month)),)\n\n    \n    df_list = [purchase['m1'],purchase['m2'],purchase['m3']]\n    pivoted_dict = {}\n    for i, df in enumerate(df_list, 1):\n        df['cdr_date'] = dd.to_datetime(df['cdr_date'])  \n        df = df[df['cdr_date'].dt.dayofweek < 7]\n        df['day_of_week'] = df['cdr_date'].dt.day_name()\n        df['day_of_week'] = df['day_of_week'].astype('category').cat.as_known()\n        pivoted = df.pivot_table(index='product_id', columns='day_of_week', values='total_cnt', aggfunc='sum')\n        pivoted_dict[f\"m{i}\"] = pivoted\n    pivot_tables = [pivoted_dict['m1'],pivoted_dict['m2'], pivoted_dict['m3']]\n    \n    merged_pivot = reduce(lambda left, right: dd.merge(left, right, on='product_id', how='outer'), pivot_tables)\n    merged_pivot = merged_pivot.fillna(0)\n    \n    columns_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    \n    for day in columns_order:\n        merged_pivot[f\"{day.lower()}\"] = merged_pivot[f\"{day}_x\"] + merged_pivot[f\"{day}_y\"] + merged_pivot[day]\n    columns  = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\n    merged_pivot['total'] = merged_pivot[columns].sum(axis=1)\n    \n    merged_pivot = merged_pivot.sort_values(by='total', ascending=False)\n    \n    merged_pivot = merged_pivot.drop(columns=['Monday_x', 'Tuesday_x', 'Wednesday_x', 'Thursday_x', 'Friday_x', 'Saturday_x', 'Sunday_x',\n                           'Monday_y', 'Tuesday_y', 'Wednesday_y', 'Thursday_y', 'Friday_y', 'Saturday_y', 'Sunday_y',\n                           'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n    merged_pivot['top_purchased_day_1'] = merged_pivot[columns].idxmax(axis=1)\n    merged_pivot['top_purchased_day_2'] = merged_pivot[columns].apply(lambda x: x.nlargest(2).index[1], axis=1)\n    merged_pivot['top_purchased_day_3'] = merged_pivot[columns].apply(lambda x: x.nlargest(3).index[2], axis=1)\n    merged_pivot = merged_pivot.reset_index()\n\n    merged_pivot_pd =merged_pivot.compute()\n    \n    weekday_product_purchase_count_path = os.path.join(file_path, \"weekday_product_purchase_count.csv\")\n    \n    print(\"weekday_product_purchase_count   file output is ongoing \")\n    merged_pivot_pd.to_csv(weekday_product_purchase_count_path,header=True,index=False)"
              }
            }, {
              "id": "77150d04-ffa4-4e20-b24b-4dd3b2cf2a03",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nimport numpy as np\r\nimport os\r\nimport pickle\r\n\r\n\r\ndef transform(dataframe):\r\n    dataframe = dataframe.toPandas()\r\n    file_path = '/home/tnmops/seahorse3_bkp/'\r\n    dag_run_id = \"manual__2023-07-10T11:06:51\"    \r\n    \r\n    path_d = os.path.join(file_path, \"dict.pickle\")\r\n    features_path = os.path.join(file_path, \"features.pickle\")\r\n    data = load_pickle_file(path_d)\r\n    data_feature = load_pickle_file(features_path)\r\n\r\n    re = RuleExtraction(dag_run_id=dag_run_id, features_path=features_path, path_d=path_d)\r\n    re.execute()\r\n    return dataframe\r\n\r\ndef load_pickle_file(filename):\r\n    with open(filename, 'rb') as handle:\r\n        data = pickle.load(handle)\r\n    return data\r\n\r\n\r\nclass RuleExtraction(object):\r\n    def __init__(self, dag_run_id=None, features_path=None, path_d=None, db=None):\r\n        self.path_d = path_d\r\n        self.dag_run_id = dag_run_id\r\n        self.features_path = features_path\r\n        self.db = db\r\n\r\n        # going to load these variables\r\n        self.data = None\r\n        self.filtered_dict = None\r\n        self.data_feature = None\r\n        # loading\r\n\r\n        self.load_pickle()\r\n        self.filter_dict()\r\n\r\n    def load_pickle(self):\r\n        self.data = load_pickle_file(self.path_d)\r\n        self.data_feature = load_pickle_file(self.features_path)\r\n\r\n    def filter_dict(self):\r\n        # needed_segments = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\r\n        # self.filtered_dict = {k: v for k, v in self.data.items() if k in needed_segments}\r\n        self.filtered_dict = {k: v for k, v in self.data.items()}\r\n\r\n    def execute(self):\r\n        c=0\r\n        # info = pd.DataFrame(columns=['dag_run_id', 'segment_name', 'segment_length', 'customer_status', 'cluster_description', 'cluster_no', 'samples'])\r\n        \r\n        infod = {'segment_name':[], 'segment_length':[], 'customer_status':[], 'cluster_description':[], 'cluster_no':[], 'samples':[]}\r\n        info = pd.DataFrame(infod)\r\n        \r\n        file_path = '/home/tnmops/seahorse3_bkp/'\r\n\r\n        for item, val in self.filtered_dict.items():\r\n            # val is the path, item is the segment name\r\n            df = pd.read_csv(val)\r\n            print(f\"the cluster is {item}\")\r\n            cluster_conditions = extract_rules(df, self.data_feature[item])\r\n            print('cluster_conditions is ', cluster_conditions)\r\n            if cluster_conditions is None:\r\n                print(f\"the segment {item} is none, the path is {val}\")\r\n            for cluster, rule in cluster_conditions.items():\r\n                try:\r\n                    \r\n                    # info.loc[c] = [self.dag_run_id, f\"{item}-{str(cluster)}\", str(len(df)), \"active\", rule, int(cluster), 15 if len(rule) > 1 else 0]\r\n                    infod['dag_run_id'] = self.dag_run_id\r\n                    infod['segment_name'] = f\"{item}-{str(cluster)}\"\r\n                    infod['segment_length']=str(len(df))\r\n                    infod['customer_status']=\"active\"\r\n                    infod['cluster_description']=rule\r\n                    infod['cluster_no']=int(cluster)\r\n                    infod['samples']=15 if len(rule) > 1 else 0\r\n                    \r\n                    info = info.append(infod,ignore_index=True)\r\n                    \r\n                    # SegmentRepo.create(db=self.db, segment=info)  # Assuming `SegmentRepo` has a `create` method\r\n                except Exception as e:\r\n                    print(\"error occurred in rule insertion\")\r\n                    print(e)\r\n                    raise ValueError(e)\r\n                    c=c+1\r\n        \r\n        \r\n        rule_path = os.path.join(file_path, \"rule.csv\")\r\n\r\n        info.to_csv(rule_path, index=False)\r\n\r\n\r\ndef extract_rules(df, features):\r\n    features.append('label')    \r\n    r1 = df[features]\r\n        \r\n    cluster_conditions = {}\r\n    for c in r1['label'].unique(): \r\n        op_li = []\r\n        cluster_conditions[c] = [] \r\n        r = r1[r1['label'] == c]        \r\n        for i in r.columns:\r\n            if i != 'label':                \r\n                if r[i].min() == r[i].max():\r\n                    str1 = i + '==' + str(r[i].min())\r\n                else:\r\n                    str1 = i + '>=' + str(r[i].min()) + ' & ' + i + '<=' + str(r[i].max()) \r\n                \r\n            op_li.append(str1)       \r\n            condition = \" & \".join(op_li)            \r\n            cluster_conditions[c] = condition\r\n    return cluster_conditions\r\n"
              }
            }, {
              "id": "cd27063b-f4c3-cd42-76b1-b08804b1baff",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from datetime import datetime, timedelta,date\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nfrom sklearn.svm import SVC\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport dask.array as da\nfrom dask.dataframe import merge\nimport pickle\nimport pathlib\nimport dask.dataframe as dd\n\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m3','m2']\ndag_run_id = 'manual__2023-07-11T10:20:22'\nfile_path = '/home/tnmops/seahorse3_bkp/'\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_jan.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_march.csv\"},\n            \n        \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan.csv\",\n            \"c4\": \"campaign_detailed_fct_dec.csv\"            \n                        \n        }\n        }\n        \n        \n        \ndef transform(dataframe):\n    \n        c=0\n        file_name_dict = get_file_names()\n\n        print('inside campaign_response_model started')\n        pack_info = pd.read_csv(os.path.join(file_path, \"pack_info.csv\"))\n        for i, j in zip(campaign_df, campaign_usage):\n        \n            jan = pd.read_csv(os.path.join(file_path, file_name_dict.get(\"campaign_data\").get(i)))    \n            usage_jan= pd.read_csv(os.path.join(file_path, file_name_dict.get(\"usage\").get(j)))\n            jan_filter = jan[['msisdn','action_group','TG_Response','CG_Response','Product_Promoted']]\n            jan_filter.dropna(inplace = True)\n\n\n\n            jan_filter.loc[:, 'TG_Response'] = np.where(jan_filter['TG_Response']>0 , 1, jan_filter['TG_Response'])\n            jan_filter.loc[:, 'CG_Response'] = np.where(jan_filter['CG_Response']>0 , 1, jan_filter['CG_Response'])\n\n\n            # Separate DataFrame based on 'action_group'\n            df_test = jan_filter[jan_filter['action_group'] == 'TEST_GROUP']\n            df_control = jan_filter[jan_filter['action_group'] == 'CONTROL_GROUP']\n\n\n            df_test['response'] = df_test['TG_Response'] \n            df_control['response'] = df_control['CG_Response'] \n            print('adding to  table')\n            if c==0:        \n                model_fit(df_test,df_control, usage_jan, dag_run_id)        \n                c=1\n\n            else:\n                x = model_predict(df_test,df_control,usage_jan, dag_run_id)\n        \n        campaign_response_model_predicted=pd.read_csv(os.path.join(file_path,'campaign_response_model_predicted.csv'))\n        # segements_two = SegementRepo.findByAutoPilotId(db=db, _id=dag_run_id)\n        rule_path = os.path.join(file_path, \"rule.csv\")\n        db = pd.read_csv(rule_path)\n        \n        for index, seg in db.iterrows():            \n            item = seg.segment_name\n            segements_data = item.split(\"-\")\n            trent = segements_data[0]\n            rfm = segements_data[1]\n            service = segements_data[2]\n            filename = trent+\"-\"+rfm+\"-\"+service+\".csv\"\n            \n            \n            \n            print('filename is ',filename)\n            segment_df_path = os.path.join(file_path, filename)\n            segment_df = pd.read_csv(segment_df_path)\n            campaign_response_model_predicted1=campaign_response_model_predicted[campaign_response_model_predicted['msisdn'].isin(segment_df['msisdn'])]\n            \n            print('len(campaign_response_model_predicted1)',len(campaign_response_model_predicted1))\n            print(\"len(usage_jan)\",len(usage_jan))\n            if(len(campaign_response_model_predicted1) == 0 or campaign_response_model_predicted1 is None ):\n                perc=0\n                print(f\"for item{item} the campaign_response_percentage is {perc}\")\n            else:\n                perc = calculate_percentage_of_ones(list(campaign_response_model_predicted1['y_pred_full']))\n                print(f\"for item{item} the campaign_response_percentage is {perc}\")\n            \n            # seg.campaign_response_percentage=perc\n            # campaign_type = seg.campaign_type\n            # if campaign_type == \"upsell\" or campaign_type == \"crossell\":\n            #     current_product_id = seg.current_product\n            #     rec_product_id = int(seg.recommended_product_id)\n            #     current_product_price = GetCurrentPackprice(current_product_id, pack_info)\n            #     reco_product_price = getpackprice(rec_product_id,pack_info)\n            #     prodd_diff= reco_product_price-current_product_price\n\n            #     actual_target_count = seg.actual_target_count\n            #     total_revenue = seg.total_revenue\n            #     uplift_revenue= seg.uplift_revenue\n            #     current_arpu=  total_revenue/actual_target_count\n            #     seg.current_arpu =current_arpu\n            #     predicted_arpu = current_arpu+uplift_revenue \n            #     seg.predicted_arpu = predicted_arpu\n            #     seg.incremental_revenue =prodd_diff *(actual_target_count*perc/100)\n            #     seg.uplift = (predicted_arpu-current_arpu)/current_arpu\n            \n#             SegementRepo.update(db=db, item_data=seg)\n\n        return segment_df\n\n    \n    \n    \n    \n    \n    \ndef calculate_percentage_of_ones(lst):\n\n    print('type of  lst is', type(lst))\n    total_count = len(lst)\n    ones_count = lst.count(1)\n    percentage = (ones_count / total_count) * 100\n    return percentage   \n    \n    \ndef model_fit(df_test, df_control, usage_jan, dag_run_id):    \n    \n    # Perform undersampling within 'TEST_GROUP' only\n    df_test_under = df_test[df_test['TG_Response'] == 0].sample(int(df_test['TG_Response'].sum()), random_state=42)\n\n    # Concatenate undersampled class 0 DataFrame with original class 1 DataFrame within 'TEST_GROUP'\n    df_test_under = pd.concat([df_test_under, df_test[df_test['TG_Response'] == 1]], axis=0)\n\n    # Combine the undersampled 'TEST_GROUP' DataFrame with the original 'CONTROL_GROUP' DataFrame\n    \n    df_under = pd.concat([df_test_under, df_control], axis=0)  \n   \n    \n    df_under.drop(['TG_Response','CG_Response'], inplace = True,axis =1)\n    \n    df_under_test_group = df_under[df_under[\"action_group\"] == \"TEST_GROUP\"]\n    \n    final_all_groups = pd.merge(usage_jan,df_under,how='inner',on='msisdn')\n    final = pd.merge(usage_jan,df_under_test_group,how='inner',on='msisdn')\n    \n    final.fillna(0,inplace=True)\n    final_all_groups.fillna(0,inplace=True)   \n    \n    cat_col = final.select_dtypes(include =['object','category'] ).columns.to_list()\n    msisdn = final.pop(\"msisdn\")\n    groups_test = final['action_group']\n    final.drop(columns=cat_col, inplace = True)\n    \n    msisdn_all = final_all_groups.pop(\"msisdn\")\n    groups = final_all_groups['action_group']\n    final_all_groups.drop(columns=cat_col, inplace = True)\n    \n    \n    xgboost_model(final,dag_run_id)   \n    \n    #full data prediction \n    X_full=final_all_groups.drop(['response'],axis=1)\n    y_full=final_all_groups['response']    \n    \n    \n    path = os.path.join(file_path, \"campaign_xgmodel.pickle\")\n    xgb_model = pickle.load(open(path, \"rb\"))\n    \n    \n    y_pred_prob_full = xgb_model.predict_proba(X_full)\n    X_full['predicted_prob'] =  y_pred_prob_full[:,1]\n    \n    \n    X_full['action_group']  =  groups\n    X_full['msisdn']  =  msisdn_all\n    X_full['response'] = y_full\n    \n    uplift_df = uplift_data(X_full)    \n    \n    df_product = X_full[X_full['Product_Promoted'] == 5.0]    \n    uplift_df['Difference_Percentage'] = ((uplift_df['Predicted_Uplift'] - uplift_df['Real_Uplift']) / uplift_df['Real_Uplift']) * 100\n    \n    path = os.path.join(file_path, \"campaign_uplift_model.csv\")\n    uplift_df.to_csv(path, index=False)\n    \n    \n    \n    \n\n\ndef xgboost_model(final,dag_run_id):\n    \n    X=final.drop(['response'],axis=1)\n    y=final['response']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\n    \n    xgb_model = xgb.XGBClassifier().fit(X_train, y_train)\n    \n       \n    \n    path = os.path.join(file_path, \"campaign_xgmodel.pickle\") \n\n    pickle.dump(xgb_model, open(path, 'wb'))\n    \n    \n    print('Accuracy of XGB classifier on training set: {:.2f}'\n           .format(xgb_model.score(X_train, y_train)))\n    print('Accuracy of XGB classifier on test set: {:.2f}'\n           .format(xgb_model.score(X_test[X_train.columns], y_test)))\n    \n    y_pred = xgb_model.predict(X_test)\n    \n    print(classification_report(y_test, y_pred))\n    \n    \n    \n    \n    \n    \ndef uplift_data(X_full):\n    # Initialize lists to store results\n    products = []\n    real_uplifts = []\n    predicted_uplifts = []\n\n    # Get unique products\n    unique_products = X_full['Product_Promoted'].unique()\n\n    # Calculate real and predicted uplift for each product\n    for product in unique_products:\n        # Get data for this product\n        df_product = X_full[X_full['Product_Promoted'] == product]\n\n        # Calculate real uplift\n        real_uplift = df_product[df_product['action_group'] == 'TEST_GROUP']['response'].mean() - df_product[df_product['action_group'] == 'CONTROL_GROUP']['response'].mean()\n\n        # Calculate predicted uplift\n        predicted_uplift = df_product[df_product['action_group'] == 'TEST_GROUP']['predicted_prob'].mean() - df_product[df_product['action_group'] == 'CONTROL_GROUP']['predicted_prob'].mean()\n\n        # Append results\n        products.append(product)\n        real_uplifts.append(real_uplift)\n        predicted_uplifts.append(predicted_uplift)\n\n    # Store results in a DataFrame\n    uplift_df = pd.DataFrame({\n        'Product_Promoted': products,\n        'Real_Uplift': real_uplifts,\n        'Predicted_Uplift': predicted_uplifts\n    })\n    \n    return uplift_df\n    \n    \n    \n    \ndef model_predict(df_test,df_control,usage_jan, dag_run_id):\n    \n    df_under = pd.concat([df_test, df_control], axis=0)\n\n    df_under.drop(['TG_Response','CG_Response'], inplace = True,axis =1)\n\n    df_under =df_under[df_under['msisdn'].isin(usage_jan['msisdn'])]\n    df_under1 = df_under.sample(n=min(100000, len(df_under)))\n\n    final = pd.merge(usage_jan,df_under,how='inner',on='msisdn')\n\n\n\n    cat_col = final.select_dtypes(include =['object','category'] ).columns.to_list()\n    msisdn = final.pop(\"msisdn\")\n    groups_test = final['action_group']\n    final.drop(columns=cat_col, inplace = True)\n\n    # load model from file\n    path = os.path.join(file_path, \"campaign_xgmodel.pickle\")\n    xgb_model = pickle.load(open(path, \"rb\"))\n\n    X=final.drop(['response'],axis=1)\n    y=final['response']  \n    \n\n    y_pred_full = xgb_model.predict(X)\n    print(classification_report(y, y_pred_full))#without random classifier\n\n    X['y_pred_full']=list(y_pred_full)\n    X['msisdn']=list(msisdn)\n    X=X[['msisdn','y_pred_full']]\n\n    X.to_csv(os.path.join(file_path,'campaign_response_model_predicted.csv'),header=True,index=False)\n\n    return X"
              }
            }, {
              "id": "22ded90b-ab97-cd27-1e1b-10270de81f58",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nimport numpy as np\r\nimport pickle\r\nimport os\r\nfrom sklearn.cluster import KMeans\r\n\r\ndef transform(dataframe):\r\n    dataframe = dataframe.toPandas()\r\n    file_path = '/home/tnmops/seahorse3_bkp/'\r\n    dag_run_id = \"manual__2023-07-10T11:06:51\"\r\n    \r\n    path_d = os.path.join(file_path, \"dict.pickle\")\r\n    \r\n    with open(path_d, 'rb') as handle:\r\n        data = pickle.load(handle)\r\n    \r\n    filtered_dict = {k: v for k, v in data.items()}\r\n    data_list = []\r\n    \r\n    for item, val in filtered_dict.items():\r\n            # val is the path item is the segment name\r\n            df = pd.read_csv(val)\r\n            # print(f\"the cluster is {item}\")\r\n            val = perform_k_modes(df, val)\r\n            val['segement_name'] = item\r\n            data_list.append(val)\r\n\r\n    df_final = pd.concat(data_list)\r\n    file_path = '/home/tnmops/seahorse3_bkp/'\r\n\r\n    rfm_path = os.path.join(file_path, \"rfm.csv\")\r\n    rfm_dd = pd.read_csv(rfm_path)\r\n\r\n    df_final1=df_final[['msisdn','trend','segments','label']]\r\n    df_final1=pd.merge(df_final1,rfm_dd[['msisdn','Recency','monitery','Frequency','R_Score','F_Score','M_Score','value']],\r\n                               on = 'msisdn',how = 'left')\r\n\r\n\r\n    df_final1['dag_run_id']=str(dag_run_id)\r\n    df_final1.to_csv(os.path.join(file_path, \"rfm_segement_mode.csv\"), index=False, header=True)\r\n   \r\n   \r\n    df_final = df_final.groupby(['segement_name', 'label']).agg({\"msisdn\": \"count\"}).reset_index()\r\n    df_final['dag_run_id']=str(dag_run_id)\r\n   \r\n    # data_list.append(val)\r\n    # dataframe = perform_k_means(dataframe, dag_run_id)\r\n\r\n    # dataframe['segment_name'] = dataframe['trend'] + \"-\" + dataframe['segments'] + \"-\" + dataframe['service_band']\r\n\r\n    # df_final = dataframe.groupby(['segment_name', 'label']).agg({\"msisdn\": \"count\"}).reset_index()\r\n    # df_final['dag_run_id']=str(dag_run_id)\r\n    \r\n    return df_final\r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\ndef perform_k_modes(df, path):\r\n    try:\r\n        # K_modes_location = os.path.join(cfg.Config.ml_location, dag_run_id, \"kmodes\")\r\n        # Path(K_modes_location).mkdir(parents=True, exist_ok=True)\r\n        objList = df.select_dtypes(include=\"object\").columns\r\n        X = df[objList]\r\n        X = pd.get_dummies(X)\r\n\r\n        # elbow = find_k_kmodes(X=X)\r\n\r\n        km = KMeans(n_clusters=2)\r\n        km.fit(X)\r\n        X['label'] = km.labels_\r\n        df['label'] = km.labels_\r\n        if df['label'].unique() is None or df['label'][0] is None :\r\n             df['label'] =0\r\n\r\n        print(df['label'].value_counts())\r\n        df.to_csv(path, header=True, index=False)\r\n        print(f\"outputed to path {path}\")\r\n        # for label in df['label'].unique():\r\n        #     df_temp = df[df['label'] == label]\r\n        #     name_path = f\"{df_name}_cluster_{label}.csv\"\r\n        #     output_path = os.path.join(K_modes_location, dag_run_id, name_path)\r\n        #     name_list[name_path] = output_path\r\n        #     print(f\"the output path is \", output_path)\r\n        #     df_temp.to_csv(output_path, header=True, index=False)\r\n        #     print(f\"done exporting \")\r\n\r\n        return df\r\n    except Exception as e:\r\n        print(e)\r\n        \r\n        \r\ndef perform_k_means(df, dag_run_id):\r\n    objList = df.select_dtypes(include=\"object\").columns\r\n    X = df[objList]\r\n    X = pd.get_dummies(X)\r\n\r\n    km = KMeans(n_clusters=2)\r\n    km.fit(X)\r\n    X['label'] = km.labels_\r\n    df['label'] = km.labels_\r\n    if df['label'].unique() is None or df['label'][0] is None:\r\n        df['label'] = 0\r\n\r\n    return df"
              }
            }, {
              "id": "29640985-7273-3f93-ff88-eef6aaeb69f2",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nimport pickle\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport dask.dataframe as dd\n\n# def transform(dataframe):\n    \n#         dataframe = dataframe.toPandas()\n#         file_path = '/home/tnmops/seahorse3_bkp/'\n#         msisdn_name = 'msisdn'\n#         TRANSACTION_PRODUCT_NAME = 'product_id'\n#         MSISDN_COL_NAME = 'msisdn'\n#         threshold = 0.50\n         \n#         # purchase_filter1 = pd.DataFrame()  \n            \n#         pf_path = os.path.join(file_path,'pur_filter.csv')\n#         matrix_path = os.path.join(file_path, \"matrix.csv\")\n#         purchase_filter_path = os.path.join(file_path, \"purchased_for_association\")\n    \n#         Path(purchase_filter_path).mkdir(parents=True, exist_ok=True)\n#         purchase = pd.read_csv(pf_path)\n#         # matrix = pd.read_csv(matrix_path)\n        \n    \n#         path_d = os.path.join(file_path, \"dict.pickle\")\n#         with open(path_d, 'rb') as handle:\n#             data = pickle.load(handle)\n#         # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\n#         # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\n#         filtered_dict = {k: v for k, v in data.items()}\n#         result = {}\n#         purchase_list = []\n        \n        \n        \n#         for item, val in filtered_dict.items():\n#             # val is the path item is the segment name\n#             df = pd.read_csv(val)\n#             purchase_filter = purchase[purchase[msisdn_name].isin(df[msisdn_name])]\n           \n#             # get all the unique products  in the filtered purchase\n#             products = purchase_filter[TRANSACTION_PRODUCT_NAME].unique()\n#             purchase_list = []\n#             for product in products:\n#                 purchase_filter_one_product = purchase_filter[purchase_filter[TRANSACTION_PRODUCT_NAME] == product]\n#                 # read the mathix\n#                 product = str(product)\n#                 # matrix_check = pd.read_csv(matrix_path, nrows=2)\n#                 matrix_path = os.path.join(file_path, \"matrix.csv\")\n#                 matrix_check = pd.read_csv(matrix_path, nrows=2)\n                \n#                 if product not in matrix_check.columns:\n#                     continue\n                \n#                 matrix = pd.read_csv(matrix_path, usecols=[MSISDN_COL_NAME, product])\n#                 purchase_filter1 = purchase_filter_one_product.merge(matrix, on=msisdn_name, how='inner')\n#                 purchase_filter2 = purchase_filter1[purchase_filter1[product] > threshold]\n#                 purchase_filter2 = purchase_filter2.drop(columns=[product])\n#                 purchase_list.append(purchase_filter2)\n            \n#             # #20-4-23 change after manjus changes\n#             if purchase_list is None or len(purchase_list)==0:\n#                 result[item] = None\n#                 result_path = os.path.join(purchase_filter_path, 'dict.pickle')\n#                 with open(result_path, 'wb') as handle:\n#                     pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n#                 continue\n            \n            \n         \n#             # # #20-4-23 change after manjus changes\n\n#             purchase_final = pd.concat(purchase_list)\n\n\n#             op_path = os.path.join(pur  chase_filter_path, item + \".csv\")\n#             purchase_final.to_csv(op_path)\n#             result[item] = op_path\n#             result_path = os.path.join(purchase_filter_path, 'dict.pickle')\n#             with open(result_path, 'wb') as handle:\n#                 pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n                \n                \n#         return matrix    \n\n\n\ndef transform(dataframe):\n    dataframe = dataframe.toPandas()\n\n    file_path = '/home/tnmops/seahorse3_bkp/'\n    msisdn_name = 'msisdn'\n    TRANSACTION_PRODUCT_NAME = 'product_id'\n    MSISDN_COL_NAME = 'msisdn'\n    threshold = 0.0\n\n    pf_path = os.path.join(file_path, 'pur_filter.csv')\n    matrix_path = os.path.join(file_path, \"matrix.csv\")\n    purchase_filter_path = os.path.join(file_path, \"purchased_for_association\")\n\n    Path(purchase_filter_path).mkdir(parents=True, exist_ok=True)\n    purchase = pd.read_csv(pf_path)\n\n    path_d = os.path.join(file_path, \"dict.pickle\")\n    with open(path_d, 'rb') as handle:\n        data = pickle.load(handle)\n\n    filtered_dict = {k: v for k, v in data.items()}\n    result = {}\n    purchase_list = []\n\n    for item, val in filtered_dict.items():\n        df = pd.read_csv(val)\n        purchase_filter = purchase[purchase[msisdn_name].isin(df[msisdn_name])]\n\n        products = purchase_filter[TRANSACTION_PRODUCT_NAME].unique().tolist()\n        purchase_list = []\n        \n        list_of_integers = []\n        for product in products:\n            list_of_integers.append(int(product))\n        \n        for product in list_of_integers:\n            \n            purchase_filter_one_product = purchase_filter[purchase_filter[TRANSACTION_PRODUCT_NAME] == product]\n            product = str(product)\n            matrix_path = os.path.join(file_path, \"matrix.csv\")\n            matrix_check = pd.read_csv(matrix_path, nrows=2)\n\n            if product not in matrix_check.columns:\n                continue\n            \n            product = str(product)\n            matrix = pd.read_csv(matrix_path, usecols=[MSISDN_COL_NAME, product])\n            purchase_filter1 = purchase_filter_one_product.merge(matrix, on=msisdn_name, how='inner')\n            purchase_filter2 = purchase_filter1[purchase_filter1[product] > threshold]\n            purchase_filter2 = purchase_filter2.drop(columns=[product])\n            purchase_list.append(purchase_filter2)\n\n        if purchase_list is None or len(purchase_list) == 0:\n            result[item] = None\n            result_path = os.path.join(purchase_filter_path, 'dict.pickle')\n            with open(result_path, 'wb') as handle:\n                pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n            continue\n\n        purchase_final = pd.concat(purchase_list)\n\n        op_path = os.path.join(purchase_filter_path, item + \".csv\")\n        purchase_final.to_csv(op_path)\n        result[item] = op_path\n        result_path = os.path.join(purchase_filter_path, 'dict.pickle')\n        with open(result_path, 'wb') as handle:\n            pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        \n        \n        # products = purchase_filter[TRANSACTION_PRODUCT_NAME].unique().tolist()\n        # list_of_integers = []\n        # for product in products:\n        #     list_of_integers.append(int(product))\n        # # string_numbers = \",\".join([str(number) for number in list_of_integers])\n        \n        # for product in list_of_integers:\n        #     product = str(product)\n        #     matrix123 = pd.read_csv(matrix_path, usecols=[MSISDN_COL_NAME,product])\n        \n    return purchase_final\n\n\n"
              }
            }, {
              "id": "9d76f61e-6901-2e98-020d-ddea4c9be1c5",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nimport numpy as np\nimport os\nimport pickle\nimport traceback\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.utils import resample\nimport math\n\ndag_run_id = 'manual__2023-07-11T10:20:22'\nfile_path = '/home/tnmops/seahorse3_bkp/'\nusage_no_months = ['m1','m2','m3']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_NEEDED_COLUMN = [\n'onnet_revenue',\n'onnet_usage_da',\n'onnet_usage',\n'onnet_voice_count',\n'onnet_da_revenue',\n'offnet_revenue',\n'offnet_usage_da',\n'offnet_usage',\n'offnet_voice_count',\n'offnet_da_revenue',\n'idd_revenue',\n'idd_usage_da',\n'idd_usage',\n'idd_voice_count',\n'idd_da_revenue',\n'voice_rmg_revenue',\n'voice_rmg_usage_da',\n'voice_rmg_usage',\n'voice_rmg_count',\n'voice_rmg_da_revenue',\n'data_rmg_revenue',\n'data_rmg_usage_da',\n'data_rmg_usage',\n'data_rmg_da_revenue',\n'data_revenue',\n'data_usage_da',\n'data_usage',\n'da_data_rev',\n'sms_revenue',\n'sms_usage_da',\n'sms_usage',\n'sms_da_revenue',\n'sms_idd_revenue',\n'sms_idd_usage_da',\n'sms_idd_usage',\n'sms_idd_da_revenue',\n'magik_voice_amount',\n'rbt_subscription_rev',\n'emergency_credit_rev',\n'package_revenue',\n'voice_rev',\n'sms_rev',\n'onn_rev',\n'off_rev',\n'total_data_rev',\n'vas_rev',\n'vas_rev_others',\n'total_revenue',\n'total_voice_count',\n'total_voice_duration',\n'total_mainaccount_data_usage',\n'total_sms_count',\n'total_package_count',\n'total_other_vas_count',\n'total_voice_usage',\n'total_data_usage',\n'total_sms_usage']\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_jan.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_march.csv\",}\n        }\n        \n\n\n\ndef otliner_removal(df, per=0.97):\n    try:\n\n        q = df['total_revenue'].quantile(per)\n        print(\"the length brfore is\", len(df))\n        df = df[df['total_revenue'] < q]\n        print(\"the length after is\", len(df))\n        return df\n    except Exception as e:\n        print(e)\n        raise RuntimeError(e)\n        \n        \ndef filter_data(temp_df):\n    cols = [p + \"_\" + s for s in CUSTOMER_NEEDED_COLUMN for p in usage_no_months]\n    cols.append(\"msisdn\")\n    temp_df = temp_df[cols]\n\n    return temp_df\n    \ndef train_model(data, sample=False):\n    \n    def randomsample(X, y):\n        rus = RandomUnderSampler(random_state=42)\n        X_res, y_res = rus.fit_resample(X, y)\n        return X_res, y_res\n\n\n    # def randomsample(X, y):\n    #     # Combine X and y into a single DataFrame for easier resampling\n    #     data = pd.concat([X, y], axis=1)\n    \n    #     # Separate the majority and minority classes based on the target variable\n    #     majority_class = data[data[y.name] == data[y.name].value_counts().idxmax()]\n    #     minority_class = data[data[y.name] != data[y.name].value_counts().idxmax()]\n    \n    #     # Perform random under-sampling on the majority class\n    #     majority_class_resampled = resample(majority_class,\n    #                                         replace=False,\n    #                                         n_samples=len(minority_class),\n    #                                         random_state=42)\n    \n    #     # Combine the resampled majority class and the minority class\n    #     resampled_data = pd.concat([majority_class_resampled, minority_class])\n    \n    #     # Separate the feature matrix (X_res) and the target vector (y_res)\n    #     X_res = resampled_data.drop(y.name, axis=1)\n    #     y_res = resampled_data[y.name]\n    \n    #     return X_res, y_res\n    \n    \n    def run_decision_tree(X_train, X_test, y_train, y_test):\n        clf = DecisionTreeClassifier(max_leaf_nodes=12)\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        return clf\n\n    X = data.drop(['msisdn', 'label'], axis=1)\n    y = data['label']\n\n    if sample:\n        X, y = randomsample(X=X, y=y)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    # sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\n    \n    sel = SelectFromModel(DecisionTreeClassifier(max_leaf_nodes=12))\n    sel.fit(X_train, y_train)\n    sel.get_support()\n    features = X_train.columns[sel.get_support()]\n\n    X_train_rfc = sel.transform(X_train)\n    X_test_rfc = sel.transform(X_test)\n    clf1 = run_decision_tree(X_train_rfc, X_test_rfc, y_train, y_test)\n    features_importance = list(clf1.feature_importances_)\n    dic = {features[i]: features_importance[i] for i in range(len(features))}\n    # t = None\n    # try:\n    #     t = pd.read_csv('temp.csv')\n    #     tt = t.append(dic, ignore_index=True)\n    # except:\n    #     t = pd.DataFrame(columns=list(X_train.columns))\n    #     tt = t.append(dic, ignore_index=True)\n    #\n    # print(tt.head(5))\n    # tt.to_csv('temp.csv', header=True, index=False)\n    return clf1, features\n\n    \ndef load_picke_file(filename):\n    with open(filename, 'rb') as handle:\n        data = pickle.load(handle)\n    return data\n    \ndef get_top_product_ids(row):\n    return row.nlargest(2).index.tolist()    \n   \n   \n   \ndef getpackname(product_id, packinfo_df):\n    product_name = \"No product name\"\n    try:\n        if (packinfo_df is not None):\n            if (product_id in packinfo_df['product_id'].values):\n\n                product_name = str(packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"product_name\"])\n                print(\"the product name of \", product_id, \" is \", product_name)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackname \", e)\n\n    return product_name\n    \n    \n\ndef getpackprice(product_id, packinfo_df):\n    price = 0\n    try:\n        if (packinfo_df is not None):\n            if (product_id in packinfo_df['product_id'].values):\n\n                price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\n                print(\"the price  of \", product_id, \" is \", price)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackprice \", e)\n\n    return price\n    \n    \ndef transform(dataframe):\n        file_path = '/home/tnmops/seahorse3_bkp/'\n        dataframe = dataframe.toPandas()\n        pack_info = os.path.join(file_path, 'pack_info.csv')\n        pack_info_df = pd.read_csv(pack_info)\n        file_name_dict = get_file_names()\n        data = {}\n        for month in usage_no_months:\n            data[month] = pd.read_csv(os.path.join(file_path, file_name_dict.get(\"usage\").get(month)),)\n    \n        df = data.get('m1')\n        months = usage_no_months\n        temp_df = None\n        \n        for month in months:\n            df = data.get(month)\n            df = df.fillna(0)\n            total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n            # df['tot_rev'] = df[total_revenue]\n            df = otliner_removal(df.copy())#-------------------------------------------------------------\n            new_df = df.add_prefix(f\"{month}_\")\n            \n            df = new_df.rename(columns={f\"{month}_msisdn\": \"msisdn\"})\n            if temp_df is None:\n                temp_df = df\n            else:\n\n                temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n                temp_df = temp_df.fillna(0)\n        # temp_df = temp_df.compute()\n        df = filter_data(temp_df)#----------------------------------------------------\n        df['label'] = ((df['m3_total_revenue'] > 0) &\n                            (df['m2_total_revenue'] > 0) &\n                            (df['m1_total_revenue'] == 0)).astype(int)\n                            \n        temp_df = None\n        temp_df = df\n        \n        \n        data_for_uplift = temp_df[['msisdn','m1_total_revenue']]\n        df = temp_df.loc[:, ~temp_df.columns.str.startswith('m1')]\n        temp_df = None\n        temp_df = df\n        clf1, features = train_model(temp_df, sample=True)#----------------------------------------------\n        # ic(f\"the features selected are \", features)\n        data_for_prediction = temp_df[features]\n        \n        msisdns = temp_df.pop(\"msisdn\")\n        probabilities = clf1.predict_proba(data_for_prediction)#------------------------------------------\n        data_for_prediction['predictions'] = clf1.predict(data_for_prediction)\n        data_for_prediction['predict_proba_0'] = probabilities[:, 0]\n        data_for_prediction['predict_proba_1'] = probabilities[:, 1]\n        data_for_prediction['msisdn'] = msisdns\n        \n        \n        matrix_path = os.path.join(file_path, \"matrix.csv\")\n        matrix = pd.read_csv(matrix_path)\n        \n        matrix = matrix.drop('_c0', axis=1)\n        matrix = matrix.drop('Unnamed: 0', axis=1)\n\n        data_path = os.path.join(file_path, \"dict.pickle\")\n        data_dict = load_picke_file(data_path)\n        filtered_dict = {k: v for k, v in data_dict.items()}\n        final_df_ls=[]\n        \n        \n        \n        for item, val in filtered_dict.items():\n            data = pd.read_csv(filtered_dict.get(item))\n            data_filter = data_for_prediction[data_for_prediction['msisdn'].isin(data['msisdn'])]\n            data_filter_churners = data_filter[data_filter['predict_proba_1'] > .50]\n            data_for_uplift_temp = data_for_uplift[data_for_uplift['msisdn'].isin(data_filter_churners['msisdn'])]\n\n            matrix_filter = matrix[matrix['msisdn'].isin(data['msisdn'])]\n            if matrix_filter is None or len(matrix_filter)==0:\n                continue\n            \n            # matrix_filter = matrix_filter.drop('_c0', axis=1)\n            # matrix_filter = matrix_filter.drop('Unnamed: 0', axis=1)\n\n            msisdns_list = matrix_filter.pop(\"msisdn\")\n            matrix_filter['top_2_products'] = matrix_filter.iloc[:, 1:].apply(get_top_product_ids, axis=1)\n            matrix_filter['highest_product'] = matrix_filter['top_2_products'].apply(lambda x: x[0])\n            matrix_filter['second_highest_product'] = matrix_filter['top_2_products'].apply(lambda x: x[1])\n            \n            \n            \n            # matrix_filter['highest_product'] = matrix_filter['highest_product'].astype(int)\n            \n            df = matrix_filter[['highest_product','second_highest_product','top_2_products']]\n            \n            recommenting_product = matrix_filter['highest_product'].value_counts().idxmax()\n            data_filter_churners['recommended_product_id'] = recommenting_product\n            recommenting_product_name = str(getpackname(int(recommenting_product), pack_info_df))\n            data_filter_churners['recommenting_product_name'] = recommenting_product_name\n            data_filter_churners['segement_id'] = 0000\n\n            segements_data = item.split(\"-\")\n            trend = segements_data[0]\n            rfm = segements_data[1]\n            data_filter_churners['segement_name'] = item\n            data_filter_churners['trend'] = trend\n            data_filter_churners['rfm'] = rfm\n\n\n            current_arpu = data_for_uplift_temp['m1_total_revenue'].mean()\n            total_revenue = int(data_for_uplift_temp['m1_total_revenue'].sum())\n            if math.isnan(current_arpu):\n                current_arpu=0\n            \n            # pack_info= os.path.join(file_path, \"pack_info.csv\")\n            # pack_info_df = pd.read_csv(pack_info)\n\n            recommenting_product_price = getpackprice(int(recommenting_product), pack_info_df)\n\n            incremental_revenue =  recommenting_product_price * len(data_for_uplift_temp)\n            if incremental_revenue !=0:\n                incremental_revenue = (  60 / incremental_revenue) * 100\n            print('incremental_revenue',incremental_revenue)\n            initial_sum = data_for_uplift_temp['m1_total_revenue'].sum()\n            \n            \n            if initial_sum == 0 or incremental_revenue ==0:\n                 print('either initial_sum or incremental_revenue is 0'  )\n                 uplift =0\n            else:\n                print('initial_sum',initial_sum)\n                uplift = (incremental_revenue / initial_sum) * 100\n\n                uplift = round(uplift, 2)\n                \n            \n            predicted_arpu = int(current_arpu + ((current_arpu*uplift)/100))\n\n            if len(data_filter_churners) ==0:\n                continue\n\n            final_df_ls.append(data_filter_churners)\n            # insert_segement_info_churn(item, dag_run_id, data_filter_churners, recommenting_product,recommenting_product_name,\n            #                           incremental_revenue,uplift,current_arpu,predicted_arpu,total_revenue, db)\n        \n        final_df=pd.concat(final_df_ls)\n        final_df['dag_run_id']=str(dag_run_id)\n        final_df=final_df[['msisdn', 'segement_id', 'segement_name', 'trend', 'rfm', 'predictions',\n          'predict_proba_0', 'predict_proba_1', 'recommended_product_id','dag_run_id']]    \n            \n        # final_df = final_df_ls[0]\n        final_df.to_csv(os.path.join(file_path, \"APA_output.csv\"), header=True, index=False)\n\n        final_df.to_csv(os.path.join(file_path, \"churn_propensity.csv\"),\n                                   header=True, index=False)\n        \n        \n        return final_df\n\n"
              }
            }, {
              "id": "abfd8b7c-e546-ab0b-4293-68f6faaaa4bb",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom icecream import ic\nfrom urllib.parse import quote  \nimport datetime\nfrom typing import List, Optional\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nimport sklearn\nimport json\nimport dask.dataframe as dd\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport dask.dataframe as dd\nimport os\n# import configuration.config as cfg\n# import configuration.features as f\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nfrom pathlib import Path\nimport requests\n# from sql_app.repositories import AssociationRepo\n\n# config = cfg.Config().to_json()\n# features = f.Features().to_json()\n\n\n\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\n\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\n\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\n# 0 index for inbundle 1 index for outbundled  2 index for total\n\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_NEEDED_COLUMN = [\n'onnet_revenue',  'onnet_usage', 'onnet_voice_count', \n'offnet_revenue', 'offnet_usage', 'offnet_voice_count',  \n'idd_revenue', 'idd_usage', 'idd_voice_count',\n'voice_rmg_revenue', 'voice_rmg_usage', 'voice_rmg_count', \n'data_rmg_revenue',  'data_rmg_usage',  \n'data_revenue', 'data_usage', 'sms_revenue', 'sms_usage',  'sms_idd_revenue', \n'sms_idd_usage', 'magik_voice_amount', 'rbt_subscription_rev', 'emergency_credit_rev', \n'package_revenue', 'voice_rev', 'sms_rev', 'onn_rev', 'off_rev', 'total_data_rev', \n'vas_rev', 'vas_rev_others', 'total_revenue', 'total_voice_count', 'total_voice_duration', \n'total_mainaccount_data_usage', 'total_sms_count', 'total_package_count', 'total_other_vas_count', \n'total_voice_usage', 'total_data_usage', 'total_sms_usage'\n]\n\n\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n\n\n\n\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_jan.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_march.csv\",}\n        }\n\n\n\nfile_path = '/home/tnmops/seahorse3_bkp/'\nrule_path = os.path.join(file_path, \"rule.csv\")\ndag_run_id = \"manual__2023-07-10T11:06:51\"\nnot_needed_rfm_segment = ['Lost']\n\n\n\nusage_no_months = ['m1','m2','m3']\nmsisdn_name = MSISDN_COL_NAME\n\n\n\n\n\n\n\ndef findByAutoPilotIdAndSegementNameAll(df, _id, segment_name, cluster_number):\n    return df[\n        (df['dag_run_id'] == _id) &\n        (df['segment_name'] == segment_name) &\n        (df['cluster_no'] == cluster_number)\n    ]\n    \n\n# def findByAutoPilotIdAndSegementNameAll(df, _id, segment_name, cluster_number):\n#     filtered_df = df[\n#         (df['dag_run_id'] == _id) &\n#         (df['segment_name'] == segment_name) &\n#         (df['cluster_no'] == cluster_number)\n#     ]\n#     result = []\n#     for _, row in filtered_df.iterrows():\n#         result.append(row.to_dict())\n#     return result\n\n\n# def findByAutoPilotIdAndSegementNameAll(df, _id, segment_name, cluster_number):\n#     filtered_df = df[\n#         (df['dag_run_id'] == _id) &\n#         (df['segment_name'] == segment_name) &\n#         (df['cluster_no'] == cluster_number)\n#     ]\n#     return filtered_df.to_dict(orient='records')\n\ndef findByAutoPilotIdAndSegementNamewithoutcluster(df, _id, segement_name):\n    return df[(df['dag_run_id'] == _id) & (df['segment_name'] == segement_name)]\n\n\n\n\n# def deleteById(db, _ids):\n#     # Filter out rows with matching IDs\n#     db = db[~db['id'].isin(_ids)]\n#     return db\n\n\n# def update(db, item_data):\n#     # Update the DataFrame with the item_data\n#     db.loc[db['id'] == item_data['id']] = item_data\n#     return db\n\n\ndef update(db, item_data):\n    # Check if the 'id' exists in the DataFrame\n    if 'id' in item_data:\n        # Find the index of the row with the corresponding 'id' in the DataFrame\n        index_to_update = db.index[db['id'] == item_data['id']].tolist()\n        if index_to_update:\n            index_to_update = index_to_update[0]\n            # Update the row with the provided item_data\n            db.loc[index_to_update] = item_data\n            return db\n        else:\n            # If 'id' not found, you can add it as a new row\n            db = db.append(item_data, ignore_index=True)\n            return db\n    else:\n        # If 'id' is not provided in item_data, raise an error or handle it accordingly\n        raise ValueError(\"'id' is required in item_data for updating the DataFrame.\")\n\n        \n        \n        \n        \n        \n        \ndef form_data(p2, df, anti_conci):\n    try:\n        product_id = PACK_INFO_PACK_COLUMN_NAME\n        purchase = p2[p2[product_id].isin(anti_conci)]\n        pgp = purchase.copy()\n        # pgp = purchase.groupby(['msisdn', 'product_id']).agg({f.Features.PA: \"sum\"}).reset_index()\n\n        anti_df = pgp[pgp[product_id].isin(anti_conci[:-1])]\n        conci_df = pgp[pgp[product_id] == anti_conci[-1]]\n\n        anti_df_msisdn = anti_df[~anti_df['msisdn'].isin(conci_df['msisdn'].values)]['msisdn'].unique()\n        conci_df_msisdn = conci_df[~conci_df['msisdn'].isin(anti_df['msisdn'].values)]['msisdn'].unique()\n        anti_data = df[df['msisdn'].isin(anti_df_msisdn)]\n        conci_data = df[df['msisdn'].isin(conci_df_msisdn)]\n        print(f\"the length of anti data ios {len(anti_data)} and unique is {anti_data['msisdn'].nunique()}\")\n        print(f\"the length of conci data ios {len(conci_data)} and unique is {conci_data['msisdn'].nunique()}\")\n        anti_data['label'] = 1\n        conci_data['label'] = 0\n        data = pd.concat([anti_data, conci_data], axis=0)\n        print(\"label counts\", data['label'].value_counts())\n        return data\n    except Exception as e:\n        print(\"the error occoured in form_data\", e)\n        raise ValueError(e)\n\n        \n        \n        \n        \n        \nclass DecisionTreeConverter(object):\n\n    def __init__(self, my_tree=None, features=None, class_names=None, df=None):\n        self.my_tree = my_tree\n        self.features = features\n        self.class_names = class_names\n        self.df = df\n        self.json = None\n\n        self.json_string = \"\"\n\n        # self.recursion(self.my_tree.tree_, 0)\n\n        self.recurse(self.my_tree.tree_, 0)\n\n    def node_to_str(self, tree, node_id, criterion):\n        if True:\n            criterion = \"impurity\"\n\n        value = tree.value[node_id]\n        if tree.n_outputs == 1:\n            value = value[0, :]\n\n        jsonValue = ', '.join([str(x) for x in value])\n\n        if tree.children_left[node_id] == sklearn.tree._tree.TREE_LEAF:\n            l = 1\n            try:\n\n                probablity = np.round(100.0 * value[l] / np.sum(value), 2)\n            except:\n                probablity = np.round(100.0 * value[0] / np.sum(value), 2)\n            return '\"id\": \"%s\", \"criterion\": \"%s\", \"impurity\": \"%s\", \"samples\": \"%s\",\"recomentedPackProbablity\":%s, ' \\\n                   '\"value\": [%s]' \\\n                   % (node_id,\n                      criterion,\n                      tree.impurity[node_id],\n                      tree.n_node_samples[node_id],\n                      probablity,\n                      jsonValue)\n        else:\n\n            if self.features is not None:\n                feature = self.features[tree.feature[node_id]]\n            else:\n                feature = tree.feature[node_id]\n\n            if \"=\" in feature:\n                ruleType = \"=\"\n                ruleValue = \"false\"\n            else:\n                ruleType = \"<=\"\n                ruleValue = \"%.2f\" % tree.threshold[node_id]\n\n            return '\"id\": \"%s\", \"rule\": \"%s %s %s\", \"%s\": \"%s\", \"samples\": \"%s\"' \\\n                   % (node_id,\n                      feature,\n                      ruleType,\n                      ruleValue,\n                      criterion,\n                      tree.impurity[node_id],\n                      tree.n_node_samples[node_id])\n\n    def recurse(self, tree, node_id, criterion='impurity', parent=None, depth=0):\n        tabs = \"  \" * depth\n        self.json_string = \"\"\n\n        left_child = tree.children_left[node_id]\n        right_child = tree.children_right[node_id]\n\n        self.json_string = self.json_string + \"\\n\" + \\\n                           tabs + \"{\\n\" + \\\n                           tabs + \"  \" + self.node_to_str(tree, node_id, criterion)\n\n        if left_child != sklearn.tree._tree.TREE_LEAF:\n            self.json_string = self.json_string + \",\\n\" + \\\n                               tabs + '  \"left\": ' + \\\n                               self.recurse(tree, left_child, criterion=criterion, parent=node_id,\n                                            depth=depth + 1) + \",\\n\" + \\\n                               tabs + '  \"right\": ' + \\\n                               self.recurse(tree,\n                                            right_child,\n                                            criterion=criterion,\n                                            parent=node_id,\n                                            depth=depth + 1)\n\n        self.json_string = self.json_string + tabs + \"\\n\" + \\\n                           tabs + \"}\"\n\n        return self.json_string\n\n    def recursion(self, tree, node_id, parent=None, depth=0, location=None):\n        tabs = \" \" * depth\n        self.json = \"\"\n        left_child = tree.children_left[node_id]\n        right_child = tree.children_right[node_id]\n        self.json = self.json + tabs + \"{\" + tabs + \" \" + self.get_node_to_string(tree, node_id, location)\n        print(f\"the json got in recursion is {self.json}\")\n        if left_child != sklearn.tree._tree.TREE_LEAF:\n            self.json = self.json + \",\" + tabs + '\"left\": ' + self.recursion(tree, left_child, node_id, depth + 1,\n                                                                             \"left\") + \",\" + \\\n                        tabs + '\"right\": ' + self.recursion(tree, right_child, node_id, depth + 1, \"right\")\n            print(\"json formed inside resursion when not tree leaf  \", self.json)\n        self.json = self.json + tabs + tabs + \"}\"\n        print(\"json formed inside resursion when  tree leaf  \", self.json)\n        return self.json\n\n    def get_node_to_string(self, tree, node_id, location):\n        value = tree.value[node_id]\n        print(f\"the value of the node  {node_id}  is -- {value}\")\n        if tree.n_outputs == 1:\n            value = value[0, :]\n            print(f\" the type of value is {type(value)}\")\n        json_val = \", \".join([str(x) for x in value])\n\n        if tree.children_left[node_id] == sklearn.tree._tree.TREE_LEAF:\n            # l = np.argmax(value)\n            l = 1\n            try:\n\n                probablity = np.round(100.0 * value[l] / np.sum(value), 2)\n            except:\n                probablity = np.round(100.0 * value[0] / np.sum(value), 2)\n\n            print(f\"the left child is a leaf node \")\n            return_val = f' \"id\": {node_id}, \"class\":\"{self.class_names[l]}\" ,\"samples\":{tree.n_node_samples[node_id]} , \"recomentedPackProbablity\": {probablity}'\n            print(f\"the return value of convert tree to json when left child is a leaf node is {return_val}\")\n            return return_val\n\n        else:\n            if self.features is not None:\n                print(f\" the features are  {self.features} ,   the node is is {node_id}\")\n                print(location)\n\n                feature = self.features[tree.feature[node_id]]\n            else:\n                print(\"no features list given \")\n\n            rule_val = \"%.2f\" % tree.threshold[node_id]\n\n            if location is not None and location == 'left':\n                minimum_value = self.df[feature].min()\n                if minimum_value == 0:\n                    operator = f\"<= {rule_val}\"\n                else:\n                    operator = f'between {round(float(minimum_value), 2)} and {round(float(rule_val), 2)} '\n\n            else:\n\n                maximum_value = self.df[feature].max()\n                operator = f'between {round(float(rule_val), 2)} and {round(float(maximum_value), 2)} '\n\n            # no need of samples and values\n\n            return_val = f' \"id\":{node_id}, \"rule\": \"{feature} {operator} \" '\n            print(f\"the return value of convert tree to json when left child is a  not leaf node is {return_val}\")\n            return return_val\n\n    def get_json(self):\n        if self.json_string is not None:\n            return self.json_string\n\n\n        \n        \ndef replace_rule(rule_condition, columns_for_dummies):\n    try:\n        if rule_condition is None:\n            return rule_condition\n        name = rule_condition.split(\"<=\")[0]\n        if name is None or len(name) == 0:\n            return rule_condition\n\n        split_name = name.split(\":\")\n        key = split_name[0]\n        if key not in columns_for_dummies:\n            return rule_condition\n        value = split_name[1]\n        value = value.strip()\n        rule_m = f\"{key} == '{round(float(value), 2)}' \"\n        return rule_m\n\n    except Exception as e:\n        print(\"the error in replace rule is \", e)\n        return rule_condition\n\n    \ndef pruneTree(root, columns_for_dummies):\n    if root.get('recomentedPackProbablity') is not None and root.get('recomentedPackProbablity') < 50:\n        return None\n\n    if root.get('left') is not None:\n        root['rule'] = replace_rule(root.get('rule'), columns_for_dummies)\n        root['left'] = pruneTree(root.get('left'), columns_for_dummies)\n    if root.get('right') is not None:\n        root['rule'] = replace_rule(root.get('rule'), columns_for_dummies)\n        root['right'] = pruneTree(root.get('right'), columns_for_dummies)\n    if root.get('left') is not None or root.get('right') is not None or root.get(\n            'recomentedPackProbablity') is not None and root.get('recomentedPackProbablity') >= 50:\n        # samples =samples +int(root.get('samples'))\n        return root\n    else:\n        return None\n\n    \n    \ndef generate_boundries(features, data_json, tree):\n    def add_conditions(feature, count):\n        rule_json = {}\n        rule_json['id'] = -1\n\n        min_max = data_json.get(feature)\n        if min_max.get('min') == 0:\n            rule = f\"{feature} <= {round(float(min_max.get('max')), 2)}\"\n\n\n        else:\n            rule = f\"{feature} between {round(float(min_max.get('min')), 2)} and {round(float(min_max.get('max')), 2)}\"\n\n        rule_json['rule'] = rule\n        if count < len(features) - 1:\n            rule_json['left'] = add_conditions(features[count + 1], count + 1)\n        else:\n            rule_json['left'] = tree\n        return rule_json\n\n    try:\n        print(\"inside generate_boundries\")\n        return add_conditions(features[0], 0)\n\n    except Exception as e:\n        print(f\"error occoured in generating boundries {e}\")\n\n\n        \n\n        \ndef negate(rule):\n    mapper = ((\"<=\", \">\"), (\">=\", \"<\"), (\">\", \"<=\"), (\"<\", \">=\"), (\"==\", '!='))\n    for operator, negated in mapper:\n        if operator in rule:\n            return rule.replace(operator, negated)\n    return \"not (\" + rule + \")\"  # Default when operator not recognised\n\n\n\ndef nodeToLeafPaths(node):\n    if not node:  # base case: nothing in this direction\n        return\n    rule = node.get('rule')\n    if rule is None:  # base case: a leaf with a value\n        # value_list.append(node.get('value'))\n        yield [], int(node.get('samples'))  # empty path\n        return\n\n    negated_rule = negate(rule)\n    for path, value in nodeToLeafPaths(node.get('left')):\n        # print(f\"the left path {path} and value {value} \")\n        if 'between' in rule:\n            between_split = rule.split('between')\n            key = between_split[0].strip()\n            value_temp = between_split[1].strip()\n            ant_split = value_temp.split(\"and\")\n            value1 = ant_split[0].strip()\n            value2 = ant_split[1].strip()\n\n            rule = f\"{key} > {round(float(value1), 2)} and {key} <= {round(float(value2), 2)}\"\n\n        yield [rule, *path], value  # Extend path with current rule\n    for path, value in nodeToLeafPaths(node.get('right')):\n        yield [negated_rule, *path], value\n\n        \n        \ndef rootToLeafConjugations(root):\n    print('inside rootToLeafConjugations ')\n    final_dic = {}\n    for path, value in nodeToLeafPaths(root):\n        final_dic[\" and \".join(path)] = value\n        # print({\" AND \".join(path):value  for path ,value in nodeToLeafPaths(root) })\n    return final_dic\n\n\n\ndef get_prod_diff(product_id_1, product_id_2, packinfo_df):\n    print('packinfo_df.columns', packinfo_df.columns)\n    diff = 0\n    try:\n        if isinstance(product_id_1, str):\n            split = product_id_1.split(\"|\")\n            print('split is ', split)\n            split_length = len(split)\n            if split_length > 1:\n                max = 0\n                for i in range(split_length):\n                    product = int(float(split[i]))\n                    print('product is ', product)\n                    price = packinfo_df[packinfo_df['product_id'] == product].iloc[0][\"price\"]\n                    print('price is', price)\n                    if price > max:\n                        max = price\n\n                product_id_1 = packinfo_df[packinfo_df['price'] == max].iloc[0][\"product_id\"]\n                print('product_id_1 is', product_id_1)\n\n            else:\n                product_id_1 = int(split[0])\n\n        if packinfo_df is not None:\n            print('product_id_1 is ', product_id_1)\n            print('product_id_2 is ', product_id_2)\n            #print(\"packinfo_df['product_id'].values\", packinfo_df['product_id'].values)\n            if product_id_1 in packinfo_df['product_id'].values:\n                print('product_id_1 ture')\n\n            print('packinfo_df.columns', packinfo_df.columns)\n            if product_id_2 in packinfo_df['product_id'].values:\n                print('product_id_2 ture')\n\n            else:\n                print('product_id_2 fasle')\n\n            if ((product_id_1 in packinfo_df['product_id'].values) and (\n                    product_id_2 in packinfo_df['product_id'].values)):\n\n                print('going to calcualte the diff ')\n\n                diff = abs(int(float(packinfo_df[packinfo_df['product_id'] == product_id_1].iloc[0][\"price\"]) -\n                               float(packinfo_df[packinfo_df['product_id'] == product_id_2].iloc[0][\"price\"])))\n\n                print(\"the difference is \", diff)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in get_prod_diff \", e)\n\n    return diff\n\n\n\ndef add_clusters_rules(features, features_val, tree):\n    def add_conditions(feature, count):\n        rule_json = {}\n        rule_json['id'] = -1\n\n        rule = f\"{features[count]} == {features_val[count]}\"\n        rule_json['rule'] = rule\n        if count < len(features) - 1:\n            rule_json['left'] = add_conditions(features[count + 1], count + 1)\n        else:\n            rule_json['left'] = tree\n        return rule_json\n\n    try:\n        print(\"inside generate_boundries\")\n        return add_conditions(features[0], 0)\n\n    except Exception as e:\n        print(f\"error occoured in generating boundries {e}\")\n\n        \ndef GetCurrentPackprice(product_id, packinfo_df):\n    price = 0\n    \n    try:\n        print('product_id in GetCurrentPackprice is ',product_id)\n        print('type of product_id in GetCurrentPackprice is ',type(product_id))\n        if (packinfo_df is not None):\n            if isinstance(product_id, str):\n                split = product_id.split(\"|\")\n                \n                print('split in GetCurrentPackprice is ', split)\n                split_length = len(split)\n                if split_length > 1:\n                    highest = 0\n                    for i in range(split_length):\n                        product = int(float(split[i]))\n                        print('product is ', product)\n                        price = packinfo_df[packinfo_df['product_id'] == product].iloc[0][\"price\"]\n                        print('price is', price)\n                        if price > highest:\n                            highest = price\n                \n                    print(\"the price  of \", product_id, \" is \", price)\n                \n                else:\n                    \n                    product_id = int(split[0])\n                    \n                    price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\n                    highest = price\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackprice \", e)\n\n    return highest\n\n\n\ndef getpackprice(product_id, packinfo_df):\n    price = 0\n    try:\n        if (packinfo_df is not None):\n            if (product_id in packinfo_df['product_id'].values):\n\n                price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\n                print(\"the price  of \", product_id, \" is \", price)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackprice \", e)\n\n    return price\n\n\n\n\ndef train_model(data):\n    def run_decision_tree(X_train, X_test, y_train, y_test):\n        clf = DecisionTreeClassifier(max_leaf_nodes=12)\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        print('Accuracy run_decision_tree: ', accuracy_score(y_test, y_pred))\n        #print(\"confusion matrix run_decision_tree\", confusion_matrix(y_test, y_pred))\n        print(classification_report(y_test, y_pred))\n        return clf\n\n    X = data.drop(['msisdn', 'label'], axis=1)\n    y = data['label']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    print(f\"X_train.shape = {X_train.shape}, X_test.shape =  {X_test.shape}\")\n    # sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\n    sel = SelectFromModel(DecisionTreeClassifier(max_leaf_nodes=12))\n    sel.fit(X_train, y_train)\n    sel.get_support()\n    features = X_train.columns[sel.get_support()]\n    print(\"the feature selection features are \", features)\n    X_train_rfc = sel.transform(X_train)\n    X_test_rfc = sel.transform(X_test)\n    clf1 = run_decision_tree(X_train_rfc, X_test_rfc, y_train, y_test)\n    features_importance = list(clf1.feature_importances_)\n    dic = {features[i]: features_importance[i] for i in range(len(features))}\n    t = None\n    # try:\n    #     t = pd.read_csv('temp.csv')\n    #     tt = t.append(dic, ignore_index=True)\n    # except:\n    #     t = pd.DataFrame(columns=list(X_train.columns))\n    #     tt = t.append(dic, ignore_index=True)\n\n    # print(tt.head(5))\n    # tt.to_csv('temp.csv', header=True, index=False)\n    return clf1, features\n\n\n\n\n\n\n\nclass RuleGenerator(object):\n    def __init__(self, df, dag_run_id, cluster_name, cluster_number, purchase_filtered, pack_info):\n\n        self.df = df\n        self.dag_run_id = dag_run_id\n        self.cluster_name = cluster_name\n        self.cluster_name = cluster_number\n        self.purchase = purchase_filtered\n        self.usage_filter2 = None\n        self.pack_info = pack_info\n        self.op = None\n        pass\n\n    def generate_rules(self, segementss, usage):\n        segment_list = []\n        dftnm_ls = []\n        data_dict_query = {}\n        \n        # print(type(segementss))\n#         print(segementss)\n        for index, segements in segementss.iterrows():\n            print(segements)\n            if segements['recommended_product_id'] is None:\n                continue\n            usage_filter1 = usage[usage['msisdn'].isin(self.df['msisdn'])]\n            # usage_filter1 = usage_filter1.compute()\n            conci = int(segements.recommended_product_id)\n#             anti = [int(x) for x in segements.current_product.split(\"|\")]\n            \n            \n            if isinstance(segements.current_product, str) and '|' in segements.current_product:\n                anti = [int(x) for x in segements.current_product.split(\"|\")]\n            else:\n                # If segements.current_product is not a string or doesn't contain '|', convert it to a list with a single integer element\n                anti = [int(segements.current_product)]\n            \n            \n            anti.append(conci)\n            df = form_data(p2=self.purchase, df=usage_filter1, anti_conci=anti)\n\n#             print(\"df columns ==>\",df.columns)\n            self.usage_filter2 = usage_filter1.merge(self.df[['msisdn','trend']], on='msisdn', how='inner')\n            value_counts = df['label'].value_counts()\n            if 0 in value_counts.index and 1 in value_counts.index:\n                if value_counts.loc[0] >= 10 and value_counts.loc[1] >= 10:\n                    print(\"Both values have at least two entries\")\n                    # path_temp = os.path.join(cfg.Config.ml_location, self.dag_run_id, \"model\")\n                    # Path(path_temp).mkdir(parents=True, exist_ok=True)\n                    # df.to_csv(os.path.join(path_temp, segements.segment_name + \".csv\"), header=True, index=False)\n                    # ic(\"outputerrrrddddd\")\n                else:\n                    print(\"At least one value doesn't have two entries\")\n                    continue\n            else:\n                print(\"Both values are not present in the column\")\n                continue\n\n            clf1, features = train_model(df)\n           \n            print('features -------------------', features)\n\n            \n            # for BTC purpose end\n            \n            # features = list(features)\n            # print('features is ', features)\n            # print('type features is ', type(features))\n            # features.append('m1_total_revenue')\n            # features = list(set(features))\n            decision_tree_obj = DecisionTreeConverter(clf1, features, ['differentpack', 'whatsappPack'],\n                                                          df[df['label'] == 1])\n            treetojson = decision_tree_obj.get_json()\n            #print(\"tree fromed\", treetojson)\n            prune_tree = pruneTree(root=json.loads(treetojson), columns_for_dummies=[])\n            df_temp = df[df['label'] == 1]\n            df1 = df_temp.agg(['min', 'max'])\n            df_json = json.loads(df1.to_json())\n            prune_tree = generate_boundries(features, df_json, prune_tree)\n            segement_names = ['trend', 'rfm']\n            segement_values = segements.segment_name.split(\"-\")[:-1]\n            prune_tree1 = add_clusters_rules(segement_names, segement_values, prune_tree)\n\n            final = rootToLeafConjugations(prune_tree)\n            print('after  rootToLeafConjugations ')\n            samples = sum(final.values())\n            rule = \" or \".join(list(final.keys()))\n\n            temp = self.usage_filter2.query(rule)\n\n            # for BTC purpose\n            dftnm = temp.copy()\n            msisdns = dftnm.pop(\"msisdn\")\n            dftnm = dftnm[features]\n            X = dftnm.drop(['msisdn', 'label'], axis=1, errors=\"ignore\")\n            # y = dftnm['label']\n            predictions = clf1.predict(X)\n            prediction_probabilities = clf1.predict_proba(X)\n            dftnm['predictions'] = predictions\n            dftnm['predict_proba_0'] = prediction_probabilities[:, 0]  # Probability of class 0\n            dftnm['predict_proba_1'] = prediction_probabilities[:, 1]  # Probability of class 1\n            dftnm['msisdn'] = msisdns\n            dftnm[\"segement_id\"] = segements.id\n            dftnm[\"recommended_product_id\"] = segements.recommended_product_id\n            segement_names = segement_names\n            segement_values = segements.segment_name.split(\"-\")[:-1]\n            dftnm[segement_names[0]] = segement_values[0]\n            dftnm[segement_names[1]] = segement_values[1]\n            dftnm[\"segement_name\"] = segements.segment_name\n            dftnm = dftnm[\n                [\"msisdn\",\"segement_id\",\"segement_name\", segement_names[0], segement_names[1], \"predictions\", \"predict_proba_0\", \"predict_proba_1\",\"recommended_product_id\"]]\n            # path_op = os.path.join(cfg.Config.ml_location, self.dag_run_id, \"output_data\")\n            # Path(path_op).mkdir(parents=True, exist_ok=True)\n            # dftnm.to_csv(os.path.join(path_op, segements.segment_name + \".csv\"), header=True, index=False)\n            dftnm_ls.append(dftnm)\n            dftnm_final = pd.concat(dftnm_ls)\n            self.op = dftnm_final\n\n\n            # segements.rule = json.dumps(prune_tree1)\n\n            # segements.rule = make_request(prune_tree1)\n            segements.rule = \"testing\"\n            segements.query = rule\n            segements.actual_target_count = len(dftnm)\n            \n            #print('rule is', rule)\n            print('going to query')\n            #print('columns of self.usage_filter2 is ', self.usage_filter2.columns)\n            \n\n            filename = os.path.join(file_path,'query_count.pickle')\n            if os.path.isfile(filename):\n                # Read data from file\n                with open(filename, 'rb') as handle:\n                    data_dict_query = pickle.load(handle)\n\n                    \n            print('len of temp is ', len(temp))\n            segment_name_qr = segements.segment_name\n            print('segment_name_qr is ',segment_name_qr)\n            print('type of segement_values is', type(segment_name_qr))\n\n            data_dict_query[segment_name_qr] = len(temp)\n\n            print('len appended')\n           \n            with open(filename, 'wb') as handle:\n                pickle.dump(data_dict_query, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n            \n\n            \n            \n            if temp is None or len(temp) == 0:\n                continue\n            print('len of temp is ', len(temp))\n\n            op_path_dir = os.path.join(file_path,  'segment_data')\n            Path(op_path_dir).mkdir(parents=True, exist_ok=True)\n\n            print(str(int(segements.recommended_product_id)))\n            anti_list = [str(x) for x in segements.current_product.split(\"|\")]\n            anti_prod = ''.join(str(e) for e in anti_list)\n            print('anti_prod is', anti_prod)\n            print('segement_values is',segement_values)\n            filename = '_'.join(str(e) for e in segement_values)+'_'+str(int(segements.recommended_product_id))+'_'+anti_prod+'.csv'\n            print('filename is ',filename)\n\n            \n            df_sub=df.query(rule)\n            \n            df_sub.to_csv(os.path.join(op_path_dir,filename), header=True, index=False)\n\n            segements.samples = samples\n\n            #segements.current_arpu = int(temp['m1_total_revenue'].mean())\n            total_revenue = int(temp['m1_total_revenue'].sum())\n            segements.total_revenue=total_revenue\n\n            current_product_id = segements.current_product\n            rec_product_id = int(segements.recommended_product_id)\n\n            incremental_revenue = get_prod_diff(current_product_id, rec_product_id, self.pack_info) * samples\n            incremental_revenue = (incremental_revenue / 80) * 100\n            initial_sum = temp['m1_total_revenue'].sum()\n            #uplift = (incremental_revenue / initial_sum) * 100\n            \n            current_product_price = GetCurrentPackprice(current_product_id, self.pack_info)\n            reco_product_price = getpackprice(rec_product_id,self.pack_info)\n            segements.uplift_revenue = reco_product_price-current_product_price\n\n            prodd_diff= reco_product_price-current_product_price\n            # uplift=(prodd_diff/current_product_price) *100\n            # segements.uplift = round(uplift, 2)\n            segements.incremental_revenue = incremental_revenue\n\n\n            segment_length=str(len(self.df))\n            print('segment_length is' ,segment_length)\n\n            segements.segment_length = segment_length\n\n\n\n            #segements.predicted_arpu = int(segements.current_arpu + ((segements.current_arpu*uplift)/100))\n\n            segment_list.append(segements)\n        return segment_list\n\n\n        \n        \ndef load_picke_file(filename):\n    with open(filename, 'rb') as handle:\n        data = pickle.load(handle)\n    return data\n\n\n\n\ndef filter_data(temp_df):\n    cols = [p + \"_\" + s for s in CUSTOMER_NEEDED_COLUMN for p in usage_no_months]\n    cols.append(\"msisdn\")\n    temp_df = temp_df[cols]\n\n    return temp_df\n\n\n\ndef otliner_removal(df, per=0.97):\n    try:\n        # df = df[\"needed_col\"]\n        tot_rev = CUSTOMER_TOTAL_REVENUE[0]\n        q = df[tot_rev].quantile(per)\n        print(\"the length brfore is\", len(df))\n        df = df[df[tot_rev] < q]\n        print(\"the length after is\", len(df))\n        return df\n    except Exception as e:\n        print(e)\n        raise RuntimeError(e)\n        \n        \n        \n\n        \n        \n\ndef transform(dataframe):\n      \n    dag_run_id = \"manual__2023-07-10T11:06:51\"\n    file_name_dict = get_file_names()\n    data = {}\n    for month in usage_no_months:\n        data[month] = pd.read_csv(os.path.join(file_path, file_name_dict.get(\"usage\").get(month)),)\n\n\n    months = usage_no_months\n    temp_df = None\n\n    for month in months:\n        df = data.get(month)\n        df = df.fillna(0)\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n        # df['tot_rev'] = df[total_revenue]\n        df = otliner_removal(df.copy())\n        df = df.add_prefix(f\"{month}_\")\n        df = df.rename(columns={f\"{month}_msisdn\": \"msisdn\"})\n        if temp_df is None:\n            temp_df = df\n        else:\n\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n            temp_df = temp_df.fillna(0)\n    # temp_df = temp_df.compute()\n    temp_df = filter_data(temp_df)\n    result_dict_path = os.path.join(file_path, \"purchased_for_association\", 'dict.pickle')\n    data_path = os.path.join(file_path, \"dict.pickle\")\n    data_dict = load_picke_file(data_path)\n    pack_info = os.path.join(file_path, 'pack_info.csv')\n    pack_info_df = pd.read_csv(pack_info)\n    data = load_picke_file(result_dict_path)\n\n    filtered_dict = {k: v for k, v in data.items()}\n    filtered_data__dict = {k: v for k, v in data_dict.items()}\n\n    # btc purpose\n    output_d_list = []\n    # btc purpose end\n\n    db = pd.read_csv(rule_path)\n    db['id'] = range(1, len(db) + 1)\n\n    # columns_to_convert = ['current_pack_ids', 'current_product', 'recommended_product_id', 'recommended_pack_id']\n    # def convert_to_int(value):\n    #     # Convert float to string and split values separated by '|' only if the value is a string\n    #     if isinstance(value, str):\n    #         return '|'.join(str(int(float(part))) for part in value.split('|'))\n    #     else:\n    #         # If the value is already a float, convert it to an integer\n    #         return int(value)\n\n    # db = db.fillna(0.0)\n    # db[columns_to_convert] = db[columns_to_convert].applymap(convert_to_int)  \n\n\n    for item, val in filtered_dict.items():\n        data = pd.read_csv(filtered_data__dict.get(item))\n        try:\n            purchase = pd.read_csv(val)\n        except:\n            purchase_none_segements = findByAutoPilotIdAndSegementNamewithoutcluster(df=db, _id=dag_run_id,\n                                                                        segement_name=segements_name)\n\n            for index, seg in purchase_none_segements.iterrows():\n                print(f\"the segment name{seg.segment_name}\")\n    #                     ic(\"deleteing segement if they dont have purchase \")\n    #             deleteById(db=db, _ids=[['id']])\n            continue\n\n\n        #for 1 month purchase \n        #purchase = pd.read_csv('/log/btc_autopilot/etl_files/purchase_feb.csv')\n\n        #for 1 month purchase\n\n\n        for cluster in data['label'].unique():\n            segements_name = f\"{item}-{str(cluster)}\"\n            data_temp = data[data['label'] == cluster]\n            #purchase_filtered = purchase[purchase[msisdn_name].isin(data_temp[msisdn_name])]\n\n            purchase_filtered = purchase[purchase[msisdn_name].isin(data_temp[msisdn_name])]\n\n            print('len of purchase in association_process ', len(purchase_filtered))\n            rg = RuleGenerator(df=data_temp, dag_run_id=dag_run_id, cluster_name=item, cluster_number=cluster,\n                               purchase_filtered=purchase_filtered, pack_info=pack_info_df)\n            segements = findByAutoPilotIdAndSegementNameAll(df=db, _id=dag_run_id,\n                                                                         segment_name=segements_name,\n                                                                         cluster_number=int(cluster))\n\n\n            segements = segements.fillna(0.0)\n\n            columns_to_convert = ['current_pack_ids', 'current_product', 'recommended_product_id', 'recommended_pack_id']\n            print(segements[columns_to_convert])\n\n            def convert_to_int(value):\n                # Convert float to string and split values separated by '|' only if the value is a string\n                if isinstance(value, str):\n                    return '|'.join(str(int(float(part))) for part in value.split('|'))\n                else:\n                    # If the value is already a float, convert it to an integer\n                    return int(value)\n\n\n            segements[columns_to_convert] = segements[columns_to_convert].applymap(convert_to_int)\n\n            print(\"len of seg---------\",len(segements))\n            print(\"type of seg---------\",type(segements))\n\n\n            if segements is None:\n                print('segments is none')\n                continue\n\n            # pd.to_csv(\"seg.csv\")\n\n            # check service info for which segement to give which serives\n            # service_df = pd.read_sql_table('service_info', engine)\n            # segements_data = item.split(\"-\")\n            # trent = segements_data[0]\n            # rfm = segements_data[1]\n            # q = f\"trend == '{trent}' and rfm == '{rfm}'\"\n            # service_df = service_df.query(q).iloc[:, 3:]\n            # if not service_df.empty and service_df.all().all():\n            #     print(\"All boolean columns are True.\")\n            # else:\n            #     print(\"Not all boolean columns are True or no rows matched the query.\")\n\n            segements_new = rg.generate_rules(segements, temp_df)\n            # btc purpose\n            if rg.op is  not None :\n                output_d_list.append(rg.op)\n            # btc purpose end\n            for seg in segements_new:\n                if seg is not None:\n                    if seg.rule is not None:\n                        update(db=db, item_data=seg)\n                    else:\n                        print(\"deleting segement\")\n    #                     deleteById(db=db, _ids=[seg['id']])\n            segements_two = findByAutoPilotIdAndSegementNameAll(df=db, _id=dag_run_id,\n                                                                        segment_name=segements_name,\n                                                                        cluster_number=int(cluster))\n\n\n\n\n            print(\"segements_two======>\", segements_two)\n            print(\"segements_two rule ======>\", segements_two['rule'].unique())\n\n\n\n            for index, seg in segements_two.iterrows():\n    #                     print(f\"the segment name{seg.segment_name} and segment id is {seg.id}\")\n                if seg.samples < 10:\n                    ic(\"deleteing segement with less samples \")\n    #                 deleteById(db=db, _ids=[seg['id']])\n    #             if seg.rule is  None or len(seg.rule) < 3:\n                if isinstance(seg.rule, str) and len(seg.rule) >= 3:\n                    ic(\"deleteing segement if rule is none or len is less than 3 \")\n    #                         print(\"seg.id is \",seg.id)\n    #                 deleteById(db=db, _ids=[seg['id']])\n                if seg.recommended_product_id is  None or seg.recommended_product_id ==0:\n                    ic(\"deleteing segement if recommended_product_id is none \")\n    #                         print(\"seg.id is \",seg.id)\n    #                 deleteById(db=db, _ids=[seg['id']])\n    #             for rfm_seg in not_needed_rfm_segment:\n    #                 if  rfm_seg in seg.segment_name:\n    #                     ic(\"deleteing segements that have no needed rfm segments\")\n    #                     deleteById(db=db, _ids=[seg['id']])\n\n\n\n\n                # recommended_pid = int(seg.recommended_product_id)\n                # print('recommended_pid is ',recommended_pid)\n                # print('type recommended_pid is ',type(recommended_pid))\n\n                # print('type next_purchase_date_df is ',next_purchase_date_df.dtypes)\n\n                # print('len is ', weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id'] == recommended_pid])\n                # print(\"weekday is \", weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0])\n                # seg.top_purchased_day_1 = weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0]\n                # seg.top_purchased_day_2=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_2'].iloc[0]\n                # seg.top_purchased_day_3=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_3'].iloc[0]\n                # seg.next_purchase_date_range=next_purchase_date_df[next_purchase_date_df['recommended_product_id']==recommended_pid]['range'].iloc[0]\n                # SegementRepo.update(db=db, item_data=seg)\n\n\n    # btc purpose\n    path_op = os.path.join(file_path, \"output_data\")\n    Path(path_op).mkdir(parents=True, exist_ok=True)\n    dftnm = pd.concat(output_d_list)\n    dftnm['dag_run_id']=dag_run_id\n    dftnm.drop_duplicates(inplace=True)\n    \n    return dftnm"
              }
            }, {
              "id": "e10734f7-154b-58df-94d2-88b023882980",
              "operation": {
                "id": "f94b04d7-ec34-42f7-8100-93fe235c89f8",
                "name": "Source"
              },
              "parameters": {

              }
            }, {
              "id": "a545f7de-abef-e373-a119-a9080e24aabd",
              "operation": {
                "id": "e652238f-7415-4da6-95c6-ee33808561b2",
                "name": "Sink"
              },
              "parameters": {

              }
            }, {
              "id": "2b9edb6c-245d-69f5-f9a9-2dea226479cf",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nimport json\r\nimport os\r\nimport traceback\r\nimport numpy as np\r\nfrom pathlib import Path\r\nimport pickle\r\nfrom pathlib import Path\r\nimport requests\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nfrom sklearn.feature_selection import SelectKBest, chi2\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.feature_selection import SelectFromModel\r\nfrom sklearn.tree import DecisionTreeClassifier\r\n\r\n\r\ndef transform(dataframe):\r\n    dataframe = dataframe.toPandas()\r\n    file_path = '/home/tnmops/seahorse3_bkp/'\r\n\r\n    path_d = os.path.join(file_path, \"dict.pickle\")\r\n    features_path = os.path.join(file_path, \"features.pickle\")\r\n    \r\n    with open(path_d, 'rb') as handle:\r\n        data = pickle.load(handle)\r\n    \r\n    filtered_dict = {k: v for k, v in data.items()}\r\n    result_features = {}\r\n    \r\n    for item, val in filtered_dict.items():\r\n        df = pd.read_csv(val)\r\n        features = get_features(df)\r\n        result_features[item] = features\r\n        print(f\"The features are {features} for the segment {item}\")\r\n    \r\n    \r\n    write_pickle(data=result_features, path=features_path)\r\n    \r\n    return features\r\n\r\n\r\n\r\ndef get_features(df):\r\n    numerical_cols = df.select_dtypes(include=[np.number]).columns\r\n    df=df[numerical_cols]\r\n    X = df.drop(['msisdn', 'label'], axis=1)\r\n    y = df['label']\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n    print(f\"X_train.shape = {X_train.shape}, X_test.shape =  {X_test.shape}\")\r\n    # sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\r\n    sel = SelectFromModel(DecisionTreeClassifier(max_leaf_nodes=12))\r\n    sel.fit(X_train, y_train)\r\n    sel.get_support()\r\n    features = X_train.columns[sel.get_support()]\r\n    print(\"the feature selection features are \", features)\r\n    selected_features = [feature for feature in features if feature != 'label']\r\n    \r\n    return selected_features\r\n    \r\n    \r\ndef write_pickle(data, path):\r\n    with open(path, 'wb') as handle:\r\n        print(f'opened {path} ')\r\n        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n        print(f'data dumped  {data}')"
              }
            }],
            "connections": [{
              "from": {
                "nodeId": "e10734f7-154b-58df-94d2-88b023882980",
                "portIndex": 0
              },
              "to": {
                "nodeId": "22ded90b-ab97-cd27-1e1b-10270de81f58",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "abfd8b7c-e546-ab0b-4293-68f6faaaa4bb",
                "portIndex": 0
              },
              "to": {
                "nodeId": "9d76f61e-6901-2e98-020d-ddea4c9be1c5",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "c2f0a205-ed28-04e7-ed27-0d0a9b49d7fa",
                "portIndex": 0
              },
              "to": {
                "nodeId": "abfd8b7c-e546-ab0b-4293-68f6faaaa4bb",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "abfd8b7c-e546-ab0b-4293-68f6faaaa4bb",
                "portIndex": 0
              },
              "to": {
                "nodeId": "85dd0b8f-b63a-351c-ecc8-a9930ba70944",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "29640985-7273-3f93-ff88-eef6aaeb69f2",
                "portIndex": 0
              },
              "to": {
                "nodeId": "c2f0a205-ed28-04e7-ed27-0d0a9b49d7fa",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "22ded90b-ab97-cd27-1e1b-10270de81f58",
                "portIndex": 0
              },
              "to": {
                "nodeId": "2b9edb6c-245d-69f5-f9a9-2dea226479cf",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "abfd8b7c-e546-ab0b-4293-68f6faaaa4bb",
                "portIndex": 0
              },
              "to": {
                "nodeId": "cd27063b-f4c3-cd42-76b1-b08804b1baff",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "77150d04-ffa4-4e20-b24b-4dd3b2cf2a03",
                "portIndex": 0
              },
              "to": {
                "nodeId": "29640985-7273-3f93-ff88-eef6aaeb69f2",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "cd27063b-f4c3-cd42-76b1-b08804b1baff",
                "portIndex": 0
              },
              "to": {
                "nodeId": "a545f7de-abef-e373-a119-a9080e24aabd",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "2b9edb6c-245d-69f5-f9a9-2dea226479cf",
                "portIndex": 0
              },
              "to": {
                "nodeId": "77150d04-ffa4-4e20-b24b-4dd3b2cf2a03",
                "portIndex": 0
              }
            }]
          },
          "thirdPartyData": {
            "gui": {
              "name": "Inner workflow of custom transformer",
              "nodes": {
                "e10734f7-154b-58df-94d2-88b023882980": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 4934,
                    "y": 4868
                  }
                },
                "a545f7de-abef-e373-a119-a9080e24aabd": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5112,
                    "y": 5727
                  }
                },
                "22ded90b-ab97-cd27-1e1b-10270de81f58": {
                  "uiName": "k_modes",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4822,
                    "y": 4971
                  }
                },
                "9d76f61e-6901-2e98-020d-ddea4c9be1c5": {
                  "uiName": "churn_prediction",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4548,
                    "y": 5573
                  }
                },
                "29640985-7273-3f93-ff88-eef6aaeb69f2": {
                  "uiName": "matrix_filter",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4780,
                    "y": 5255
                  }
                },
                "cd27063b-f4c3-cd42-76b1-b08804b1baff": {
                  "uiName": "campaign response model",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5009,
                    "y": 5572
                  }
                },
                "abfd8b7c-e546-ab0b-4293-68f6faaaa4bb": {
                  "uiName": "rule_generation",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4740,
                    "y": 5433
                  }
                },
                "c2f0a205-ed28-04e7-ed27-0d0a9b49d7fa": {
                  "uiName": "association_process",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4761,
                    "y": 5341
                  }
                },
                "85dd0b8f-b63a-351c-ecc8-a9930ba70944": {
                  "uiName": "next_purchase_day",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4774,
                    "y": 5574
                  }
                },
                "2b9edb6c-245d-69f5-f9a9-2dea226479cf": {
                  "uiName": "feature_selection",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4810,
                    "y": 5070
                  }
                },
                "77150d04-ffa4-4e20-b24b-4dd3b2cf2a03": {
                  "uiName": "rule extraction",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4798,
                    "y": 5160
                  }
                }
              }
            }
          },
          "publicParams": []
        }
      }
    }, {
      "id": "5665b055-50b8-1edf-40a9-d5b884a0ee2f",
      "operation": {
        "id": "65240399-2987-41bd-ba7e-2944d60a3404",
        "name": "Create Custom Transformer"
      },
      "parameters": {
        "inner workflow": {
          "workflow": {
            "nodes": [{
              "id": "40bb0f20-c5c3-ba2a-042f-7b46af063106",
              "operation": {
                "id": "e652238f-7415-4da6-95c6-ee33808561b2",
                "name": "Sink"
              },
              "parameters": {

              }
            }, {
              "id": "54df344a-6d6e-7975-7b5d-96719277ce68",
              "operation": {
                "id": "f94b04d7-ec34-42f7-8100-93fe235c89f8",
                "name": "Source"
              },
              "parameters": {

              }
            }, {
              "id": "f04a5e13-bce6-62f8-6a7c-e1e716dc959e",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom urllib.parse import quote  \nfrom datetime import datetime\nfrom icecream import ic\nfrom pathlib import Path\nimport dask.array as da\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\npurchase_location = '/home/tnmops/seahorse3_bkp/'\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\netl_location = '/home/tnmops/seahorse3_bkp/'\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nlog_file = '/home/tnmops/seahorse3_bkp/'\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\n\n\n\n\n\ndef write_pickle(data, path):\n    with open(path, 'wb') as handle:\n        print(f'opened {path} ')\n        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        print(f'data dumped  {data}')\n        \n\n\ndef get_features(df):\n    numerical_cols = df.select_dtypes(include=[np.number]).columns\n    df=df[numerical_cols]\n    X = df.drop(['msisdn', 'label'], axis=1)\n    y = df['label']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    print(f\"X_train.shape = {X_train.shape}, X_test.shape =  {X_test.shape}\")\n    # sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\n    sel = SelectFromModel(DecisionTreeClassifier(max_leaf_nodes=12))\n    sel.fit(X_train, y_train)\n    sel.get_support()\n    features = X_train.columns[sel.get_support()]\n    print(\"the feature selection features are \", features)\n    selected_features = [feature for feature in features if feature != 'label']\n    \n    return selected_features\n    \n    \n\n\n\ndef feature_selection(dag_run_id):\n    try:\n        path_d = os.path.join(ml_location, \"dict.pickle\")\n        features_path = os.path.join(ml_location, \"features.pickle\")\n        with open(path_d, 'rb') as handle:\n            data = pickle.load(handle)\n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\n        # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\n        # for key, value in data.items():\n        #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n            \n        filtered_dict = {k: v for k, v in data.items()}\n        result_features = {}\n        for item, val in filtered_dict.items():\n            # val is the path item is the segment name\n            df = pd.read_csv(val)\n            features = get_features(df)#-----------------------------\n            result_features[item] = features\n            print(f\"the features are {features} for the segement {item}\")\n        \n        return df\n        write_pickle(data=result_features, path=features_path)\n    except Exception as e:\n        print(e)\n        #raise HTTPException(status_code=400, detail=\"error occoureds in k_modes\" + str(e))\n    #return schemas.BaseResponse(statusCode=200, message=\"success\", status=\"success\")\n    \n    \ndef transform(dataframe):\n    df = feature_selection(\"manual__2023-07-10T11:06:51\" )\n    return df"
              }
            }, {
              "id": "9d4ef19c-49fd-3be7-f2b5-995298fa74e2",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nimport sklearn\nimport json\nimport dask.dataframe as dd\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nfrom kmodes.kmodes import KModes\n# import configuration.config as cfg\n# import configuration.features as f\nimport traceback\nimport numpy as np\n# from fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom pathlib import Path\nimport requests\n# from sql_app.repositories import AssociationRepo\nimport pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\nfrom urllib.parse import quote  \n\n\n\n\n\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\n\n\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\npurchase_location = '/home/tnmops/seahorse3_bkp/'\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\netl_location = '/home/tnmops/seahorse3_bkp/'\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nlog_file = '/home/tnmops/seahorse3_bkp/'\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\npack_name = \"product_name\"\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCUSTOMER_NEEDED_COLUMN = [\n'onnet_revenue',\n'onnet_usage_da',\n'onnet_usage',\n'onnet_voice_count',\n'onnet_da_revenue',\n'offnet_revenue',\n'offnet_usage_da',\n'offnet_usage',\n'offnet_voice_count',\n'offnet_da_revenue',\n'idd_revenue',\n'idd_usage_da',\n'idd_usage',\n'idd_voice_count',\n'idd_da_revenue',\n'voice_rmg_revenue',\n'voice_rmg_usage_da',\n'voice_rmg_usage',\n'voice_rmg_count',\n'voice_rmg_da_revenue',\n'data_rmg_revenue',\n'data_rmg_usage_da',\n'data_rmg_usage',\n'data_rmg_da_revenue',\n'data_revenue',\n'data_usage_da',\n'data_usage',\n'da_data_rev',\n'sms_revenue',\n'sms_usage_da',\n'sms_usage',\n'sms_da_revenue',\n'sms_idd_revenue',\n'sms_idd_usage_da',\n'sms_idd_usage',\n'sms_idd_da_revenue',\n'magik_voice_amount',\n'rbt_subscription_rev',\n'emergency_credit_rev',\n'package_revenue',\n'voice_rev',\n'sms_rev',\n'onn_rev',\n'off_rev',\n'total_data_rev',\n'vas_rev',\n'vas_rev_others',\n'total_revenue',\n'total_voice_count',\n'total_voice_duration',\n'total_mainaccount_data_usage',\n'total_sms_count',\n'total_package_count',\n'total_other_vas_count',\n'total_voice_usage',\n'total_data_usage',\n'total_sms_usage']\n\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n\n\ndef perform_k_modes(df, path, dag_run_id):\n    try:\n        # K_modes_location = os.path.join(cfg.Config.ml_location, dag_run_id, \"kmodes\")\n        # Path(K_modes_location).mkdir(parents=True, exist_ok=True)\n        objList = df.select_dtypes(include=\"object\").columns\n        X = df[objList]\n        X = pd.get_dummies(X)\n\n        # elbow = find_k_kmodes(X=X)\n\n        km = KModes(n_clusters=2)\n        km.fit(X)\n        X['label'] = km.labels_\n        df['label'] = km.labels_\n        if df['label'].unique() is None or df['label'][0] is None :\n             df['label'] =0\n\n        print(df['label'].value_counts())\n        df.to_csv(path, header=True, index=False)\n        print(f\"outputed to path {path}\")\n        # for label in df['label'].unique():\n        #     df_temp = df[df['label'] == label]\n        #     name_path = f\"{df_name}_cluster_{label}.csv\"\n        #     output_path = os.path.join(K_modes_location, dag_run_id, name_path)\n        #     name_list[name_path] = output_path\n        #     print(f\"the output path is \", output_path)\n        #     df_temp.to_csv(output_path, header=True, index=False)\n        #     print(f\"done exporting \")\n\n        return df\n    except Exception as e:\n        print(e)\n        \n\n\n\n\ndef k_modes(dag_run_id):\n    try:\n        path_d = os.path.join(ml_location, \"dict.pickle\")\n        path_dd = os.path.join(ml_location, \"cluster_analysis.csv\")\n        with open(path_d, 'rb') as handle:\n            data = pickle.load(handle)\n        \n        \n#         for key, value in data_dict.items():\n#             data_dict[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n\n        # for key, value in data.items():\n        #     if value is None:\n        #         continue\n        #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\n        # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\n        filtered_dict = {k: v for k, v in data.items()}\n        data_list = []\n\n        for item, val in filtered_dict.items():\n            # val is the path item is the segment name\n            df = pd.read_csv(val)\n            print(f\"the cluster is {item}\")\n            val = perform_k_modes(df, val, dag_run_id)\n            val['segement_name'] = item\n            data_list.append(val)\n\n        df_final = pd.concat(data_list)\n        rfm_path = os.path.join(ml_location, \"rfm\")\n        rfm_dd = dd.read_parquet(rfm_path)\n        rfm_df=rfm_dd.compute()\n\n        df_final1=df_final[['msisdn','trend','Segment','label']]\n        df_final1=pd.merge(df_final1,rfm_df[['msisdn','Recency'\t,'Revenue'\t,'Frequency',\t'R_Score',\t'F_Score',\t'M_Score',\t'RFM_Segment']],\n                           on = 'msisdn',how = 'left')\n                           \n\n        df_final1['dag_run_id']=str(dag_run_id)\n        df_final1.to_csv(os.path.join(ml_location, \"rfm_segement_mode.csv\"), index=False, header=True)\n#         for df_chunk in pd.read_csv(os.path.join(ml_location, \"rfm_segement_mode.csv\"),\n#                                     chunksize=10000):\n#             # insert each chunk into database table\n#             df_chunk.to_sql('APA_rfm_segement_mode', engine, if_exists='append', index=False)\n\n        df_final = df_final.groupby(['segement_name', 'label']).agg({\"msisdn\": \"count\"}).reset_index()\n        df_final['dag_run_id']=str(dag_run_id)\n        \n        df_final.to_csv(path_dd,index=False,header = True)\n#         for df_chunk in pd.read_csv(os.path.join(ml_location, \"cluster_analysis.csv\"),chunksize=10000):\n#             # insert each chunk into database table\n#             df_chunk.to_sql('APA_rfm_segement_mode_groupby', engine, if_exists='append', index=False)\n        \n        return df_final\n\n\n    except Exception as e:\n        print(e)\n        \n        \n\n\n\n\ndef transform(dataframe):\n    df = k_modes(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }, {
              "id": "67f08fff-ef82-9059-29c9-da33161a19b5",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {

              }
            }],
            "connections": [{
              "from": {
                "nodeId": "54df344a-6d6e-7975-7b5d-96719277ce68",
                "portIndex": 0
              },
              "to": {
                "nodeId": "9d4ef19c-49fd-3be7-f2b5-995298fa74e2",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "9d4ef19c-49fd-3be7-f2b5-995298fa74e2",
                "portIndex": 0
              },
              "to": {
                "nodeId": "f04a5e13-bce6-62f8-6a7c-e1e716dc959e",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "f04a5e13-bce6-62f8-6a7c-e1e716dc959e",
                "portIndex": 0
              },
              "to": {
                "nodeId": "67f08fff-ef82-9059-29c9-da33161a19b5",
                "portIndex": 0
              }
            }]
          },
          "thirdPartyData": {
            "gui": {
              "name": "Inner workflow of custom transformer",
              "nodes": {
                "9d4ef19c-49fd-3be7-f2b5-995298fa74e2": {
                  "uiName": "k_modes",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4851,
                    "y": 4996
                  }
                },
                "40bb0f20-c5c3-ba2a-042f-7b46af063106": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5236,
                    "y": 5247
                  }
                },
                "54df344a-6d6e-7975-7b5d-96719277ce68": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 4857,
                    "y": 4902
                  }
                },
                "67f08fff-ef82-9059-29c9-da33161a19b5": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5192,
                    "y": 5116
                  }
                },
                "f04a5e13-bce6-62f8-6a7c-e1e716dc959e": {
                  "uiName": "feature_selection",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5055,
                    "y": 5015
                  }
                }
              }
            }
          },
          "publicParams": []
        }
      }
    }, {
      "id": "61e42a89-d515-6c47-5856-b0329358e276",
      "operation": {
        "id": "643d8706-24db-4674-b5b4-10b5129251fc",
        "name": "Transform"
      },
      "parameters": {
        "Parameters of input Transformer": {

        },
        "report type": {
          "Extended report": {

          }
        }
      }
    }, {
      "id": "3ce83530-6bff-53a8-4f0e-3617ef1503bc",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_feb.csv\",\n            \"m2\": \"purchase_m1_20230206203137.csv\",\n            \"m3\": \"purchase_m2_20230206203137.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n    \nclass UsageCategory(object):\n    def __init__(self, data):\n        self.data = data\n        self.current_iteration_month = None\n        # self.categorize()\n\n    def voice_band(self, x, m):\n        \n        if x == 0.0000:\n            y = 'a)Zero'\n        elif (x > 0) & (x <= 8):\n            y = 'b)<=8'\n        elif (x > 8) & (x <= 20):\n            y = 'c)8 - 20'\n        elif (x > 20) & (x <= 40):\n            y = 'd)20 - 40'\n        elif (x > 40) & (x <= 100):\n            y = 'e)40 - 100'\n        elif (x > 100) & (x <= 200):\n            y = 'f)100 - 200'\n        elif (x > 200) & (x <= 500):\n            y = 'g)200 - 500'\n        elif (x > 500) & (x <= 1000):\n            y = 'h)500 - 1000'\n        else:\n            y = 'i)1000+'\n        return m + \"_\" + y\n\n    def data_band(self, x, m):\n        \n        if x == 0.0000:\n            y = 'a)Zero'\n        elif (x > 0) & (x <= 50):\n            y = 'b)0-50 MB'\n        elif (x > 50) & (x <= 100):\n            y = 'c)50-100 MB'\n        elif (x > 100) & (x <= 250):\n            y = 'd)100-250 MB'\n        elif (x > 250) & (x <= 512):\n            y = 'e)250-512 MB'\n        elif (x > 512) & (x <= 1536):\n            y = 'f)512 MB-1.5 GB'\n        elif (x > 1536) & (x <= 3072):\n            y = 'g)1.5-3 GB'\n        else:\n            y = 'h)3 GB +'\n        return m + \"_\" + y\n\n    def categorize(self):\n        months = ['m1', 'm2', 'm3']\n        temp_df = None\n        for month in months:\n            self.current_iteration_month = month\n\n            needed_col = CUSTOMER_CATEG_NEEDED_COL\n            dataset = self.data.get(month)[needed_col]\n            dataset = dataset.fillna(0)\n            # 0 index for inbundle 1 index for outbundled  2 index for total\n            voice_col = CUSTOMER_VOICE_COL_NAME[0]\n            data_col = CUSTOMER_DATA_COL_NAME[0]\n            dataset[month + '_voice_band'] = dataset[voice_col].apply(self.voice_band, args=(month,))\n            dataset[month + '_data_band'] = dataset[data_col].apply(self.data_band, args=(month,))\n            dataset[month +'_'+ voice_col] = dataset[voice_col]\n            dataset[month +'_'+ data_col] = dataset[data_col]\n            dataset = dataset.drop(columns=[voice_col, data_col])\n            if temp_df is None:\n                temp_df = dataset\n            else:\n                temp_df = temp_df.merge(dataset, on='msisdn', how=\"left\")\n                temp_df[month + '_voice_band'] = temp_df[month + '_voice_band'].fillna(month + \"_\"'a)Zero')\n                temp_df[month + '_data_band'] = temp_df[month + '_data_band'].fillna(month + \"_\"'a)Zero')\n                dataset[month +'_'+ voice_col] = dataset[month +'_'+ voice_col].fillna(0)\n                dataset[month +'_'+ data_col] = dataset[month +'_'+ data_col].fillna(0)\n\n        return temp_df\n        \n        \n\n\ndef arpu_trend(data):\n    months = usage_no_months\n    temp_df = None\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    #for daily summarised\n\n    usage={}\n\n    for month in usage_no_months:\n        usage[month] = dd.read_csv(\n            os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)))\n        \n    usage_3m = dd.concat(list(usage.values()))\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    temp_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n\n    print('len(temp_df) is',len(temp_df) )\n\n    #for daily summarised\n\n\n\n    for month in months:\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n        # total_voice_rev = CUSTOMER_VOICE_REVENUE[2]\n        needed_col = [MSISDN_COL_NAME, total_revenue]\n        df = data.get(month)[needed_col]\n        df['tot_rev'] = df[total_revenue]\n        df = df.drop(total_revenue, axis=1)\n        df = df.fillna(0)\n        #df = otliner_removal(df)\n\n        df['rev_segment'] = df.tot_rev.apply(put_revenue_segement)\n        df = df.rename(columns={\"rev_segment\": \"rev_segment_\" + month})\n        df['tot_rev' + month] = df['tot_rev']\n\n        df = df.drop('tot_rev', axis=1)\n        \n\n        if temp_df is None:\n            temp_df = df\n        else:\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n            temp_df[\"rev_segment_\" + month] = temp_df[\"rev_segment_\" + month].fillna(\"Zero\")\n            temp_df[\"tot_rev\" + month] = temp_df[\"tot_rev\" + month].fillna(0)\n\n    temp_df['tot_sum'] = temp_df['tot_revm1'] + temp_df['tot_revm2'] + temp_df['tot_revm3']\n    temp_df_non = temp_df\n    print(\"len of temp_df_non  after filtering is is  \", len(temp_df_non))\n    replace_map = get_banding_confitions().get(\"common\")\n\n    df1 = temp_df_non.replace(replace_map)\n    print(type(temp_df_non))\n\n    #to convert rev_segment_cols to int \n    rev_segment_cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    df1[rev_segment_cols] = df1[rev_segment_cols].astype('int32')\n    #to convert rev_segment_cols to int \n\n    cols = df1.columns[df1.columns.str.startswith('tot_revm')].tolist()\n\n    #to find trend using segment\n    #cols = df1.columns[df1.columns.str.startswith('rev_segment_')].tolist()\n    #to find trend using segment\n\n    for col in cols:\n        df1[col] = df1[col].astype('int')\n        df1[col] = df1[col]\n\n    output = df1[cols].map_partitions(lambda part: part.apply(lambda x: pandas_wrapper(x), axis=1), meta=tuple)\n    output1 = output.to_frame(name='trend')\n    op = dd.concat([df1, output1], axis=1)\n\n    return op\n    \n    \n\n\ndef get_banding_confitions():\n        return {\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\n                               6: \"high_high\"}\n        }\n        \n        \n        \n\n\n\n\ndef put_revenue_segement(x):\n    y = 'Zero'\n    if x == 0.0000:\n        y = 'Zero'\n    elif (x > 0) & (x <= 16):\n        y = 'very_low'\n    elif (x > 16) & (x <= 36):\n        y = 'low'\n    elif (x > 36) & (x <= 61):\n        y = 'medium'\n    elif (x > 61) & (x <= 175):\n        y = 'high'\n    else:\n        y = 'high_high'\n    return y\n    \n    \ndef trends(numbers):\n    if len(numbers) != 3:\n        return \"Invalid input\"\n\n    if numbers[0] == numbers[1] == numbers[2]:\n        return \"flat\"\n    elif numbers[0] > numbers[1] > numbers[2]:\n        return \"Downtrend\"\n    elif numbers[0] < numbers[1] < numbers[2]:\n        return \"Uptrend\"\n    else:\n        return \"Zigzag\"\n        \n        \n\n\ndef pandas_wrapper(row):\n    return trends([row[2], row[1], row[0]])\n    \n    \n\ndef usage_process(dag_run_id):\n    try:\n\n\n\n        file_name_dict = get_file_names()\n        print(\"finding trend ongoing \")\n        data = {}\n        for month in usage_no_months:\n            data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\n                                      dtype= CUSTOMER_DTYPES)\n\n        # ic(\"the length of m1 in usage \", len(data['m1']))\n        # ic(\"the length of m2 in usage \", len(data['m2']))\n        # ic(\"the length of m3 in usage \", len(data['m3']))\n        # ic(\"the length of unique msisdn m3 in usage \", data['m1']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in usage \", data['m2'\n        #                                                     '']['msisdn'].nunique().compute())\n        # ic(\"the length of unique msisdn m3 in usage \", data['m3']['msisdn'].nunique().compute())\n\n        trend_df = arpu_trend(data)\n\n        # ic(\"the trend value counts is \", trend_df['trend'].value_counts().compute())\n\n        path = os.path.join(ml_location, \"trend\")\n        Path(path).mkdir(parents=True, exist_ok=True)\n        #trend_df = trend_df.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n\n        trend_df.to_parquet(path)\n        uc = UsageCategory(data)\n        path = os.path.join(ml_location, \"usage_bands\")\n        print(\"finding usage band ongoing \")\n        df = uc.categorize()\n\n        #to fill null values if there is in data and voice band \n        # df['m1_voice_band'] = df['m1_voice_band'].fill('m1_Zero')\n        # df['m1_data_band '] = df['m1_data_band'].fill('m1_Zero')\n        # df['m2_voice_band'] = df['m2_voice_band'].fill('m2_Zero')\n        # df['m2_data_band '] = df['m2_data_band'].fill('m2_Zero')\n        # df['m3_voice_band'] = df['m3_voice_band'].fill('m3_Zero')\n        # df['m3_data_band '] = df['m3_data_band'].fill('m3_Zero')\n\n        #to fill null values if there is in data and voice band \n\n\n\n\n        df.to_parquet(path)\n        print(\"finding usage band done \")\n        return df.compute()\n    except Exception as e:\n        print(e)\n        \n        \n\n\n\n\n\n\n\ndef transform(dataframe):\n    df = usage_process(\"manual__2023-07-10T11:06:51\")\n    return df"
      }
    }, {
      "id": "63d7e415-bda3-ac95-06d8-d4289cbf19e5",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\nimport vaex\nimport vaex.utils\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_march_20230809132937.csv\",\n            \"m2\": \"recharge_feb_20230809132937.csv\",\n            \"m3\": \"recharge_jan_20230809132937.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_march_20230517180929.csv\",\n            \"m2\": \"weekwise_feb_20230517180929.csv\",\n            \"m3\": \"weekwise_jan_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_march_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_jan_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_nov_full_df.csv\",\n            \"p2\": \"purchase_dec_full_df.csv\",\n            \"p3\": \"purchase_jan_full_df.csv\",\n            \"p4\": \"purchase_feb_full_df.csv\",\n            \"p5\": \"purchase_march_full_df.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n\ndef segment_data(recharge_trend_usage_rfm, dag_run_id):\n    path_dict = {}\n    try:\n\n        print('inside segment_data')\n        #recharge_trend_usage_rfm['Segment'] = recharge_trend_usage_rfm['Segment'].apply(lambda x: str(x), vectorize=True)\n        for trend in recharge_trend_usage_rfm['trend'].unique():\n            print('inside for loop1')\n\n            for segement in recharge_trend_usage_rfm['Segment'].unique():\n                # if segement in cfg.Config.not_needed_rfm_segment:\n                #     continue\n                print('inside for loop2')\n                print('recharge_trend_usage_rfm.columns',recharge_trend_usage_rfm.columns)\n                for service_band in recharge_trend_usage_rfm['service_band'].unique():\n                    temp = recharge_trend_usage_rfm[\n                        (recharge_trend_usage_rfm['trend'] == trend) & (recharge_trend_usage_rfm['Segment'] == segement)& (recharge_trend_usage_rfm['service_band'] == service_band)]\n\n                    name = f\"{trend}-{segement}-{service_band}\"\n                    name = r\"\" + name\n                    file_name = f\"{name}.csv\"\n                    print(' file_name is ', file_name)\n\n                    length = len(temp)\n                    if length > 20:\n                        path = os.path.join(ml_location,  file_name)\n                        print(f\"the length is suff {length} file name {name} \")\n                        # print('path_dict before is' ,path_dict)\n\n                        path_dict[str(name)] = str(path)\n                        # print('path_dict after  is' ,path_dict)\n\n   \n                        temp.to_csv(r\"\" + path,index=False)\n                        print('file_exported')\n\n                    else:\n                        print(f\"the length is  insuff {length} file name {name} \")\n        # print('here')\n        path_d = os.path.join(ml_location, \"dict.pickle\")\n        print('path_d is', path_d)\n        print('path_dict is', path_dict)\n        with open(path_d, 'wb') as handle:\n            print('opened')\n            pickle.dump(path_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n            print('path_dict dumped ')\n\n    except Exception as e:\n        print(\"error occoured in segment_data\")\n        traceback.print_exc()\n        raise Exception(e)\n        \n        \ndef no_of_purchase_days_count(dag_run_id):\n    \n    msisdn_name =  MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    purchase_no_months = ['m1', 'm2', 'm3']\n    cdr_date = RECHARGE_TRANSACTION_PURCHASE_DATE_NAME\n    \n    print(file_name_dict)\n    print(\"finding no_of_purchase_days_count ongoing \")\n    purchase = {}\n    \n    for month in purchase_no_months:\n        purchase[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"purchase\").get(month)),)\n\n    purchase_3m = dd.concat(list(purchase.values()))\n    \n    purchase_3m_msisdn = purchase_3m[msisdn_name].unique().compute()\n    print('len(purchase_3m_msisdn) is',len(purchase_3m_msisdn) )\n\n    purchase_dict = {msisdn_name: purchase_3m_msisdn}\n    #print(usage_3m_dict)\n\n    print('len(usage_3m_msisdn) is',len(purchase_3m_msisdn) )\n        \n    purchase_base_df = dd.from_pandas(pd.DataFrame(purchase_dict), npartitions=1)\n    \n    \n    lis_df  = [purchase['m1'],purchase['m2'],purchase['m3']]\n    result = []\n    weekdays = []\n\n    def extract_day_of_week(series):\n            return series.dt.day_name().unique()\n\n    for i, df in enumerate(lis_df):\n        df[cdr_date] = dd.to_datetime(df[cdr_date])\n        df = df.groupby(msisdn_name)[cdr_date].apply(extract_day_of_week).reset_index()\n        weekdays.append(df)\n\n\n    for i, df in enumerate(lis_df, start =1):\n        new_column = f\"m{i}_no_of_days\"\n        df[cdr_date] = dd.to_datetime(df[cdr_date])\n        resultss = df.groupby(msisdn_name)[cdr_date].nunique().rename(new_column).reset_index()\n        result.append(resultss)\n    \n    df = purchase_base_df[[msisdn_name]].merge(result[0], on=msisdn_name, how='left').merge(result[1], on=msisdn_name, how='left').merge(result[2], on=msisdn_name, how='left').merge(weekdays[0], on=msisdn_name, how='left').merge(weekdays[1], on=msisdn_name, how='left').merge(weekdays[2], on=msisdn_name, how='left')\n    df = df.rename(columns={'cdr_date_x':'m1_weekdays', 'cdr_date_y': 'm2_weekdays','cdr_date': 'm3_weekdays'})\n    column = ['m1_weekdays','m2_weekdays','m3_weekdays']\n    \n    for col in column:\n        df[col] = df[col].apply(lambda x: ','.join(str(i) for i in x) if isinstance(x, list) else str(x))\n        df[col] = df[col].fillna(np.nan).replace('nan', '', regex=True)\n        \n    df =df.fillna(0)\n    \n    def assign_label(column):\n            labels = np.empty(len(column), dtype='object')\n            labels[(column == 0)] = 'A_[0]'\n            labels[(column > 0) & (column <= 5)] = 'B_[0_5]'  \n            labels[(column > 5 ) & (column <= 10)] = 'C_[5_10]'  \n            labels[(column > 10) & (column <= 15)] = 'D_[10_15]'  \n            labels[(column > 15) & (column <= 20)] = 'E_[15_20]'\n            labels[(column > 20) & (column <= 25)] = 'F_[20_25]'\n            labels[column >  25] = 'G_[25 +]'\n            return labels\n\n    columns = ['m1_no_of_days','m2_no_of_days','m3_no_of_days']\n    \n    for column in columns:\n        new_column = column + '_bands'\n        df[new_column] = df[column].map_partitions(assign_label, meta='object')\n    df = df.reset_index(drop =True)\n    \n    no_of_purchase_days_count_path = os.path.join(ml_location, \"no_of_purchase_days_count\")\n    Path(no_of_purchase_days_count_path).mkdir(parents=True, exist_ok=True)\n    print(\"no_of_purchase_days_count   file output is ongoing \")\n    print('df.dtypes',df.dtypes)\n    df.to_parquet(no_of_purchase_days_count_path)\n    print(\"no_of_purchase_days_count   file output is completed \")\n    \n\n\ndef calculate_pct_drop_daily_weekly(dag_run_id):\n    \n    msisdn_name =  MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    weekly_daily_no_months = ['m1', 'm2', 'm3']\n    print(file_name_dict)\n    print(\"finding calculate_pct_drop ongoing \")\n    weekly_daily = {}\n    for month in weekly_daily_no_months:\n        weekly_daily[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"weekly_daily\").get(month)))\n                                  \n\n    usage_3m = dd.concat(list(weekly_daily.values()))\n    \n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n    #print(usage_3m_dict)\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n    print(usage_base_df,'usage_base_df')\n    \n    m1 = weekly_daily['m1']\n    m2 = weekly_daily['m2']\n    m3 = weekly_daily['m3']\n   \n    \n    \n\n    etl_3_df = usage_base_df.merge(m1, on=msisdn_name, how='left').merge(m2, on=msisdn_name, how='left').merge(m3, on=msisdn_name, how='left')\n    print(etl_3_df)\n    etl_3_df = etl_3_df.fillna(0)\n    etl_3_df = dd.merge(dd.merge(m1, m2, on=msisdn_name), m3, on=msisdn_name)\n    new_column_names = {}\n    for col_name in etl_3_df.columns:\n        if col_name == msisdn_name:\n            new_col_name = col_name\n        elif col_name.endswith('_x'):\n            new_col_name = 'm1_' + col_name[:-2]\n        elif col_name.endswith('_y'):\n            new_col_name = 'm2_' + col_name[:-2]\n        else:\n            new_col_name = 'm3_' + col_name\n        new_column_names[col_name] = new_col_name\n\n    etl_3_df = etl_3_df.rename(columns=new_column_names)\n    etl_3_df = etl_3_df[['msisdn', 'm1_weekly_avg_voice_usage', 'm1_weekly_avg_data_usage','m1_daily_avg_voice_usage','m1_daily_avg_data_usage',\n                         'm2_weekly_avg_voice_usage', 'm2_weekly_avg_data_usage','m2_daily_avg_voice_usage','m2_daily_avg_data_usage',\n                         'm3_weekly_avg_voice_usage', 'm3_weekly_avg_data_usage','m3_daily_avg_voice_usage','m3_daily_avg_data_usage']]\n    df = etl_3_df.copy()\n    revenue_types = ['weekly_avg_voice_usage', 'weekly_avg_data_usage', 'daily_avg_voice_usage','daily_avg_data_usage']\n    for i in range(1, 3):\n        for rt in revenue_types:\n            col1 = f'm{i+1}_{rt}'\n            col2 = f'm{i}_{rt}'\n            pct_drop_col = f'{col1}_{col2}_pt_drop'\n            df[pct_drop_col] = (df[col1] - df[col2]) / df[col1] * 100\n            df[pct_drop_col] = df[pct_drop_col].replace([np.inf, -np.inf], np.nan)\n            df[pct_drop_col] = df[pct_drop_col].fillna(0)\n            \n    columns_to_select = ['msisdn'] + [col for col in df.columns if 'pt_drop' in col]\n    df = df.loc[:, columns_to_select]\n    \n        \n    def assign_label(column):\n        labels = np.empty(len(column), dtype='object')\n        labels[column == 0] = 'd)0'  \n        labels[(column > 0) & (column <= 50)] = 'e)0_50'  \n        labels[(column > 50) & (column <= 100)] = 'f)50_100'  \n        labels[column > 100] = 'g)>100'\n        labels[(column < 0) & (column >= -50)] = 'c)<0_-50' \n        labels[(column < -50) & (column >= -100)] = 'b)-50_-100'  \n        labels[column < -100] = 'a)>-100'\n        return labels\n\n\n    columns = ['m2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop','m2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop',\n    'm2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop','m2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop','m3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop','m3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop','m3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop','m3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop']\n        \n\n    for column in columns:\n        new_column = column + '_band'\n        df[new_column] = df[column].map_partitions(assign_label, meta='object')\n    \n    \n    daily_weekly_pct_drop = os.path.join(ml_location,\"daily_weekly_pct_drop\")\n    Path(daily_weekly_pct_drop).mkdir(parents=True, exist_ok=True)\n    print(\"daily_weekly_pct_drop  file output is going on \")\n\n    df.to_parquet(daily_weekly_pct_drop)\n    \n    \n\ndef delta_calculation_weekwise(dag_run_id):\n    \n    msisdn_name =  MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    weekwise_no_months = ['m1', 'm2', 'm3']\n    print(file_name_dict)\n    print(\"finding calculate_pct_drop ongoing \")\n    weekwise = {}\n    for month in weekwise_no_months:\n        weekwise[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"weekwise\").get(month)))\n\n    usage_3m = dd.concat(list(weekwise.values()))\n    \n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n   \n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n   \n    \n    \n\n    datasets = [ weekwise['m1'],weekwise['m2'],weekwise['m3']]\n    merged_dict = {}\n  \n    for i, df in enumerate(datasets, start=1):\n        dataset_prefix = f'm{i}'\n        voice_usage_columns = [col for col in df.columns if 'total_voice_usage' in col]\n        data_usage_columns = [col for col in df.columns if 'data_usage' in col]\n\n        for j in range(1, len(voice_usage_columns)):\n            if j != 2:  \n                prev_col = voice_usage_columns[j-1]\n                col = voice_usage_columns[j]\n                print('prev_col', prev_col)\n                print('col', col)\n                pct_drop_col = f'{prev_col}_{col}_pct_drop_{dataset_prefix}'\n                df[pct_drop_col] = ((df[prev_col] - df[col]) / df[prev_col]) * 100\n                df[pct_drop_col] = df[pct_drop_col].replace([np.inf, -np.inf], np.nan).fillna(0).astype(float)\n\n                prev_data_col = data_usage_columns[j-1]\n                data_col = data_usage_columns[j]\n                data_pct_drop_col = f'{prev_data_col}_{data_col}_pct_drop_{dataset_prefix}'\n                df[data_pct_drop_col] = ((df[prev_data_col] - df[data_col]) / df[prev_data_col]) * 100\n                df[data_pct_drop_col] = df[data_pct_drop_col].replace([np.inf, -np.inf], np.nan).fillna(0).astype(float)\n\n        w3_voice_pct_drop_col = f'w3_total_voice_usage_w4_total_voice_usage_pct_drop_{dataset_prefix}'\n        w3_data_pct_drop_col = f'w3_data_usage_w4_data_usage_pct_drop_{dataset_prefix}'\n\n        df[w3_voice_pct_drop_col] = ((df['w3_total_voice_usage'] - df['w4_total_voice_usage']) / df['w3_total_voice_usage']) * 100\n        df[w3_data_pct_drop_col] = ((df['w3_data_usage'] - df['w4_data_usage']) / df['w3_data_usage']) * 100\n\n        df = df.map_partitions(lambda df: df.replace([np.inf, -np.inf], np.nan).fillna(0).astype(float))\n\n       \n        merged_dict[dataset_prefix] = df\n\n   \n    merged_df = None\n    for key, value in merged_dict.items():\n        if merged_df is None:\n            merged_df = value\n        else:\n            merged_df = merged_df.merge(value, on=msisdn_name, suffixes=('', f'_{dataset_prefix}'))\n            \n    merged_df  =  usage_base_df[[msisdn_name]].merge(merged_df, on=msisdn_name, how='left')\n    merged_df = merged_df.fillna(0)\n    columns_to_select = ['msisdn'] + [col for col in merged_df.columns if 'pct_drop' in col]\n\n    merged_df = merged_df.loc[:, columns_to_select]\n    \n    def assign_label(column):\n        labels = np.empty(len(column), dtype='object')\n        labels[column == 0] = 'd)0'  # if 0\n        labels[(column > 0) & (column <= 50)] = 'e)0_50'  # 0 (exclusive) to 50 (inclusive)\n        labels[(column > 50) & (column <= 100)] = 'f)50_100'  # 50 (exclusive) to 100 (inclusive)\n        labels[column > 100] = 'g)>100'\n        labels[(column < 0) & (column >= -50)] = 'c)<0_-50'  # less than 0\n        labels[(column < -50) & (column >= -100)] = 'b)-50_-100'  # -50 (inclusive) to -100 (exclusive)\n        labels[column < -100] = 'a)>-100'\n        return labels\n\n    \n\n    \n    columns = ['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1',\n    'w1_data_usage_w2_data_usage_pct_drop_m1',\n    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1',\n    'w3_data_usage_w4_data_usage_pct_drop_m1',\n    'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2',\n    'w1_data_usage_w2_data_usage_pct_drop_m2',\n    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2',\n    'w3_data_usage_w4_data_usage_pct_drop_m2',\n    'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3',\n    'w1_data_usage_w2_data_usage_pct_drop_m3',\n    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3',\n    'w3_data_usage_w4_data_usage_pct_drop_m3']\n\n    for column in columns:\n        new_column = column + '_banded'\n        merged_df[new_column] = merged_df[column].map_partitions(assign_label, meta='object')\n\n    \n    weekwise_pct_drop = os.path.join(ml_location,\"weekwise_pct_drop\")\n    Path( weekwise_pct_drop).mkdir(parents=True, exist_ok=True)\n    print(\" weekwise_pct_drop  file output is going on \")\n\n    merged_df.to_parquet(weekwise_pct_drop)\n    \n    \n\n\ndef inactive_days_band(dag_run_id):\n    \n    \n    date_name  =   DAILY_TRANSACTION_PURCHASE_DATE_NAME  \n\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    daily_summerized_month =  ['m1','m2','m3']\n    daily_summerized_dict = {}\n    for month in daily_summerized_month:\n        daily_summerized_dict[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"daily_summerized\").get(month)))\n        \n    m1 = daily_summerized_dict['m1']\n    m2 = daily_summerized_dict['m2']\n    m3 = daily_summerized_dict['m3']\n    \n\n    df = dd.concat([m1, m2, m3], axis=0)\n    \n    df[date_name] = dd.to_datetime(df[date_name])\n    current_date = df[date_name].max().compute()\n    last_active_date = df.groupby(msisdn_name)[date_name].max()\n    consecutive_inactive_days = (current_date - last_active_date).dt.days\n    inactive_days_df = consecutive_inactive_days.reset_index()\n    inactive_days_df.columns = ['msisdn', 'consecutive_inactive_days']\n    inactive_days_df = inactive_days_df[inactive_days_df['consecutive_inactive_days'] > 0]\n    bins = [-1, 5, 10, 15, 30, 90, float('inf')]\n    labels = ['a)0-5', 'b)5-10', 'c)10-15', 'd)15-30', 'e)30-90', 'f)90+']\n    inactive_days_df['inactive_days_band'] = inactive_days_df['consecutive_inactive_days'].map_partitions(lambda s: pd.cut(s, bins=bins, labels=labels).astype(str))\n    inactive_days_df['inactive_days_band'] = inactive_days_df['inactive_days_band'].astype('category')\n    \n    inactive_days_banding_path = os.path.join(ml_location, \"inactive_days_banding_path\")\n    Path(inactive_days_banding_path).mkdir(parents=True, exist_ok=True)\n    print(\"inactive_days_banding_path bands  file output is going on \")\n\n    inactive_days_df.to_parquet(inactive_days_banding_path)\n    \n    \ndef three_month_engagement_index(dag_run_id):\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    daily_summerized_month =  ['m1','m2','m3']\n    daily_summerized_dict = {}\n    for month in daily_summerized_month:\n        daily_summerized_dict[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"daily_summerized\").get(month)))\n        \n    m1 = daily_summerized_dict['m1']\n    m2 = daily_summerized_dict['m2']\n    m3 = daily_summerized_dict['m3']\n    \n\n    df = dd.concat([m1, m2, m3], axis=0)\n    daily_summ_col = DAILY_TRANSACTION_PURCHASE_DATE_NAME\n    m1[daily_summ_col] = dd.to_datetime(m1[daily_summ_col])\n    m2[daily_summ_col] = dd.to_datetime(m2[daily_summ_col])\n    m3[daily_summ_col] = dd.to_datetime(m3[daily_summ_col])    \n    \n    \n    date_sum = m1[daily_summ_col].dt.day.max() + m2[daily_summ_col].dt.day.max() + m3[daily_summ_col].dt.day.max()\n\n    df_final = df[df['total_revenue'] > 0]\n    result = df_final.groupby(msisdn_name).fct_dt.count().reset_index()\n    result.columns = ['msisdn', 'fct_days']\n    result['eng_index'] = result['fct_days'] / date_sum\n    result['eng_index'] = result['eng_index'] * 100\n\n    df1 = df[df['total_revenue'] == 0]    \n    df1 = df1.merge(df_final[[msisdn_name]], how='left', indicator=True)\n    df1 = df1[df1['_merge'] == 'left_only'].drop('_merge', axis=1)\n\n\n    df1 = dd.from_pandas(pd.DataFrame(df1[msisdn_name].unique()), npartitions=1).reset_index(drop=True).rename(columns={0: 'msisdn'})\n    df1['eng_index'] = 0.0\n\n    result = result.drop('fct_days', axis=1)\n    f_result = dd.concat([result, df1], axis=0)\n    bins = [-1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 101]\n    labels = ['a)0_10', 'b)10_20', 'c)20_30', 'd)30_40', 'e)40_50',\n          'f)50_60', 'g)60_70', 'h)70_80', 'i)80_90', 'j)90_100']\n\n    f_result['eng_index_band'] = dd.from_dask_array(f_result['eng_index'].to_dask_array(), columns=['eng_index_band']).map_partitions(\n        lambda df: pd.cut(df['eng_index_band'], bins=bins, labels=labels))\n    \n    engagment_index_banding_path = os.path.join(ml_location, \"engagment_index_banding_path\")\n    Path(engagment_index_banding_path).mkdir(parents=True, exist_ok=True)\n    print(\"engagment_index bands  file output is going on \")\n    f_result.to_parquet(engagment_index_banding_path)\n    \n    \ndef usage_banding_process(dag_run_id):\n\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    print(\"finding calculate_pct_drop ongoing \")\n    usage = {}\n    for month in usage_no_months:\n        usage[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\n                                    dtype=CUSTOMER_DTYPES,usecols =CUSTOMER_USAGE_COLUMNS )\n\n    usage_3m = dd.concat(list(usage.values()))\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n\n\n    m1 = usage['m1']\n    m2 = usage['m2']\n    m3 = usage['m3']\n\n    \n    etl_3_df = usage_base_df.merge(m1, on=msisdn_name, how='left').merge(m2, on=msisdn_name, how='left').merge(m3, on=msisdn_name, how='left')\n    \n    etl_3_df = etl_3_df.fillna(0)\n    new_column_names = {}\n    for col_name in etl_3_df.columns:\n        if col_name == 'msisdn':\n            new_col_name = col_name\n        elif col_name.endswith('_x'):\n            new_col_name = 'm1_' + col_name[:-2]\n        elif col_name.endswith('_y'):\n            new_col_name = 'm2_' + col_name[:-2]\n        else:\n            new_col_name = 'm3_' + col_name\n        new_column_names[col_name] = new_col_name\n    etl_3_df = etl_3_df.rename(columns=new_column_names)\n    etl_3_df = etl_3_df[['msisdn', 'm1_total_voice_usage', 'm1_total_data_usage',\n                         'm2_total_voice_usage', 'm2_total_data_usage',\n                         'm3_total_voice_usage', 'm3_total_data_usage']]\n    df = etl_3_df.copy()\n    df['m1_m2_m3_average_voice'] = df[['m1_total_voice_usage', 'm2_total_voice_usage', 'm3_total_voice_usage']].mean(axis=1)\n    df['m1_m2_m3_average_data'] = df[['m1_total_data_usage', 'm2_total_data_usage', 'm3_total_data_usage']].mean(axis=1)\n    return df \n    \n    \n    \ndef usage_category(df, column_name, band_function):\n    df[column_name + '_band'] = df[column_name].map_partitions(lambda x: x.apply(band_function), meta=('object'))\n    return df[['msisdn', column_name + '_band',column_name]]\n\n\n\ndef voice_band(val):\n    \n    if val == 0.0:\n        return 'Zero'\n    elif (val > 0) and (val <= 119):\n        return 'low'\n    elif (val > 119) and (val <= 238):\n        return 'medium'\n    elif (val > 238) and (val <= 357):\n        return 'high'\n    elif val > 357:\n        return 'very_high'\n    else:\n        return None\n        \n        \ndef data_band(val):\n    \n    if val == 0.0:\n        return 'Zero'\n    elif (val > 0) and (val <= 1542.):\n        return 'low'\n    elif (val > 1542.) and (val <= 3084.):\n        return 'medium'\n    elif (val > 3084.) and (val <= 4626.):\n        return 'high'\n    elif val > 4626.:\n        return 'very_high'\n    else:\n        return None\n        \n        \n\ndef data_voice_usage_banding(dag_run_id):\n    usage_process_df = usage_banding_process(dag_run_id)\n    df_voice_band = usage_category(usage_process_df, 'm1_m2_m3_average_voice', voice_band)\n    df_data_band = usage_category(usage_process_df, 'm1_m2_m3_average_data', data_band)\n    usage_band_df = dd.merge(df_voice_band, df_data_band, on='msisdn')\n\n    voice_data_usage_banding_path = os.path.join(ml_location, \"voice_data_usage_banding\")\n    Path(voice_data_usage_banding_path).mkdir(parents=True, exist_ok=True)\n    print(\"usage bands  file output is ongoing \")\n    usage_band_df.to_parquet(voice_data_usage_banding_path)\n    \n\ndef calculate_pct_drop(dag_run_id):\n\n    \n\n    def assign_label(column):\n        \n        labels = np.empty(len(column), dtype='object')\n        labels[column == 0] = 'd)0'  \n        labels[(column > 0) & (column <= 50)] = 'e)0_50'  \n        labels[(column > 50) & (column <= 100)] = 'f)50_100'  \n        labels[column > 100] = 'g)>100'\n        labels[(column < 0) & (column >= -50)] = 'c)<0_-50'  \n        labels[(column < -50) & (column >= -100)] = 'b)-50_-100' \n        labels[column < -100] = 'a)>-100'\n        return labels\n\n    msisdn_name = MSISDN_COL_NAME\n    file_name_dict = get_file_names()\n    print(\"finding calculate_pct_drop ongoing \")\n    usage = {}\n    for month in usage_no_months:\n        usage[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\n                                    dtype= CUSTOMER_DTYPES,usecols = CUSTOMER_DROP_COLUMNS )\n\n    usage_3m = dd.concat(list(usage.values()))\n    usage_3m_msisdn = usage_3m[msisdn_name].unique().compute()\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n\n    usage_3m_dict = {msisdn_name: usage_3m_msisdn}\n\n    print('len(usage_3m_msisdn) is',len(usage_3m_msisdn) )\n        \n    usage_base_df = dd.from_pandas(pd.DataFrame(usage_3m_dict), npartitions=1)\n\n\n    m1 = usage['m1']\n    m2 = usage['m2']\n    m3 = usage['m3']\n   \n    \n    columns = ['m2_total_revenue_m1_total_revenue_pct_drop', 'm2_voice_rev_m1_voice_rev_pct_drop', 'm2_data_revenue_m1_data_revenue_pct_drop', 'm3_total_revenue_m2_total_revenue_pct_drop', 'm3_voice_rev_m2_voice_rev_pct_drop', 'm3_data_revenue_m2_data_revenue_pct_drop']\n\n    etl_3_df = usage_base_df.merge(m1, on=msisdn_name, how='left').merge(m2, on=msisdn_name, how='left').merge(m3, on=msisdn_name, how='left')\n    print(etl_3_df)\n    etl_3_df = etl_3_df.fillna(0)\n    new_column_names = {}\n    for col_name in etl_3_df.columns:\n        if col_name == 'msisdn':\n            new_col_name = col_name\n        elif col_name.endswith('_x'):\n            new_col_name = 'm1_' + col_name[:-2]\n        elif col_name.endswith('_y'):\n            new_col_name = 'm2_' + col_name[:-2]\n        else:\n            new_col_name = 'm3_' + col_name\n        new_column_names[col_name] = new_col_name\n\n\n    etl_3_df = etl_3_df.rename(columns=new_column_names)\n    etl_3_df = etl_3_df[['msisdn', 'm1_total_revenue', 'm1_voice_rev', 'm1_data_revenue',\n                        'm2_total_revenue', 'm2_voice_rev', 'm2_data_revenue',\n                        'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue']]\n    df = etl_3_df.copy()\n    revenue_types = ['total_revenue', 'voice_rev', 'data_revenue']\n    for i in range(1, 3):\n        for rt in revenue_types:\n            col1 = f'm{i+1}_{rt}'\n            col2 = f'm{i}_{rt}'\n            pct_drop_col = f'{col1}_{col2}_pct_drop'\n            df[pct_drop_col] = (df[col1] - df[col2]) / df[col1] * 100\n            df[pct_drop_col] = df[pct_drop_col].replace([np.inf, -np.inf], np.nan)\n            df[pct_drop_col] = df[pct_drop_col].fillna(0)\n        df[pct_drop_col] = df[pct_drop_col].astype('float64')\n\n        \n\n    for column in columns:\n        new_column = column + '_banded'\n        df[new_column] = df[column].map_partitions(assign_label, meta='object')\n    \n    usage_rev_drop_path = os.path.join(ml_location, \"usage_rev_drop\")\n    Path(usage_rev_drop_path).mkdir(parents=True, exist_ok=True)\n    print(\"revenue drop   file output is ongoing \")\n\n    \n    \n    expected_schema = pa.schema([\n    ('msisdn', pa.int64()),\n    ('m1_total_revenue', pa.float32()),\n    ('m1_voice_rev', pa.float64()),\n    ('m1_data_revenue', pa.float64()),\n    ('m2_total_revenue', pa.float32()),\n    ('m2_voice_rev', pa.float64()),\n    ('m2_data_revenue', pa.float64()),\n    ('m3_total_revenue', pa.float32()),\n    ('m3_voice_rev', pa.float64()),\n    ('m3_data_revenue', pa.float64()),\n    ('m2_total_revenue_m1_total_revenue_pct_drop', pa.float64()),\n    ('m2_voice_rev_m1_voice_rev_pct_drop', pa.float64()),\n    ('m2_data_revenue_m1_data_revenue_pct_drop', pa.float64()),\n    ('m3_total_revenue_m2_total_revenue_pct_drop', pa.float64()),\n    ('m3_voice_rev_m2_voice_rev_pct_drop', pa.float64()),\n    ('m3_data_revenue_m2_data_revenue_pct_drop', pa.float64()),\n    ('m2_total_revenue_m1_total_revenue_pct_drop_banded', pa.string()),\n    ('m2_voice_rev_m1_voice_rev_pct_drop_banded', pa.string()),\n    ('m2_data_revenue_m1_data_revenue_pct_drop_banded', pa.string()),\n    ('m3_total_revenue_m2_total_revenue_pct_drop_banded', pa.string()),\n    ('m3_voice_rev_m2_voice_rev_pct_drop_banded', pa.string()),\n    ('m3_data_revenue_m2_data_revenue_pct_drop_banded', pa.string()),\n    ('__null_dask_index__', pa.int64())\n    ])\n    \n    \n\n    df.to_parquet(usage_rev_drop_path, schema=expected_schema)\n    print(\"-------need to perquet\")\n    \n    \n\n\ndef get_banding_confitions():\n        return {\n            \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\n            \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\",\n                               6: \"high_high\"}\n        }\n        \n        \n\ndef segementation(dag_run_id):\n    try:\n        recharge_path = os.path.join(ml_location,  \"recharge_band\")\n        recharge = vaex.open(recharge_path, )\n        print(\"loaded recharge\")\n        recharge=recharge[['msisdn','recharge_total_cntm1','recharge_total_cntm2','recharge_total_cntm3']]\n\n        path_pur = os.path.join(ml_location,  \"purchase_all_months\")\n        pur_all_months = dd.read_parquet(path_pur)\n        print(\"purchase 3 months loaded\")\n\n        purchase_path = os.path.join(ml_location, \"purchase_band\")\n        purchase = vaex.open(purchase_path)\n        print(\"loaded purchase\")\n        purchase=purchase[['msisdn','purchase_total_cntm1','purchase_total_cntm2','purchase_total_cntm3']]\n\n        path_recharge = os.path.join(ml_location, \"recharge_all_months\")\n        recharge_all_months = dd.read_parquet(path_recharge)\n        print(\"recharge 3 months loaded\")\n\n        trend_path = os.path.join(ml_location, \"trend_filtered\")\n        trend = vaex.open(trend_path)\n        print(\"loaded trend\")\n        #trend = trend[['msisdn', 'tot_revm1', 'tot_revm2', 'tot_revm3']]\n        trend = trend[['msisdn', 'rev_segment_m1', 'rev_segment_m2', 'rev_segment_m3', 'trend']]\n        replace_map = get_banding_confitions().get(\"common_reverse\")\n\n        trend['rev_segment_m1'] = trend.rev_segment_m1.map(replace_map)\n        trend['rev_segment_m2'] = trend.rev_segment_m2.map(replace_map)\n        trend['rev_segment_m3'] = trend.rev_segment_m3.map(replace_map)\n\n        usage_path = os.path.join(ml_location,  \"usage_bands\")\n        usage = vaex.open(usage_path)\n        usage=usage[['msisdn','m1_total_voice_usage','m1_total_data_usage','m2_total_voice_usage', 'm2_total_data_usage','m3_total_voice_usage', 'm3_total_data_usage']]\n        print(\"loaded usage\")\n\n        rfm_path = os.path.join(ml_location, \"rfm\")\n        rfm = vaex.open(rfm_path)\n        rfm = rfm[['msisdn', 'Segment']]\n        print(\"loaded rfm\")\n        # ------------nnew logic----------------#\n        # trend_rfm = trend.join(rfm, on='msisdn', how=\"inner\")\n        trend_rfm = trend.copy()\n        trend_rfm=trend_rfm[['msisdn','trend']]\n        trend_rfm_recharge = trend_rfm.join(recharge, on='msisdn', how=\"left\")\n        trend_rfm_recharge['recharge_total_cntm1'] = trend_rfm_recharge['recharge_total_cntm1'].fillna(0)\n        trend_rfm_recharge['recharge_total_cntm2'] = trend_rfm_recharge['recharge_total_cntm2'].fillna(0)\n        trend_rfm_recharge['recharge_total_cntm3'] = trend_rfm_recharge['recharge_total_cntm3'].fillna(0)\n        trend_rfm_recharge_usage = trend_rfm_recharge.join(usage, on='msisdn', how=\"left\")\n\n\n         #to fill null values if there is in data and voice band \n        trend_rfm_recharge_usage['m1_total_voice_usage'] = trend_rfm_recharge_usage['m1_total_voice_usage'].fillna(0)\n        trend_rfm_recharge_usage['m1_total_data_usage'] = trend_rfm_recharge_usage['m1_total_data_usage'].fillna(0)\n        trend_rfm_recharge_usage['m2_total_voice_usage'] = trend_rfm_recharge_usage['m1_total_voice_usage'].fillna(0)\n        trend_rfm_recharge_usage['m2_total_data_usage'] = trend_rfm_recharge_usage['m2_total_data_usage'].fillna(0)\n        trend_rfm_recharge_usage['m3_total_voice_usage'] = trend_rfm_recharge_usage['m1_total_voice_usage'].fillna(0)\n        trend_rfm_recharge_usage['m3_total_data_usage'] = trend_rfm_recharge_usage['m3_total_data_usage'].fillna(0)\n\n        #to fill null values if there is in data and voice band \n\n\n\n        trend_rfm_recharge_usage_purchase = trend_rfm_recharge_usage.join(purchase, on='msisdn', how=\"left\")\n        trend_rfm_recharge_usage_purchase['purchase_total_cntm1'] = trend_rfm_recharge_usage_purchase[\n            'purchase_total_cntm1'].fillna(0)\n        trend_rfm_recharge_usage_purchase['purchase_total_cntm2'] = trend_rfm_recharge_usage_purchase[\n            'purchase_total_cntm2'].fillna(0)\n        trend_rfm_recharge_usage_purchase['purchase_total_cntm3'] = trend_rfm_recharge_usage_purchase[\n            'purchase_total_cntm3'].fillna(0)\n\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase.join(rfm, on='msisdn', how=\"inner\")\n\n        #joining usage_drop_percentage\n        calculate_pct_drop(dag_run_id)\n        usage_rev_drop_path = os.path.join(ml_location, \"usage_rev_drop\")\n        usage_drop_percentage = vaex.open(usage_rev_drop_path)\n        usage_drop_percentage = usage_drop_percentage[['msisdn','m3_total_revenue_m2_total_revenue_pct_drop', \n                                                       'm3_data_revenue_m2_data_revenue_pct_drop', \n                                                       'm2_voice_rev_m1_voice_rev_pct_drop', \n                                                       'm2_total_revenue_m1_total_revenue_pct_drop', \n                                                       'm2_data_revenue_m1_data_revenue_pct_drop', \n                                                       'm3_voice_rev_m2_voice_rev_pct_drop']]\n\n\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.join(usage_drop_percentage, on='msisdn', how=\"left\")\n       \n        #joining usage_drop_percentage\n\n        #joining usage bands\n        data_voice_usage_banding(dag_run_id)\n        voice_data_usage_banding_path = os.path.join(ml_location, \"voice_data_usage_banding\")\n        voice_data_usage_banding = vaex.open(voice_data_usage_banding_path)\n        voice_data_usage_banding=voice_data_usage_banding[['msisdn','m1_m2_m3_average_voice','m1_m2_m3_average_data']]\n\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.join(voice_data_usage_banding, on='msisdn', how=\"left\")\n        trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_voice']=trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_voice'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_data']=trend_rfm_recharge_usage_purchase_rfm['m1_m2_m3_average_data'].fillna(0)\n        #joining usage bands\n\n\n        #joining engagement index \n        three_month_engagement_index(dag_run_id)\n        engagment_index_banding_path = os.path.join(ml_location, \"engagment_index_banding_path\")\n        engagment_index_banding = vaex.open(engagment_index_banding_path)\n\n        engagment_index_banding=engagment_index_banding[['msisdn','eng_index']]\n\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.join(engagment_index_banding, on='msisdn', how=\"left\")\n        trend_rfm_recharge_usage_purchase_rfm['eng_index']=trend_rfm_recharge_usage_purchase_rfm['eng_index'].fillna(0)\n\n\n        #joining engagement index \n\n\n        #joining purchase rfm\n\n        rfm__purchase_path = os.path.join(ml_location, \"rfm_purchase\")\n        rfm_purchase = vaex.open(rfm__purchase_path)\n        print('type(rfm_purchase)',type(rfm_purchase))\n        rfm_purchase.rename('RFM_Segment', 'rfm_purchase_segment', unique=False)\n        rfm_purchase = rfm_purchase[['msisdn', 'rfm_purchase_segment']]\n\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(rfm_purchase, on='msisdn', how=\"left\")\n        trend_rfm_recharge_usage_purchase_rfm['rfm_purchase_segment']=trend_rfm_recharge_usage_purchase_rfm['rfm_purchase_segment'].fillna(000)\n\n        \n        #joining purchase rfm\n\n\n         #joining inactivity \n\n        inactive_days_band(dag_run_id)\n        inactive_days_banding_path = os.path.join(ml_location, \"inactive_days_banding_path\")\n        inactivity_days_df = vaex.open(inactive_days_banding_path)\n        inactivity_days_df=inactivity_days_df[['msisdn', 'consecutive_inactive_days']]\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(inactivity_days_df, on='msisdn', how=\"left\")\n        trend_rfm_recharge_usage_purchase_rfm['consecutive_inactive_days']=trend_rfm_recharge_usage_purchase_rfm['consecutive_inactive_days'].fillna(0)\n\n\n        #joining inactivity \n\n        #joining weekly delta change\n        delta_calculation_weekwise(dag_run_id)\n        weekwise_pct_drop = os.path.join(ml_location,\"weekwise_pct_drop\")\n        weekwise_pct_drop_df = vaex.open(weekwise_pct_drop)\n        weekwise_pct_drop_df=weekwise_pct_drop_df[['msisdn',\n            'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3', \n            'w1_data_usage_w2_data_usage_pct_drop_m3',\n             'w3_data_usage_w4_data_usage_pct_drop_m2',\n              'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3',\n               'w1_data_usage_w2_data_usage_pct_drop_m1',\n                'w1_data_usage_w2_data_usage_pct_drop_m2',\n                 'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1',\n                  'w3_data_usage_w4_data_usage_pct_drop_m1',\n                   'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1',\n                    'w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2',\n                      'w3_data_usage_w4_data_usage_pct_drop_m3',\n                        'w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2']]\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(weekwise_pct_drop_df, on='msisdn', how=\"left\")\n        trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m3'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m3'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m2'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m3'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m1'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w1_data_usage_w2_data_usage_pct_drop_m2'].fillna(0)        \n        trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m1'].fillna(0)        \n        trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m1'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1']= trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m1'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w3_total_voice_usage_w4_total_voice_usage_pct_drop_m2'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m3']= trend_rfm_recharge_usage_purchase_rfm['w3_data_usage_w4_data_usage_pct_drop_m3'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2']= trend_rfm_recharge_usage_purchase_rfm['w1_total_voice_usage_w2_total_voice_usage_pct_drop_m2'].fillna(0)\n\n        #joining weekly delta change\n\n\n        #joining weekly-daily delta change\n\n\n        calculate_pct_drop_daily_weekly(dag_run_id)\n\n        daily_weekly_pct_drop = os.path.join(ml_location,\"daily_weekly_pct_drop\")\n        daily_weekly_pct_drop_df = vaex.open(daily_weekly_pct_drop)\n        daily_weekly_pct_drop_df=daily_weekly_pct_drop_df[['msisdn',\n           'm2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop',\n             'm2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop',\n               'm3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop',\n                 'm3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop',\n                   'm2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop',\n                     'm3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop',\n                       'm2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop',\n                         'm3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop']]\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(daily_weekly_pct_drop_df, on='msisdn', how=\"left\")\n        trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_voice_usage_m1_weekly_avg_voice_usage_pt_drop'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_weekly_avg_data_usage_m1_weekly_avg_data_usage_pt_drop'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_data_usage_m2_weekly_avg_data_usage_pt_drop'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_data_usage_m2_daily_avg_data_usage_pt_drop'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_voice_usage_m1_daily_avg_voice_usage_pt_drop'].fillna(0)       \n        trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_daily_avg_voice_usage_m2_daily_avg_voice_usage_pt_drop'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m2_daily_avg_data_usage_m1_daily_avg_data_usage_pt_drop'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop']= trend_rfm_recharge_usage_purchase_rfm['m3_weekly_avg_voice_usage_m2_weekly_avg_voice_usage_pt_drop'].fillna(0)\n\n\n        #joining weekly-daily delta change\n\n\n        #joining purchase number of days \n        no_of_purchase_days_count(dag_run_id)\n        no_of_purchase_days_count_path = os.path.join(ml_location, \"no_of_purchase_days_count\")\n        print('opening no_of_purchase_days_count file'  )\n        no_of_purchase_days_count_df = vaex.open(no_of_purchase_days_count_path)\n        no_of_purchase_days_count_df=no_of_purchase_days_count_df[['msisdn', 'm1_no_of_days',\n       'm2_no_of_days', 'm3_no_of_days']]\n        \n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(no_of_purchase_days_count_df, on='msisdn', how=\"left\")\n        trend_rfm_recharge_usage_purchase_rfm['m1_no_of_days']= trend_rfm_recharge_usage_purchase_rfm['m1_no_of_days'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['m2_no_of_days']= trend_rfm_recharge_usage_purchase_rfm['m2_no_of_days'].fillna(0)\n        trend_rfm_recharge_usage_purchase_rfm['m3_no_of_days']= trend_rfm_recharge_usage_purchase_rfm['m3_no_of_days'].fillna(0)\n        #joining purchase number of days \n\n        #adding usage service\n        service_path = os.path.join(ml_location,\"favorite_Service.csv\")\n        print('opening favorite_Service file'  )\n        usage_service = vaex.open(service_path)\n        usage_service=usage_service[['msisdn','service_band']]\n        trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.join(usage_service, on='msisdn', how=\"left\")\n        trend_rfm_recharge_usage_purchase_rfm['service_band']=trend_rfm_recharge_usage_purchase_rfm['service_band'].fillna('no_usage')\n       \n\n\n\n         #adding usage service\n        \n        #exporting and adding the kpi segments to db\n        print('APA_kpi_segments_analysis file going to export'  )\n        print('trend_rfm_recharge_usage_purchase_rfm.columns',trend_rfm_recharge_usage_purchase_rfm.columns)\n        print('trend_rfm_recharge_usage_purchase_rfm.dtypes',trend_rfm_recharge_usage_purchase_rfm.dtypes)\n        trend_rfm_recharge_usage_purchase_rfm.export_csv(os.path.join(ml_location, \"APA_kpi_segments_analysis.csv\"))\n        print('APA_kpi_segments_analysis file exported '  )\n        # for df_chunk in pd.read_csv(os.path.join(ml_location, \"APA_kpi_segments_analysis.csv\"),\n        #                             chunksize=5000):\n        #     # insert each chunk into database table\n        #     df_chunk['dag_run_id']=str(dag_run_id)\n        #     df_chunk.to_sql('APA_kpi_segments_analysis_num', engine, if_exists='append', index=False)\n\n        \n\n\n       \n        # trend_rfm_recharge_usage_purchase_rfm = trend_rfm_recharge_usage_purchase_rfm.extract()\n\n        trend_rfm_recharge_usage_purchase_rfm=trend_rfm_recharge_usage_purchase_rfm.to_pandas_df()\n        print('type(trend_rfm_recharge_usage_purchase_rfm)',type(trend_rfm_recharge_usage_purchase_rfm))\n        segment_data(trend_rfm_recharge_usage_purchase_rfm, dag_run_id)\n\n        ic(\"mergeing rechare and trend  and rfm \", trend_rfm_recharge_usage_purchase_rfm['msisdn'].nunique())\n\n      \n\n        df = trend_rfm_recharge_usage_purchase_rfm.groupby([\"trend\", 'Segment']).agg({\"msisdn\": \"count\"})\n        df.to_csv(os.path.join(ml_location, \"trend_segment.csv\"),index=False)\n        # ------------------------inner join logic  end-------------------------------#\n        # df = trend_rfm_recharge_usage_purchase.groupby([\"trend\", 'Segment']).agg({\"msisdn\": \"count\"})\n        # df.export_csv(os.path.join(cfg.Config.ml_location, dag_run_id, \"trend_segment.csv\"))\n        print(\"done with segmentation\")\n        return df\n\n        # segment_data_with_rfm(trend_rfm_recharge_usage_purchase, recharge_all_months.copy(), dag_run_id)\n    except Exception as e:\n        print(e)\n        \n        \n    \n\n\n\ndef transform(dataframe):\n    d=segementation(\"manual__2023-07-10T11:06:51\")\n    return d"
      }
    }, {
      "id": "403b70cb-2a48-4121-1946-9e40d96be773",
      "operation": {
        "id": "65240399-2987-41bd-ba7e-2944d60a3404",
        "name": "Create Custom Transformer"
      },
      "parameters": {
        "inner workflow": {
          "workflow": {
            "nodes": [{
              "id": "2ae6aa34-be9e-04d4-4627-a47f6dab9846",
              "operation": {
                "id": "d5f4e717-429f-4a28-a0d3-eebba036363a",
                "name": "Handle Missing Values"
              },
              "parameters": {
                "columns": {
                  "selections": [{
                    "type": "columnList",
                    "values": ["service_band"]
                  }],
                  "excluding": false
                },
                "strategy": {
                  "replace with custom value": {
                    "value": "no_usage"
                  }
                },
                "user-defined missing values": [{

                }]
              }
            }, {
              "id": "477a197a-c1d4-5098-9b92-6c79b918a3cd",
              "operation": {
                "id": "6534f3f4-fa3a-49d9-b911-c213d3da8b5d",
                "name": "Filter Columns"
              },
              "parameters": {
                "selected columns": {
                  "selections": [{
                    "type": "columnList",
                    "values": ["m1_total_data_usage", "m1_total_voice_usage", "m1_total_sms_usage", "m1_msisdn", "m3_total_data_usage", "m3_total_sms_usage", "m3_total_voice_usage", "m2_total_voice_usage", "m2_total_data_usage", "m2_total_sms_usage"]
                  }],
                  "excluding": false
                }
              }
            }, {
              "id": "8b1c9bf7-1b49-2b1e-b75e-2dab2a00bc99",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "right prefix": "m3_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "m1_msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "f682aa1a-0071-d066-e355-df338b6d3a9e",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "left prefix": "m1_",
                "right prefix": "m2_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "aa2c61c6-6ef1-4016-a011-9990009ddee6",
              "operation": {
                "id": "f94b04d7-ec34-42f7-8100-93fe235c89f8",
                "name": "Source"
              },
              "parameters": {

              }
            }, {
              "id": "919604ae-b5c0-8f8c-ed96-d48a46aa6e3f",
              "operation": {
                "id": "6534f3f4-fa3a-49d9-b911-c213d3da8b5d",
                "name": "Filter Columns"
              },
              "parameters": {
                "selected columns": {
                  "selections": [{
                    "type": "columnList",
                    "values": ["m1_total_data_usage", "m1_total_voice_usage", "m1_msisdn", "m3_data_revenue", "m3_total_data_usage", "m3_total_voice_usage", "m1_voice_rev", "m3_total_revenue", "m2_total_voice_usage", "m3_voice_rev", "m2_voice_rev", "m2_total_revenue", "m2_total_data_usage", "m1_total_revenue", "m1_data_revenue", "m2_data_revenue"]
                  }],
                  "excluding": false
                }
              }
            }, {
              "id": "a44fad41-58cd-d08d-a363-16da2bfeab11",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "a8dbba2f-156d-8cab-8734-e714679a3f9e"
              }
            }, {
              "id": "8644f710-030b-9579-ddba-dfe829cb99ae",
              "operation": {
                "id": "e652238f-7415-4da6-95c6-ee33808561b2",
                "name": "Sink"
              },
              "parameters": {

              }
            }, {
              "id": "336f5c3f-7b9b-f30e-f66c-e2635a76aaaf",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nimport pandas as pd\n\ndef transform(dataframe):\n\n    spark = SparkSession.builder.getOrCreate()\n\n    dataframe = dataframe.withColumnRenamed('m1_msisdn', 'msisdn')\n    dataframe = dataframe.fillna(0)\n    \n    df = dataframe.select('*')\n    \n    df = df.withColumn('m1_m2_m3_average_voice', (col('m1_total_voice_usage') + col('m2_total_voice_usage') + col('m3_total_voice_usage')) / 3)\n    df = df.withColumn('m1_m2_m3_average_data', (col('m1_total_data_usage') + col('m2_total_data_usage') + col('m3_total_data_usage')) / 3)\n\n    revenue_types = ['total_revenue', 'voice_rev', 'data_revenue']\n    \n    for i in range(1, 3):\n        for rt in revenue_types:\n            col1 = F.col(f'm{i+1}_{rt}')\n            col2 = F.col(f'm{i}_{rt}')\n            pct_drop_col = f'{col1}_{col2}_pct_drop'\n            \n            df = df.withColumn(pct_drop_col, (col1 - col2) / col1 * 100)\n            df = df.withColumn(pct_drop_col, F.when((F.isnan(df[pct_drop_col]) | (F.col(pct_drop_col) == float('inf')) | (F.col(pct_drop_col) == float('-inf'))), 0).otherwise(df[pct_drop_col]))\n        \n        # Convert the column to 'string' type\n        df = df.withColumn(pct_drop_col, df[pct_drop_col].cast('string'))\n    \n    columns = [\"Column<b'm2_total_revenue'>_Column<b'm1_total_revenue'>_pct_drop\",\n           \"Column<b'm2_voice_rev'>_Column<b'm1_voice_rev'>_pct_drop\",\n           \"Column<b'm2_data_revenue'>_Column<b'm1_data_revenue'>_pct_drop\",\n           \"Column<b'm3_total_revenue'>_Column<b'm2_total_revenue'>_pct_drop\",\n           \"Column<b'm3_voice_rev'>_Column<b'm2_voice_rev'>_pct_drop\",\n           \"Column<b'm3_data_revenue'>_Column<b'm2_data_revenue'>_pct_drop\"]\n\n    new_columns = ['m2_total_revenue_m1_total_revenue_pct_drop', \n                   'm2_voice_rev_m1_voice_rev_pct_drop',\n                   'm2_data_revenue_m1_data_revenue_pct_drop',\n                   'm3_total_revenue_m2_total_revenue_pct_drop', \n                   'm3_voice_rev_m2_voice_rev_pct_drop', \n                   'm3_data_revenue_m2_data_revenue_pct_drop']\n          \n    for i, column in enumerate(columns):\n        df = df.withColumnRenamed(column, new_columns[i])\n        \n        \n    df = df.withColumn(\"m2_data_revenue_m1_data_revenue_pct_drop\", col(\"m2_data_revenue_m1_data_revenue_pct_drop\").cast(\"double\"))\n    df = df.withColumn(\"m3_data_revenue_m2_data_revenue_pct_drop\", col(\"m3_data_revenue_m2_data_revenue_pct_drop\").cast(\"double\"))\n    \n    df = df.fillna(0)\n    \n    \n    \n    # assign_label_udf = udf(assign_label, StringType())\n    \n    # for column in new_columns:\n    #     new_column = column + '_banded'\n    #     df = df.withColumn(new_column, assign_label_udf(df[column]))\n\n    # assign_voice_label_udf = udf(voice_band, StringType())\n    # df = df.withColumn('m1_m2_m3_average_voice_band', assign_voice_label_udf(df['m1_m2_m3_average_voice']))\n \n    # assign_data_label_udf = udf(data_band, StringType())\n    # df = df.withColumn('m1_m2_m3_average_data_band', assign_data_label_udf(df['m1_m2_m3_average_data']))\n    \n\n    selected_col = ('msisdn',\n    'm2_total_revenue_m1_total_revenue_pct_drop_banded',\n    'm2_voice_rev_m1_voice_rev_pct_drop_banded',\n    'm2_data_revenue_m1_data_revenue_pct_drop_banded',\n    'm3_total_revenue_m2_total_revenue_pct_drop_banded',\n    'm3_voice_rev_m2_voice_rev_pct_drop_banded',\n    'm3_data_revenue_m2_data_revenue_pct_drop_banded',\n    'm1_m2_m3_average_voice_band', 'm1_m2_m3_average_data_band')\n          \n          \n    # df1 = df.selectExpr(*selected_col)\n  \n    df = df.toPandas()\n    \n    df = df[['msisdn','m3_total_revenue_m2_total_revenue_pct_drop', \n    'm3_data_revenue_m2_data_revenue_pct_drop', \n    'm2_voice_rev_m1_voice_rev_pct_drop', \n    'm2_total_revenue_m1_total_revenue_pct_drop', \n    'm2_data_revenue_m1_data_revenue_pct_drop', \n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_data','m1_m2_m3_average_voice']]\n    return df\n\n\n# def assign_label(value):\n#     if value == 0:\n#         return 'd)0'\n#     elif 0 < value <= 50:\n#         return 'e)0_50'\n#     elif 50 < value <= 100:\n#         return 'f)50_100'\n#     elif value > 100:\n#         return 'g)>100'\n#     elif -50 <= value < 0:\n#         return 'c)<0_-50'\n#     elif -100 <= value < -50:\n#         return 'b)-50_-100'\n#     else:\n#         return 'a)>-100'\n\n# def data_band(val):\n#     if val == 0.0:\n#         return 'Zero'\n#     elif (val > 0) and (val <= 1542.):\n#         return 'low'\n#     elif (val > 1542.) and (val <= 3084.):\n#         return 'medium'\n#     elif (val > 3084.) and (val <= 4626.):\n#         return 'high'\n#     elif val > 4626.:\n#         return 'very_high'\n#     else:\n#         return None\n\n\n\n# def voice_band(val):\n#     if val == 0.0:\n#         return 'Zero'\n#     elif (val > 0) and (val <= 119):\n#         return 'low'\n#     elif (val > 119) and (val <= 238):\n#         return 'medium'\n#     elif (val > 238) and (val <= 357):\n#         return 'high'\n#     elif val > 357:\n#         return 'very_high'\n#     else:\n#         return None"
              }
            }, {
              "id": "db6dde30-da35-38a8-e24c-f02c6d0ccbd0",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "7ca94422-381c-9c76-202a-6c59d148db68",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType, StringType\nfrom pyspark.sql.functions import array\n\ndef trends(numbers):\n    if len(numbers) != 3:\n        return \"Invalid input\"\n\n    if numbers[0] == numbers[1] == numbers[2]:\n        return \"flat\"\n    elif numbers[0] > numbers[1] > numbers[2]:\n        return \"Downtrend\"\n    elif numbers[0] < numbers[1] < numbers[2]:\n        return \"Uptrend\"\n    else:\n        return \"Zigzag\"\n\n\ndef get_banding_conditions():\n    return {\n        \"common\": {\"Zero\": 1, \"very_low\": 2, \"low\": 3, \"medium\": 4, \"high\": 5, \"high_high\": 6},\n        \"common_reverse\": {1: \"Zero\", 2: \"very_low\", 3: \"low\", 4: \"medium\", 5: \"high\", 6: \"high_high\"}\n    }\n\n\ndef pandas_wrapper(rows):\n    # Perform the desired transformation on the rows\n    transformed_rows = rows.apply(lambda row: trends([row[2], row[1], row[0]]), axis=1)\n    return transformed_rows\n\n\ndef transform(dataframe):\n   \n    \n    dataframe = dataframe.withColumn('tot_sum', col('m1_total_revenue') + col('m2_total_revenue') + col('m3_total_revenue'))\n\n    banding_conditions = get_banding_conditions()\n\n    # Create a broadcast variable for the dictionary\n    banding_conditions_broadcast = spark.sparkContext.broadcast(banding_conditions[\"common\"])\n\n    # Define a UDF to perform the replacement\n    replace_value_udf = udf(lambda x: banding_conditions_broadcast.value.get(x), IntegerType())\n\n    # Replace column values with the dictionary values\n    dataframe = dataframe.withColumn(\"m1_Revenue_Segment\", replace_value_udf(col(\"m1_Revenue_Segment\")))\n    dataframe = dataframe.withColumn(\"m2_Revenue_Segment\", replace_value_udf(col(\"m2_Revenue_Segment\")))\n    dataframe = dataframe.withColumn(\"m3_Revenue_Segment\", replace_value_udf(col(\"m3_Revenue_Segment\")))\n\n    trends_udf = udf(trends, StringType())\n\n    dataframe = dataframe.withColumn('trend', trends_udf(array(col('m1_total_revenue'), col('m2_total_revenue'), col('m3_total_revenue'))))\n    return dataframe"
              }
            }, {
              "id": "e545014f-e38f-339e-20ed-c974e77238c5",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "1b4b1059-db26-da32-868c-2ce0fbbb95d0"
              }
            }, {
              "id": "ff0ec115-ca6a-0b47-2cbf-e396df7a8e53",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "86696cf4-31c2-63bc-5d49-6e0b1ee41198",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql.functions import udf, col, when\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql import SparkSession\n\ndef transform(dataframe):\n    spark = SparkSession.builder.getOrCreate()\n    \n    dataframe = dataframe.fillna(0)\n    dataframe = dataframe.withColumnRenamed('m1_msisdn', 'msisdn')\n    put_revenue_segment_udf = udf(put_revenue_segment, StringType())\n\n    # voice_columns_dict = {\n    #     'm1_total_voice_usage': 'm1_voice_band',\n    #     'm2_total_voice_usage': 'm2_voice_band',\n    #     'm3_total_voice_usage': 'm3_voice_band'\n    # }\n    \n    # data_columns_dict = {\n    #     'm1_total_data_usage': 'm1_data_band',\n    #     'm2_total_data_usage': 'm2_data_band',\n    #     'm3_total_data_usage': 'm3_data_band'\n    # }\n    \n    month = ['m1', 'm2', 'm3']\n    \n    # # Iterate over the voice_columns_dict and month list\n    # for col_name, alias_name in voice_columns_dict.items():\n    #     for m in month:\n    #         if m in col_name:\n    #             dataframe = dataframe.withColumn(alias_name, voice_band(col_name, m))\n    \n    # # Iterate over the data_columns_dict and month list\n    # for col_name, alias_name in data_columns_dict.items():\n    #     for m in month:\n    #         if m in col_name:\n    #             dataframe = dataframe.withColumn(alias_name, data_band(col_name, m))\n\n    columns_dict = {\n    'm1_total_Revenue': 'm1_Revenue_Segment',\n    'm2_total_Revenue': 'm2_Revenue_Segment',\n    'm3_total_Revenue': 'm3_Revenue_Segment'\n    }\n    \n    # Iterate over the dictionary and apply the UDF to create revenue segment columns\n    for col_name, alias_name in columns_dict.items():\n        dataframe = dataframe.withColumn(alias_name, put_revenue_segment_udf(col(col_name)))\n\n    # # Define the list of columns to drop\n    # columns_to_drop = ['m1_total_voice_usage', 'm1_total_data_usage', 'm2_total_voice_usage', 'm2_total_data_usage', 'm3_total_voice_usage', 'm3_total_data_usage']\n    \n    # # Drop the columns from the DataFrame\n    # dataframe = dataframe.drop(*columns_to_drop)\n    \n    return dataframe\n\n# def voice_band(x_col, m):\n#     return when(col(x_col) == 0.0, m + \"_a)Zero\") \\\n#         .when(col(x_col) <= 8, m + \"_b)<=8\") \\\n#         .when(col(x_col) <= 20, m + \"_c)8 - 20\") \\\n#         .when(col(x_col) <= 40, m + \"_d)20 - 40\") \\\n#         .when(col(x_col) <= 100, m + \"_e)40 - 100\") \\\n#         .when(col(x_col) <= 200, m + \"_f)100 - 200\") \\\n#         .when(col(x_col) <= 500, m + \"_g)200 - 500\") \\\n#         .when(col(x_col) <= 1000, m + \"_h)500 - 1000\") \\\n#         .otherwise(m + \"_i)1000+\")\n        \n# def data_band(x_col, m):\n#     return  when(col(x_col) == 0.0000, m + '_a)Zero') \\\n#         .when((col(x_col) > 0) & (col(x_col) <= 50), m + '_b)0-50 MB') \\\n#         .when((col(x_col) > 50) & (col(x_col) <= 100),m + '_c)50-100 MB') \\\n#         .when((col(x_col) > 100) & (col(x_col) <= 250),m + '_d)100-250 MB') \\\n#         .when((col(x_col) > 250) & (col(x_col) <= 512),m + '_e)250-512 MB') \\\n#         .when((col(x_col) > 512) & (col(x_col) <= 1536), m +'_f)512 MB-1.5 GB') \\\n#         .when((col(x_col) > 1536) & (col(x_col) <= 3072), m +'_g)1.5-3 GB') \\\n#         .otherwise(m + '_h)3 GB +')\n\n\ndef put_revenue_segment(x):\n    y = 'Zero'\n    if x == 0.0:\n        y = 'Zero'\n    elif (x > 0) and (x <= 16):\n        y = 'very_low'\n    elif (x > 16) and (x <= 36):\n        y = 'low'\n    elif (x > 36) and (x <= 61):\n        y = 'medium'\n    elif (x > 61) and (x <= 175):\n        y = 'high'\n    else:\n        y = 'high_high'\n    return y"
              }
            }, {
              "id": "b0c65f9c-810d-7e5b-8afa-d69f082331f1",
              "operation": {
                "id": "6534f3f4-fa3a-49d9-b911-c213d3da8b5d",
                "name": "Filter Columns"
              },
              "parameters": {
                "selected columns": {
                  "selections": [{
                    "type": "columnList",
                    "values": ["msisdn", "service_band"]
                  }],
                  "excluding": false
                }
              }
            }, {
              "id": "e9d4823c-3faf-f506-f32f-04b7fae43b30",
              "operation": {
                "id": "6534f3f4-fa3a-49d9-b911-c213d3da8b5d",
                "name": "Filter Columns"
              },
              "parameters": {
                "selected columns": {
                  "selections": [{
                    "type": "columnList",
                    "values": ["m1_total_data_usage", "m1_total_voice_usage", "m3_total_data_usage", "m3_total_voice_usage", "tot_sum", "m2_Revenue_Segment", "m2_total_voice_usage", "m1_Revenue_Segment", "trend", "m2_total_data_usage", "m3_Revenue_Segment", "msisdn"]
                  }],
                  "excluding": false
                }
              }
            }, {
              "id": "a6e01d32-78ec-a807-167e-09e83d499734",
              "operation": {
                "id": "6534f3f4-fa3a-49d9-b911-c213d3da8b5d",
                "name": "Filter Columns"
              },
              "parameters": {
                "selected columns": {
                  "selections": [{
                    "type": "columnList",
                    "values": ["m1_total_data_usage", "m1_total_voice_usage", "m1_msisdn", "m3_total_data_usage", "m3_total_voice_usage", "m3_total_revenue", "m2_total_voice_usage", "m2_total_revenue", "m2_total_data_usage", "m1_total_revenue"]
                  }],
                  "excluding": false
                }
              }
            }, {
              "id": "54d3f084-e853-989f-b4d6-6974115f00df",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\r\nimport numpy as np\r\nimport pickle\r\n\r\ndef transform(dataframe):\r\n    dataframe = dataframe.toPandas()\r\n    dataframe = dataframe.rename(columns={'m1_msisdn': 'msisdn'})\r\n\r\n    columns = ['msisdn', 'total_voice_usage', 'total_data_usage', 'total_sms_usage']\r\n    usage_no_months = ['m1', 'm2', 'm3']\r\n\r\n    final = dataframe.fillna(0)\r\n    columns.remove('msisdn')\r\n\r\n    for i in columns:\r\n        li = []\r\n        for j in usage_no_months:\r\n            li.append(j + '_' + i)\r\n\r\n        name = i + '_total'\r\n        final[name] = (final[li[0]] + final[li[1]] + final[li[2]]) / 3\r\n\r\n    final = service_band(final, 'service_band')\r\n    \r\n    # # Save the final result to a pickle file\r\n    # with open('final_result.pickle', 'wb') as file:\r\n    #     pickle.dump(final, file)\r\n    \r\n    return final\r\n\r\ndef service_band(dataset, name):\r\n    dataset = dataset[['msisdn', 'total_voice_usage_total', 'total_data_usage_total', 'total_sms_usage_total']]\r\n    \r\n    dataset['total_voice_usage_total'].fillna(0, inplace=True)\r\n    dataset['total_data_usage_total'].fillna(0, inplace=True)\r\n    dataset['total_sms_usage_total'].fillna(0, inplace=True)\r\n\r\n    conditions = [\r\n        (dataset['total_voice_usage_total'] > 0) & (dataset['total_data_usage_total'] > 0) & (dataset['total_sms_usage_total'] > 0),\r\n        (dataset['total_voice_usage_total'] > 0) & (dataset['total_data_usage_total'] > 0) & (dataset['total_sms_usage_total'] <= 0),\r\n        (dataset['total_voice_usage_total'] > 0) & (dataset['total_sms_usage_total'] > 0) & (dataset['total_data_usage_total'] <= 0),\r\n        (dataset['total_data_usage_total'] > 0) & (dataset['total_sms_usage_total'] > 0) & (dataset['total_voice_usage_total'] <= 0),\r\n        (dataset['total_voice_usage_total'] > 0) & (dataset['total_sms_usage_total'] <= 0) & (dataset['total_data_usage_total'] <= 0),\r\n        (dataset['total_data_usage_total'] > 0) & (dataset['total_sms_usage_total'] <= 0) & (dataset['total_voice_usage_total'] <= 0),\r\n        (dataset['total_sms_usage_total'] > 0) & (dataset['total_data_usage_total'] <= 0) & (dataset['total_voice_usage_total'] <= 0),\r\n        (dataset['total_sms_usage_total'] == 0) & (dataset['total_data_usage_total'] == 0) & (dataset['total_voice_usage_total'] == 0)\r\n    ]\r\n    \r\n    choices = ['VDS', 'VD', 'VS', 'DS', 'V', 'D', 'S', 'no_usage']\r\n    \r\n    dataset[name] = np.select(conditions, choices, default='nill')\r\n    \r\n    return dataset\r\n"
              }
            }],
            "connections": [{
              "from": {
                "nodeId": "db6dde30-da35-38a8-e24c-f02c6d0ccbd0",
                "portIndex": 0
              },
              "to": {
                "nodeId": "ff0ec115-ca6a-0b47-2cbf-e396df7a8e53",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "7ca94422-381c-9c76-202a-6c59d148db68",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e9d4823c-3faf-f506-f32f-04b7fae43b30",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "b0c65f9c-810d-7e5b-8afa-d69f082331f1",
                "portIndex": 0
              },
              "to": {
                "nodeId": "ff0ec115-ca6a-0b47-2cbf-e396df7a8e53",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "a44fad41-58cd-d08d-a363-16da2bfeab11",
                "portIndex": 0
              },
              "to": {
                "nodeId": "f682aa1a-0071-d066-e355-df338b6d3a9e",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "e9d4823c-3faf-f506-f32f-04b7fae43b30",
                "portIndex": 0
              },
              "to": {
                "nodeId": "db6dde30-da35-38a8-e24c-f02c6d0ccbd0",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "86696cf4-31c2-63bc-5d49-6e0b1ee41198",
                "portIndex": 0
              },
              "to": {
                "nodeId": "7ca94422-381c-9c76-202a-6c59d148db68",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "8b1c9bf7-1b49-2b1e-b75e-2dab2a00bc99",
                "portIndex": 0
              },
              "to": {
                "nodeId": "477a197a-c1d4-5098-9b92-6c79b918a3cd",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "336f5c3f-7b9b-f30e-f66c-e2635a76aaaf",
                "portIndex": 0
              },
              "to": {
                "nodeId": "db6dde30-da35-38a8-e24c-f02c6d0ccbd0",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "f682aa1a-0071-d066-e355-df338b6d3a9e",
                "portIndex": 0
              },
              "to": {
                "nodeId": "8b1c9bf7-1b49-2b1e-b75e-2dab2a00bc99",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "ff0ec115-ca6a-0b47-2cbf-e396df7a8e53",
                "portIndex": 0
              },
              "to": {
                "nodeId": "2ae6aa34-be9e-04d4-4627-a47f6dab9846",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "a6e01d32-78ec-a807-167e-09e83d499734",
                "portIndex": 0
              },
              "to": {
                "nodeId": "86696cf4-31c2-63bc-5d49-6e0b1ee41198",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "54d3f084-e853-989f-b4d6-6974115f00df",
                "portIndex": 0
              },
              "to": {
                "nodeId": "b0c65f9c-810d-7e5b-8afa-d69f082331f1",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "8b1c9bf7-1b49-2b1e-b75e-2dab2a00bc99",
                "portIndex": 0
              },
              "to": {
                "nodeId": "919604ae-b5c0-8f8c-ed96-d48a46aa6e3f",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "8b1c9bf7-1b49-2b1e-b75e-2dab2a00bc99",
                "portIndex": 0
              },
              "to": {
                "nodeId": "a6e01d32-78ec-a807-167e-09e83d499734",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "477a197a-c1d4-5098-9b92-6c79b918a3cd",
                "portIndex": 0
              },
              "to": {
                "nodeId": "54d3f084-e853-989f-b4d6-6974115f00df",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "e545014f-e38f-339e-20ed-c974e77238c5",
                "portIndex": 0
              },
              "to": {
                "nodeId": "f682aa1a-0071-d066-e355-df338b6d3a9e",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "2ae6aa34-be9e-04d4-4627-a47f6dab9846",
                "portIndex": 0
              },
              "to": {
                "nodeId": "8644f710-030b-9579-ddba-dfe829cb99ae",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "919604ae-b5c0-8f8c-ed96-d48a46aa6e3f",
                "portIndex": 0
              },
              "to": {
                "nodeId": "336f5c3f-7b9b-f30e-f66c-e2635a76aaaf",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "aa2c61c6-6ef1-4016-a011-9990009ddee6",
                "portIndex": 0
              },
              "to": {
                "nodeId": "8b1c9bf7-1b49-2b1e-b75e-2dab2a00bc99",
                "portIndex": 1
              }
            }]
          },
          "thirdPartyData": {
            "gui": {
              "name": "Inner workflow of custom transformer",
              "nodes": {
                "2ae6aa34-be9e-04d4-4627-a47f6dab9846": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5308,
                    "y": 5675
                  }
                },
                "477a197a-c1d4-5098-9b92-6c79b918a3cd": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5613,
                    "y": 5189
                  }
                },
                "f682aa1a-0071-d066-e355-df338b6d3a9e": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5034,
                    "y": 5016
                  }
                },
                "db6dde30-da35-38a8-e24c-f02c6d0ccbd0": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5165,
                    "y": 5490
                  }
                },
                "a44fad41-58cd-d08d-a363-16da2bfeab11": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5144,
                    "y": 4930
                  }
                },
                "8644f710-030b-9579-ddba-dfe829cb99ae": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5348,
                    "y": 5788
                  }
                },
                "b0c65f9c-810d-7e5b-8afa-d69f082331f1": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5618,
                    "y": 5391
                  }
                },
                "aa2c61c6-6ef1-4016-a011-9990009ddee6": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5385,
                    "y": 4940
                  }
                },
                "a6e01d32-78ec-a807-167e-09e83d499734": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4836,
                    "y": 5173
                  }
                },
                "7ca94422-381c-9c76-202a-6c59d148db68": {
                  "uiName": "to calculate total revenue and trend",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4885,
                    "y": 5353
                  }
                },
                "8b1c9bf7-1b49-2b1e-b75e-2dab2a00bc99": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5174,
                    "y": 5098
                  }
                },
                "54d3f084-e853-989f-b4d6-6974115f00df": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5653,
                    "y": 5275
                  }
                },
                "336f5c3f-7b9b-f30e-f66c-e2635a76aaaf": {
                  "uiName": "for calculate_pct_drop",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5388,
                    "y": 5318
                  }
                },
                "e545014f-e38f-339e-20ed-c974e77238c5": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4939,
                    "y": 4930
                  }
                },
                "86696cf4-31c2-63bc-5d49-6e0b1ee41198": {
                  "uiName": "to calculate voice_band, data_band and revenue_segment",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4855,
                    "y": 5261
                  }
                },
                "e9d4823c-3faf-f506-f32f-04b7fae43b30": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4850,
                    "y": 5445
                  }
                },
                "ff0ec115-ca6a-0b47-2cbf-e396df7a8e53": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5280,
                    "y": 5587
                  }
                },
                "919604ae-b5c0-8f8c-ed96-d48a46aa6e3f": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5338,
                    "y": 5220
                  }
                }
              }
            }
          },
          "publicParams": []
        }
      }
    }, {
      "id": "25f2b9ac-2efa-a3bb-990a-c58ecab04555",
      "operation": {
        "id": "65240399-2987-41bd-ba7e-2944d60a3404",
        "name": "Create Custom Transformer"
      },
      "parameters": {
        "inner workflow": {
          "workflow": {
            "nodes": [{
              "id": "73ad23a2-5c31-d77c-db67-d52c203c4b26",
              "operation": {
                "id": "f94b04d7-ec34-42f7-8100-93fe235c89f8",
                "name": "Source"
              },
              "parameters": {

              }
            }, {
              "id": "c0d041de-0e70-656d-5ca5-f7616e9d00e4",
              "operation": {
                "id": "e652238f-7415-4da6-95c6-ee33808561b2",
                "name": "Sink"
              },
              "parameters": {

              }
            }, {
              "id": "9ba4aef4-2097-7eb7-11e2-fe4f7fc823b0",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import os.path\nfrom pathlib import Path\nfrom fastapi import Depends, FastAPI, HTTPException\nimport dask.dataframe as dd\nfrom icecream import ic\nimport pandas as pd\nfrom pathlib import Path\nimport traceback\n\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\netl_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                           'total_amt': 'float32'} \nderived_pack_info_location= '/home/tnmops/seahorse3_bkp/'\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\n# dag_run_id = \"manual__2023-07-10T11:06:51\"    \nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nfeature_unit_list= ['unit_in_mb', 'price']\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\n\ndef get_file_names():\n    return {\n        \"purchase\": {\n            \"m1\": \"purchase_dec_full_df.csv\",\n            \"m2\": \"purchase_jan_full_df.csv\",\n            \"m3\": \"purchase_feb_full_df.csv\",\n        }\n    }\n    \n\n\nclass PreProcessedData:\n    def __init__(self, purchase=None, usage=None):\n        self.purchase = purchase\n        self.usage = usage\n        # combined data of purchase and pack info\n        self.purchase_pack_info = None\n        \n\n\nclass TNMData:\n    def __init__(self,\n                 purchase_df=None,\n                 usage_df=None,\n                 pack_df=None):\n        self.purchase_df = purchase_df\n        self.usage_df = usage_df\n        self.pack_df = pack_df\n        \n\ndef validity_banding(df):\n    validity_column = PACK_INFO_VALIDITY_NAME\n    df.loc[df[validity_column] < 1, 'band'] = 'Hourly'\n    df.loc[(df[validity_column] >= 1) & (df[validity_column] < 5), 'band'] = 'Daily'\n    df.loc[(df[validity_column] >= 5) & (df[validity_column] < 15), 'band'] = 'Weekly'\n    df.loc[(df[validity_column] >= 15) & (df[validity_column] <= 31), 'band'] = 'Monthly'\n    df.loc[df[validity_column] > 31, 'band'] = 'Unlimited'\n\n    return df\n    \n\ndef form_bands(pur_pack_df, packinfo_df):\n    data_li = []\n    tot = 0\n    if type(packinfo_df) == dd.core.DataFrame:\n        packinfo_df = packinfo_df.compute()\n    if type(pur_pack_df) == dd.core.DataFrame:\n        pur_pack_df = pur_pack_df.compute()\n\n    # pur_pack_df.to_csv('/data/autopilot/ml/pur_pack_df.csv',header=True,index=False)\n    # for service in pur_pack_df['Product_Type'].unique():\n\n    for service in ['DATA']:\n\n        # for service in cfg.feature_mapping.keys():\n        ic(f\"for the serive {service}\")\n        pur_pack_df_service = pur_pack_df[pur_pack_df['product_type'] == service]\n        # for unit_name in cfg.feature_mapping.get(service):\n        for unit_name in feature_unit_list:\n            ic(f\"for the unit name {unit_name}\")\n            dft = pur_pack_df_service.groupby(unit_name).agg({'msisdn': 'count'}).rename(\n                columns={\"msisdn\": \"m_count\"}).reset_index()\n            # remove if any categorical values are present\n            dft = dft.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n            dft = dft.round(0)\n            dft = dft.astype(int)\n            print('type(dft) is ', type(dft))\n            unit_count = dft.sort_values(by=unit_name)\n            unit_count = unit_count.reset_index(drop=True)\n            print('len of unit_count is ', len(unit_count))\n            print('unit_count is ', unit_count)\n            # function that return the band accourding to mininmum distribution percentage\n            b = get_bands(unit_count, unit_name)\n            # removing dup\n            b = list(set(b))\n            b.sort()\n            # for cuts method is like  (0,5] means the range not including 0 to including 5 ie: x:  x>0  and x= 5\n            b[0] = b[0] - 0.01\n            # forming labels accourding to the bands\n\n            cut_labels = []\n            for i in range(len(b) - 1):\n                cut_labels.append(f'{b[i]}-{b[i + 1]} {service} {unit_name} band')\n            tot = tot + len(cut_labels)\n            ic(f\" the bands are {b} and the cut labels are {cut_labels}\")\n            # pur_pack_df_service[unit_name] = pur_pack_df_service[unit_name].apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n\n            packinfo_df['band'] = pd.cut(packinfo_df[unit_name], bins=b, labels=cut_labels)\n            ic(\"the band value_counts\", packinfo_df['band'].value_counts())\n            data_li.append(packinfo_df.copy())\n            ic(\"len is \", len(data_li))\n    final_df = pd.concat(data_li)\n    return final_df\n    \n\ndef get_bands(dft, col, per_val=15):\n    total = dft['m_count'].sum()\n    li = []\n    last_pointer = 0\n    for i in range(0, len(dft)):\n        # print(\"the row\",i)\n        # print(dft.iloc[i])\n        per = (sum(dft.iloc[last_pointer:i + 1].m_count) / total) * 100\n        # print(\"the percentage\" , per)\n        if per > per_val:\n            band = set()\n            band.add(min(dft.iloc[last_pointer:i + 1][col]))\n            band.add(max(dft.iloc[last_pointer:i + 1][col]))\n            # band = (min(dft.iloc[last_pointer:i+1].b_validity) , max(dft.iloc[last_pointer:i+1].b_validity))\n\n            li.append(min(dft.iloc[last_pointer:i + 1][col]))\n            last_pointer = i + 1\n\n    band_last = (min(dft.iloc[last_pointer:][col]), max(dft.iloc[last_pointer:][col]))\n    band_last = set()\n    band_last.add(min(dft.iloc[last_pointer:][col]))\n    band_last.add(max(dft.iloc[last_pointer:][col]))\n    # print(\"the last band\",band_last)\n    li.append(min(dft.iloc[last_pointer:][col]))\n    li.append(max(dft.iloc[last_pointer:][col]))\n    return li\n    \n    \nclass PreprocessData:\n    def __init__(self, tnm_data=None, dag_run_id=None):\n        self.tnm_data = tnm_data\n        self.dag_run_id = dag_run_id\n        self.pre_data_obj = None\n        self.pack_features_encoded = None\n\n    def pre_process_purchase(self):\n        # self.tnm_data.purchase_df['total_cnt'] = self.tnm_data.purchase_df['total_cnt'].str.lower()\n        # self.tnm_data.purchase_df['total_cnt'] = self.tnm_data.purchase_df['total_cnt'].str.strip()\n\n        final_df = self.tnm_data.purchase_df.merge(\n            self.tnm_data.pack_df, left_on=\"product_id\", right_on=\"product_id\", how='inner')[\n            ['msisdn', 'cdr_date', 'total_cnt', 'product_type', 'validity_in_days', 'unit_in_mb', 'price','pack_types','pack_popularity',\n             'product_id']]\n        final_df = final_df.dropna()\n\n        print('len of final_df', len(final_df))\n        path = os.path.join(ml_location,  \"purchase_filtered\")\n        print('path is ', path)\n        Path(path).mkdir(parents=True, exist_ok=True)\n        final_df.to_parquet(path)\n\n        # save the file to path\n\n        self.pre_data_obj = PreProcessedData(final_df, None)\n\n    def get_pre_processed_data(self):\n        ic(\" getting pre processed data \")\n        if self.pre_data_obj is None:\n            ic(\"error - the data is not preprocessed \")\n            raise Exception(\"not procedssed\")\n\n        return self.pre_data_obj\n\n    def preprocess_pack_features(self):\n        #         test_df = self.pre_data_obj.purchase\n        #         test_df.to_csv('/data/autopilot/ml/pre_data_obj_purchase.csv',header=True,index=False)\n\n        #         print('len of self.pre_data_obj.purchase  ',len(self.pre_data_obj.purchase))\n\n        df = form_bands(self.pre_data_obj.purchase, self.tnm_data.pack_df.copy())\n        print('df.columns is ', df.columns)\n        df1 = validity_banding(self.tnm_data.pack_df.compute())\n        \n        df2 = self.tnm_data.pack_df.compute()\n        df2['band'] = df2['pack_types']\n        df3 = self.tnm_data.pack_df.compute()\n        df3['band'] = df3['pack_popularity']\n        \n        print('df1.columns is ', df1.columns)\n        final_df1 = pd.concat([df, df1,df2,df3])\n        print('final_df1.columns is ', final_df1.columns)\n        df = pd.crosstab(final_df1[PACK_INFO_PACK_COLUMN_NAME], final_df1[\"band\"])\n        df.columns.name = None\n        df.reset_index(inplace=True)\n\n        self.pack_features_encoded = df\n\n    def save_pack_features_df(self, format=None):\n        self.pack_features_encoded.to_csv(\n            os.path.join(ml_location,  'pack_features_encoded.csv'), header=True, index=False)\n            \n            \n            \n\n\n\n\ndef pre_process(dag_run_id):\n        # loaded the data\n\n    file_name_dict = get_file_names()\n    print(\"purchase preprocess  ongoing \")\n    data = {}\n    for month in purchase_no_months_seg:\n        data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"purchase\").get(month)),\n                                  dtype=RECHARGE_TRANSACTION_DTYPES)\n\n    pack_info = dd.read_csv(os.path.join(derived_pack_info_location, 'pack_info_der.csv'),\n                            dtype=PACK_INFO_DTYPES)\n    tnm_data = TNMData(purchase_df=dd.concat(list(data.values())), pack_df=pack_info)\n    pre_instance = PreprocessData(tnm_data, dag_run_id)\n    # # merging purchase and pack\n\n    pre_instance.pre_process_purchase()\n    p_data = pre_instance.get_pre_processed_data()\n    # forming user purchase count matrix\n    product_name = PACK_INFO_PACK_COLUMN_NAME\n    result = (\n        p_data.purchase.groupby([\"msisdn\", product_name])\n        .agg({product_name: \"count\"})\n        .rename(columns={product_name: \"bundle_counts\"})\n        .reset_index()\n    )\n    result = result.compute()\n    result.dropna(subset=[product_name], inplace=True)\n    df = (\n        pd.crosstab(\n            result.msisdn,\n            result[product_name],\n            values=result.bundle_counts,\n            aggfunc=sum,\n\n        )\n    )\n    ic(\"crosstab of purchase done \")\n    df1 = df.fillna(0)\n    df1 = df1.reset_index()\n    op_path_dir = os.path.join(ml_location)\n    Path(op_path_dir).mkdir(parents=True, exist_ok=True)\n    df1.to_csv(os.path.join(ml_location,'user_pack_matrix.csv'), header=True, index=False)\n    ic(\"outputed user pack matrix\")\n    # forming pack feature matrix\n    pre_instance.preprocess_pack_features()\n    pre_instance.save_pack_features_df()\n\n    ic(\"outputed  pack feature matrix \")\n    \n    \n    return df1\n    \n    \n    \n\ndef transform(dataframe):\n    df = pre_process(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }, {
              "id": "870a75fd-9a62-629e-127e-f40828830828",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\netl_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\n# dag_run_id = \"manual__2023-07-10T11:06:51\"    \nMSISDN_COL_NAME = 'msisdn'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\n\ndef matrix_operations(dag_run_id):\n    matrix_pack_features = pd.read_csv(\n        os.path.join(ml_location, \"pack_features_encoded.csv\"))\n    matrix_user_pack = pd.read_csv(os.path.join(ml_location, \"user_pack_matrix.csv\"))\n    msisdn_list = matrix_user_pack.pop(MSISDN_COL_NAME)\n    product_ids = matrix_user_pack.columns\n\n#     product_ids = [int(x) for x in product_ids]\n    product_ids = [float(x) for x in product_ids]\n    matrix_pack_features = matrix_pack_features[matrix_pack_features[PACK_INFO_PACK_COLUMN_NAME].isin(product_ids)]\n    product_id_ls = matrix_pack_features.pop(PACK_INFO_PACK_COLUMN_NAME)\n    pack_features_cols = matrix_pack_features.columns\n    ic(\"performing a x b\")\n    final_matrix = np.matmul(matrix_user_pack, matrix_pack_features)\n    final_matrix.columns = pack_features_cols\n    final_matrix.index = msisdn_list\n    # normilizing the matrix\n    ic(\"going to normalize \")\n    final_matrix = final_matrix.div(final_matrix.sum(axis=1), axis=0)\n    matrix_pack_features_t = matrix_pack_features.T\n    final_matrix_1 = np.matmul(final_matrix.values, matrix_pack_features_t.values)\n    final_matrix_1 = pd.DataFrame(final_matrix_1, index=msisdn_list, columns=product_id_ls)\n    ic(\"got the final matrix \")\n\n    final_matrix_1.columns = final_matrix_1.columns.astype(int)\n    print(final_matrix_1.columns)\n    final_matrix_1.to_csv(os.path.join(ml_location, \"matrix.csv\"), header=True,\n                          index=True)\n\n    return final_matrix_1\n        \n        \n        \n\n\ndef transform(dataframe):\n    df  = matrix_operations(\"manual__2023-07-10T11:06:51\")\n    return df"
              }
            }],
            "connections": [{
              "from": {
                "nodeId": "73ad23a2-5c31-d77c-db67-d52c203c4b26",
                "portIndex": 0
              },
              "to": {
                "nodeId": "9ba4aef4-2097-7eb7-11e2-fe4f7fc823b0",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "9ba4aef4-2097-7eb7-11e2-fe4f7fc823b0",
                "portIndex": 0
              },
              "to": {
                "nodeId": "870a75fd-9a62-629e-127e-f40828830828",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "870a75fd-9a62-629e-127e-f40828830828",
                "portIndex": 0
              },
              "to": {
                "nodeId": "c0d041de-0e70-656d-5ca5-f7616e9d00e4",
                "portIndex": 0
              }
            }]
          },
          "thirdPartyData": {
            "gui": {
              "name": "Inner workflow of custom transformer",
              "nodes": {
                "73ad23a2-5c31-d77c-db67-d52c203c4b26": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5087,
                    "y": 4959
                  }
                },
                "c0d041de-0e70-656d-5ca5-f7616e9d00e4": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5437,
                    "y": 5210
                  }
                },
                "9ba4aef4-2097-7eb7-11e2-fe4f7fc823b0": {
                  "uiName": "read_and_preprocess",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5128,
                    "y": 5051
                  }
                },
                "870a75fd-9a62-629e-127e-f40828830828": {
                  "uiName": "matrix_multiplication",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5342,
                    "y": 5090
                  }
                }
              }
            }
          },
          "publicParams": []
        }
      }
    }, {
      "id": "a6b06cd3-da8e-8500-9273-9945bf7ed450",
      "operation": {
        "id": "65240399-2987-41bd-ba7e-2944d60a3404",
        "name": "Create Custom Transformer"
      },
      "parameters": {
        "inner workflow": {
          "workflow": {
            "nodes": [{
              "id": "913e503e-0fc8-59d5-a26a-b13b351c79bd",
              "operation": {
                "id": "d5f4e717-429f-4a28-a0d3-eebba036363a",
                "name": "Handle Missing Values"
              },
              "parameters": {
                "columns": {
                  "selections": [{
                    "type": "typeList",
                    "values": ["numeric"]
                  }],
                  "excluding": false
                },
                "strategy": {
                  "replace with custom value": {
                    "value": "0"
                  }
                },
                "user-defined missing values": [{

                }]
              }
            }, {
              "id": "62cc24cd-3898-5e6b-c60c-eab1354a5e4c",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql.functions import sum, when, col, udf\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import sum as Fsum\r\nfrom pyspark.sql.types import StringType\r\nfrom pyspark.sql import functions as F\r\n\r\n\r\ndef transform(dataframe):\r\n    spark = SparkSession.builder.getOrCreate()\r\n    dataframe = dataframe.fillna(0)\r\n    dataframe = dataframe.withColumnRenamed('m1_msisdn', 'msisdn')\r\n\r\n\r\n    \r\n    dataframe = dataframe.select('msisdn', 'm1_total_cnt', 'm2_total_cnt', 'm3_total_cnt')\r\n    dataframe = dataframe.groupBy('msisdn').agg(\r\n        F.sum('m1_total_cnt').alias('m1_total_recharge_cnt'),\r\n        F.sum('m2_total_cnt').alias('m2_total_recharge_cnt'),\r\n        F.sum('m3_total_cnt').alias('m3_total_recharge_cnt') )\r\n        \r\n        \r\n\r\n    return dataframe\r\n"
              }
            }, {
              "id": "a7180f24-46b2-2c51-a451-ad8ab321f8c8",
              "operation": {
                "id": "6534f3f4-fa3a-49d9-b911-c213d3da8b5d",
                "name": "Filter Columns"
              },
              "parameters": {
                "selected columns": {
                  "selections": [{
                    "type": "columnList",
                    "values": ["m1_cdr_date", "m2_cdr_date", "m3_cdr_date"]
                  }],
                  "excluding": true
                }
              }
            }, {
              "id": "2593e9f9-83d6-6e30-8dbe-20d55ecf603a",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql.functions import col, udf, when\nfrom pyspark.sql.types import FloatType, IntegerType\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import Bucketizer\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import TimestampType\nimport pandas as pd\nimport os\nfrom pathlib import Path\n\n\ndef segmentaion_fun1(dataframe):\n    transformed_df = dataframe.withColumn(\"value\", col(\"R_Score\") * 100 + col(\"F_Score\") * 10 + col(\"M_Score\"))\n    \n    transformed_df = transformed_df.withColumn(\"segments\",\n        when(col(\"value\").isin([555, 554, 544, 545, 454, 455, 445]), \"Champions\")\n        .when(col(\"value\").isin([543, 444, 435, 355, 354, 345, 344, 335]), \"Loyal_Customers\")\n        .when(col(\"value\").isin([553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323]), \"Potential_Loyalist\")\n        .when(col(\"value\").isin([512, 511, 422, 421, 412, 411, 311]), \"Recent_Customers\")\n        .when(col(\"value\").isin([525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313]), \"Promising_Customers\")\n        .when(col(\"value\").isin([535, 534, 443, 434, 343, 334, 325, 324]), \"Customers_needing_Attention\")\n        .when(col(\"value\").isin([331, 321, 312, 221, 213]), \"About_to_Sleep\")\n        .when(col(\"value\").isin([255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124]), \"At_Risk\")\n        .when(col(\"value\").isin([155, 154, 144, 214, 215, 115, 114, 113]), \"Cant_Loose_them\")\n        .when(col(\"value\").isin([332, 322, 231, 241, 251, 233, 232, 223, 222, 132, 123, 122, 212, 211]), \"Hibernating\")\n        .otherwise(\"Lost\")\n    )\n    \n    return transformed_df\n\n\n\ndef perform_rfm(dataframe):\n    # Assuming 'dataframe' is the input DataFrame\n    dataframe = dataframe.fillna(0)\n\n\n    # Convert 'purchase_date' column to timestamp\n    dataframe = dataframe.withColumn('fct_dt', F.from_unixtime(F.unix_timestamp('fct_dt', 'yyyy-MM-dd')).cast(TimestampType()))\n    \n    recency_df = dataframe.groupBy('msisdn').agg(F.max('fct_dt').alias('MaxPurchaseDate')).select('msisdn', 'MaxPurchaseDate')\n    \n    # Calculate recency in days\n    recency_df = recency_df.withColumn('Recency', F.datediff(F.lit(recency_df.select(F.max('MaxPurchaseDate')).collect()[0][0]), 'MaxPurchaseDate'))\n    \n    # Drop the 'MaxPurchaseDate' column\n    recency_df = recency_df.drop('MaxPurchaseDate')\n    \n    print(\"done with recency\")\n    \n\n    frequency_df = dataframe.groupby('msisdn').count()\n    frequency_df = frequency_df.withColumnRenamed('count', 'Frequency')\n    \n    print(\"done with frequency\")\n    \n    \n    monitory_df = dataframe.groupBy('msisdn').agg(F.sum('total_revenue').alias('monitery'))\n\n    print(\"done with monitory\")\n    \n    rfm_data_base = recency_df.join(frequency_df, on='msisdn').join(monitory_df, on='msisdn')\n\n    return rfm_data_base\n    \ndef form_segements(dataframe):\n    # Create a SparkSession if not already created\n    spark = SparkSession.builder.getOrCreate()\n    \n    # calculating R_Score---------------------------------\n    # Compute the percentiles using Spark SQL's approx_percentile function\n    percentiles = dataframe.stat.approxQuantile(\"Recency\", [0.2, 0.4, 0.6, 0.8], 0.01)\n    bins_recency = [-1] + percentiles + [dataframe.agg({\"Recency\": \"max\"}).collect()[0][0]]\n    \n    # Sort and process duplicates in bins_recency\n    bins_recency = sorted(list(set(bins_recency)))\n    \n    # Define the bucketizer UDF\n    bucketizer_udf = udf(lambda x: float(len(bins_recency) - sum(x > i for i in bins_recency)), FloatType())\n    \n    # Apply the UDF to bucketize the recency values\n    dataframe = dataframe.withColumn(\"R_Score\", bucketizer_udf(col(\"Recency\").cast(FloatType())))\n    \n    # Cast the R_Score column to integer\n    dataframe = dataframe.withColumn(\"R_Score\", col(\"R_Score\").cast(\"integer\"))\n    \n    # calculating F_score-------------------------------------------------------------\n    # Define the number of bins\n    num_bins = 5\n    \n    # Cast the \"Frequency\" column to DoubleType\n    dataframe = dataframe.withColumn(\"Frequency\", col(\"Frequency\").cast(DoubleType()))\n    \n    # Calculate the range of frequencies\n    frequency_range = dataframe.agg({\"Frequency\": \"min\"}).collect()[0][0], dataframe.agg({\"Frequency\": \"max\"}).collect()[0][0]\n    \n    # Create evenly spaced splits based on the frequency range and the number of bins\n    splits_frequency = [frequency_range[0] + i * (frequency_range[1] - frequency_range[0]) / num_bins for i in range(num_bins+1)]\n    \n    # Create the Bucketizer transformer\n    bucketizer = Bucketizer(splits=splits_frequency, inputCol=\"Frequency\", outputCol=\"F_Score\")\n    \n    # Apply the Bucketizer transformer to create the \"F_Score\" column\n    dataframe = bucketizer.transform(dataframe)\n    \n    # Convert the \"F_Score\" column to integer type\n    dataframe = dataframe.withColumn(\"F_Score\", (col(\"F_Score\") + 1).cast(\"integer\"))\n    \n    # calculating M_Score-----------------------------------------------------------------\n    # Define the number of bins\n    num_bins = 5\n    \n    # Cast the \"monitery\" column to DoubleType\n    dataframe = dataframe.withColumn(\"monitery\", col(\"monitery\").cast(DoubleType()))\n    \n    # Calculate the range of monitery\n    monitery_range = dataframe.agg({\"monitery\": \"min\"}).collect()[0][0], dataframe.agg({\"monitery\": \"max\"}).collect()[0][0]\n    \n    # Create evenly spaced splits based on the monitery range and the number of bins\n    splits_monitery = [monitery_range[0] + i * (monitery_range[1] - monitery_range[0]) / num_bins for i in range(num_bins+1)]\n    \n    # Create the Bucketizer transformer\n    bucketizer = Bucketizer(splits=splits_monitery, inputCol=\"monitery\", outputCol=\"M_Score\")\n    \n    # Apply the Bucketizer transformer to create the \"M_Score\" column\n    dataframe = bucketizer.transform(dataframe)\n    \n    # Convert the \"F_Score\" column to integer type\n    dataframe = dataframe.withColumn(\"M_Score\", (col(\"M_Score\") + 1).cast(\"integer\"))\n    \n    dataframe = segmentaion_fun1(dataframe)\n    \n    return dataframe\n\ndef transform(dataframe):\n    dataframe = perform_rfm(dataframe)\n    dataframe = form_segements(dataframe)\n    \n    dataframe = dataframe.toPandas()\n    \n    file_path = '/home/tnmops/seahorse3_bkp/'\n    Path(file_path).mkdir(parents=True, exist_ok=True)\n\n    path_dd = os.path.join(file_path, \"rfm.csv\")\n    dataframe.to_csv(path_dd,index=False,header = True)\n    \n    return dataframe\n    "
              }
            }, {
              "id": "96a02413-c135-7622-0d80-a26d2492918d",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "9d1079c4-b1f0-c724-b05c-a64d65aeaead"
              }
            }, {
              "id": "e0b22c0e-2660-78ca-cbd5-feaac41f325f",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "right prefix": "m3_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "m1_msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "94b9c9f7-e708-faa5-a6d6-1c1b249385d9",
              "operation": {
                "id": "d5f4e717-429f-4a28-a0d3-eebba036363a",
                "name": "Handle Missing Values"
              },
              "parameters": {
                "columns": {
                  "selections": [{
                    "type": "typeList",
                    "values": ["numeric", "timestamp"]
                  }],
                  "excluding": false
                },
                "strategy": {
                  "replace with custom value": {
                    "value": "0"
                  }
                },
                "user-defined missing values": [{

                }]
              }
            }, {
              "id": "852333cb-8548-e941-b1e5-fd583bd20acd",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "a34ae56c-0b70-a4b0-b8e9-eecead0f4bf1"
              }
            }, {
              "id": "897e3e9b-1b29-4115-19f7-dda98bd5637b",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "left prefix": "m1_",
                "right prefix": "m2_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "436c9f12-665f-b09a-80e1-62879d518292",
              "operation": {
                "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
                "name": "Python Transformation"
              },
              "parameters": {
                "code": "from pyspark.sql.functions import sum, when, col, udf\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import sum as Fsum\r\nfrom pyspark.sql.types import StringType\r\nfrom pyspark.sql import functions as F\r\n\r\n\r\ndef transform(dataframe):\r\n    spark = SparkSession.builder.getOrCreate()\r\n    dataframe = dataframe.fillna(0)\r\n    dataframe = dataframe.withColumnRenamed('m1_msisdn', 'msisdn')\r\n\r\n\r\n    \r\n    dataframe = dataframe.select('msisdn', 'm1_total_cnt', 'm2_total_cnt', 'm3_total_cnt')\r\n    dataframe = dataframe.groupBy('msisdn').agg(\r\n        F.sum('m1_total_cnt').alias('m1_total_purchase_cnt'),\r\n        F.sum('m2_total_cnt').alias('m2_total_purchase_cnt'),\r\n        F.sum('m3_total_cnt').alias('m3_total_purchase_cnt') )\r\n        \r\n        \r\n\r\n    return dataframe\r\n"
              }
            }, {
              "id": "2e048706-309f-477f-ae99-14b2e21ec6b4",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "fa4db02b-1362-4e9e-1aab-9c1b9baced8b",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "right prefix": "m3_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "m1_msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "08a9b8d8-cedc-3aa1-bcb0-296aaa88fc67",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "a6a0b872-eeba-819a-941b-d196150fe68c"
              }
            }, {
              "id": "3bb1f50a-2223-d0ce-5bd3-2cf2eef06857",
              "operation": {
                "id": "e652238f-7415-4da6-95c6-ee33808561b2",
                "name": "Sink"
              },
              "parameters": {

              }
            }, {
              "id": "e2c729ed-8f17-427a-aee6-f7bbb338fc94",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "141dbda4-8c9b-6aee-df91-108063810114",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "left prefix": "m1_",
                "right prefix": "m2_",
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "7fcafcda-4d77-23ee-4860-5a260c18d24a",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "dbc838c5-c9cc-2e19-a447-324fc9da023f"
              }
            }, {
              "id": "6eb5a450-fb65-b220-3764-ac138c090074",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "48b2dd6e-b3be-82ba-a4b6-0fb518c686dc"
              }
            }, {
              "id": "575c23f1-9fb9-d633-ee53-6e888c392aed",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "f5ea927d-16f9-4c18-87e3-e23f1933f5e5"
              }
            }, {
              "id": "fb8e56f1-f8d1-e378-40fe-148efd6d286a",
              "operation": {
                "id": "06374446-3138-4cf7-9682-f884990f3a60",
                "name": "Join"
              },
              "parameters": {
                "join type": {
                  "Left outer": {

                  }
                },
                "join columns": [{
                  "left column": {
                    "type": "column",
                    "value": "msisdn"
                  },
                  "right column": {
                    "type": "column",
                    "value": "msisdn"
                  }
                }]
              }
            }, {
              "id": "59410006-afa5-d4d7-b041-10204294cedc",
              "operation": {
                "id": "f94b04d7-ec34-42f7-8100-93fe235c89f8",
                "name": "Source"
              },
              "parameters": {

              }
            }, {
              "id": "fa4ceb58-50b7-1257-4a31-b11fb84f5ab7",
              "operation": {
                "id": "1a3b32f0-f56d-4c44-a396-29d2dfd43423",
                "name": "Read DataFrame"
              },
              "parameters": {
                "data source": "338c9523-aa3a-be34-ee4d-30aa1ac21285"
              }
            }],
            "connections": [{
              "from": {
                "nodeId": "2593e9f9-83d6-6e30-8dbe-20d55ecf603a",
                "portIndex": 0
              },
              "to": {
                "nodeId": "2e048706-309f-477f-ae99-14b2e21ec6b4",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "852333cb-8548-e941-b1e5-fd583bd20acd",
                "portIndex": 0
              },
              "to": {
                "nodeId": "fa4db02b-1362-4e9e-1aab-9c1b9baced8b",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "436c9f12-665f-b09a-80e1-62879d518292",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e2c729ed-8f17-427a-aee6-f7bbb338fc94",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "913e503e-0fc8-59d5-a26a-b13b351c79bd",
                "portIndex": 0
              },
              "to": {
                "nodeId": "3bb1f50a-2223-d0ce-5bd3-2cf2eef06857",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "fa4ceb58-50b7-1257-4a31-b11fb84f5ab7",
                "portIndex": 0
              },
              "to": {
                "nodeId": "897e3e9b-1b29-4115-19f7-dda98bd5637b",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "7fcafcda-4d77-23ee-4860-5a260c18d24a",
                "portIndex": 0
              },
              "to": {
                "nodeId": "2593e9f9-83d6-6e30-8dbe-20d55ecf603a",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "e0b22c0e-2660-78ca-cbd5-feaac41f325f",
                "portIndex": 0
              },
              "to": {
                "nodeId": "436c9f12-665f-b09a-80e1-62879d518292",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "59410006-afa5-d4d7-b041-10204294cedc",
                "portIndex": 0
              },
              "to": {
                "nodeId": "fb8e56f1-f8d1-e378-40fe-148efd6d286a",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "fa4db02b-1362-4e9e-1aab-9c1b9baced8b",
                "portIndex": 0
              },
              "to": {
                "nodeId": "94b9c9f7-e708-faa5-a6d6-1c1b249385d9",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "575c23f1-9fb9-d633-ee53-6e888c392aed",
                "portIndex": 0
              },
              "to": {
                "nodeId": "141dbda4-8c9b-6aee-df91-108063810114",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "a7180f24-46b2-2c51-a451-ad8ab321f8c8",
                "portIndex": 0
              },
              "to": {
                "nodeId": "62cc24cd-3898-5e6b-c60c-eab1354a5e4c",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "6eb5a450-fb65-b220-3764-ac138c090074",
                "portIndex": 0
              },
              "to": {
                "nodeId": "897e3e9b-1b29-4115-19f7-dda98bd5637b",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "62cc24cd-3898-5e6b-c60c-eab1354a5e4c",
                "portIndex": 0
              },
              "to": {
                "nodeId": "fb8e56f1-f8d1-e378-40fe-148efd6d286a",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "94b9c9f7-e708-faa5-a6d6-1c1b249385d9",
                "portIndex": 0
              },
              "to": {
                "nodeId": "a7180f24-46b2-2c51-a451-ad8ab321f8c8",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "897e3e9b-1b29-4115-19f7-dda98bd5637b",
                "portIndex": 0
              },
              "to": {
                "nodeId": "fa4db02b-1362-4e9e-1aab-9c1b9baced8b",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "e2c729ed-8f17-427a-aee6-f7bbb338fc94",
                "portIndex": 0
              },
              "to": {
                "nodeId": "2e048706-309f-477f-ae99-14b2e21ec6b4",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "2e048706-309f-477f-ae99-14b2e21ec6b4",
                "portIndex": 0
              },
              "to": {
                "nodeId": "913e503e-0fc8-59d5-a26a-b13b351c79bd",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "08a9b8d8-cedc-3aa1-bcb0-296aaa88fc67",
                "portIndex": 0
              },
              "to": {
                "nodeId": "141dbda4-8c9b-6aee-df91-108063810114",
                "portIndex": 1
              }
            }, {
              "from": {
                "nodeId": "141dbda4-8c9b-6aee-df91-108063810114",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e0b22c0e-2660-78ca-cbd5-feaac41f325f",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "fb8e56f1-f8d1-e378-40fe-148efd6d286a",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e2c729ed-8f17-427a-aee6-f7bbb338fc94",
                "portIndex": 0
              }
            }, {
              "from": {
                "nodeId": "96a02413-c135-7622-0d80-a26d2492918d",
                "portIndex": 0
              },
              "to": {
                "nodeId": "e0b22c0e-2660-78ca-cbd5-feaac41f325f",
                "portIndex": 1
              }
            }]
          },
          "thirdPartyData": {
            "gui": {
              "name": "Inner workflow of custom transformer",
              "nodes": {
                "913e503e-0fc8-59d5-a26a-b13b351c79bd": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5541,
                    "y": 5460
                  }
                },
                "141dbda4-8c9b-6aee-df91-108063810114": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5787,
                    "y": 4740
                  }
                },
                "3bb1f50a-2223-d0ce-5bd3-2cf2eef06857": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 5576,
                    "y": 5601
                  }
                },
                "59410006-afa5-d4d7-b041-10204294cedc": {
                  "uiName": "",
                  "color": "#2F4050",
                  "coordinates": {
                    "x": 4847,
                    "y": 4867
                  }
                },
                "fb8e56f1-f8d1-e378-40fe-148efd6d286a": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5055,
                    "y": 5092
                  }
                },
                "fa4ceb58-50b7-1257-4a31-b11fb84f5ab7": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5197,
                    "y": 4598
                  }
                },
                "fa4db02b-1362-4e9e-1aab-9c1b9baced8b": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5279,
                    "y": 4735
                  }
                },
                "e0b22c0e-2660-78ca-cbd5-feaac41f325f": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5950,
                    "y": 4863
                  }
                },
                "436c9f12-665f-b09a-80e1-62879d518292": {
                  "uiName": "purchase_procees",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5772,
                    "y": 5003
                  }
                },
                "a7180f24-46b2-2c51-a451-ad8ab321f8c8": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5314,
                    "y": 4915
                  }
                },
                "62cc24cd-3898-5e6b-c60c-eab1354a5e4c": {
                  "uiName": "recharge_process",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5235,
                    "y": 5007
                  }
                },
                "94b9c9f7-e708-faa5-a6d6-1c1b249385d9": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5307,
                    "y": 4824
                  }
                },
                "852333cb-8548-e941-b1e5-fd583bd20acd": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5428,
                    "y": 4604
                  }
                },
                "2e048706-309f-477f-ae99-14b2e21ec6b4": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5519,
                    "y": 5358
                  }
                },
                "2593e9f9-83d6-6e30-8dbe-20d55ecf603a": {
                  "uiName": "rfm",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5935,
                    "y": 5160
                  }
                },
                "08a9b8d8-cedc-3aa1-bcb0-296aaa88fc67": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5938,
                    "y": 4608
                  }
                },
                "96a02413-c135-7622-0d80-a26d2492918d": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 6172,
                    "y": 4606
                  }
                },
                "7fcafcda-4d77-23ee-4860-5a260c18d24a": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 6041,
                    "y": 5008
                  }
                },
                "575c23f1-9fb9-d633-ee53-6e888c392aed": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5696,
                    "y": 4605
                  }
                },
                "e2c729ed-8f17-427a-aee6-f7bbb338fc94": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5270,
                    "y": 5214
                  }
                },
                "897e3e9b-1b29-4115-19f7-dda98bd5637b": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 5022,
                    "y": 4737
                  }
                },
                "6eb5a450-fb65-b220-3764-ac138c090074": {
                  "uiName": "",
                  "color": "#00B1EB",
                  "coordinates": {
                    "x": 4979,
                    "y": 4604
                  }
                }
              }
            }
          },
          "publicParams": []
        }
      }
    }, {
      "id": "b26bb055-2532-1ef4-c64b-7682ac02f864",
      "operation": {
        "id": "643d8706-24db-4674-b5b4-10b5129251fc",
        "name": "Transform"
      },
      "parameters": {
        "Parameters of input Transformer": {

        }
      }
    }, {
      "id": "6416e64c-29a5-4e80-3773-e8e815082a8b",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\nfrom fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nimport math\nimport pyarrow as pa\nfrom dask.delayed import delayed\nimport dask\nfrom datetime import datetime\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report\n\n\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n    'recharge_total_cntm2',\n    'recharge_total_cntm3',\n    'm1_total_voice_usage',\n    'm1_total_data_usage',\n    'm2_total_voice_usage',\n    'm2_total_data_usage',\n    'm3_total_voice_usage',\n    'm3_total_data_usage',\n    'purchase_total_cntm1',\n    'purchase_total_cntm2',\n    'purchase_total_cntm3', \n    'm3_total_revenue_m2_total_revenue_pct_drop',\n    'm3_data_revenue_m2_data_revenue_pct_drop',\n    'm2_voice_rev_m1_voice_rev_pct_drop',\n    'm2_total_revenue_m1_total_revenue_pct_drop',\n    'm2_data_revenue_m1_data_revenue_pct_drop',\n    'm3_voice_rev_m2_voice_rev_pct_drop',\n    'm1_m2_m3_average_voice',\n    'm1_m2_m3_average_data', \n    'm1_no_of_days',\n    'm2_no_of_days',\n    'm3_no_of_days',\n    'eng_index',\n    'consecutive_inactive_days']\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n    \n    \n    \n    \nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = \"/home/tnmops/seahorse3_bkp/\"\npurchase_location = \"/home/tnmops/seahorse3_bkp/\"\nrecharge_location = \"/home/tnmops/seahorse3_bkp/\"\netl_location = \"/home/tnmops/seahorse3_bkp/\"\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location  ='/home/tnmops/seahorse3_bkp/'\nlog_file = \"/home/tnmops/seahorse3_bkp/\"\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n    \n    \n\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_march.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_jan.csv\"\n        },\n        \"recharge\": {\n            \"m1\": \"recharge_feb.csv\",\n            \"m2\": \"recharge_m1_20230206165903.csv\",\n            \"m3\": \"recharge_m2_20230206165903.csv\"\n\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_march_full_df.csv\",\n            \"m2\": \"purchase_feb_full_df.csv\",\n            \"m3\": \"purchase_jan_full_df.csv\"\n\n        },\n        \"daily_summerized\": {\n            \"m1\": \"daily_summarized_march.csv\",\n            \"m2\": \"daily_summarized_feb.csv\",\n            \"m3\": \"daily_summarized_jan.csv\",\n        \n\n        },\n        \"weekwise\": \n        {\n        \n            \"m1\": \"weekwise_feb_20230517180929.csv\",\n            \"m2\": \"weekwise_jan_20230517180929.csv\",\n            \"m3\": \"weekwise_dec_20230517180929.csv\",\n        \n        },\n        \"weekly_daily\": \n        {\n        \n            \"m1\": \"daily_weekly_avg_feb_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_jan_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_dec_20230517163429.csv\",\n        \n        },\n        \"rfm_purchase\": \n        {\n\n            \"p1\": \"purchase_m3_20230206203137.csv\",\n            \"p2\": \"purchase_m2_20230206203137.csv\",\n            \"p3\": \"purchase_m1_20230206203137.csv\",\n            \"p4\": \"purchase_feb.csv\",\n            \"p5\": \"purchase_march_20230411090513.csv\"\n\n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_jan_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_dec_20230517092908.csv\",            \n                        \n        },\n    \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n\n        \"profile\":\"profile_march_20230411085416.csv\"\n    \n\n    }\n    \n    \n    \n    \ndef status_process(dag_run_id):\n    try:\n        print(\" inside status_process\")\n        file_name_dict = get_file_names()\n        trend_path = os.path.join(ml_location,  \"trend\")\n        trend_filtered_path = os.path.join(ml_location, \"trend_filtered\")\n        trend_profie_active_inactive_trend_eda_path= os.path.join(ml_location,  \"trend_profie_active_inactive_trend_eda\")\n        rfm_path = os.path.join(ml_location, \"rfm\")\n        print(\"reading perquet\")\n        trend = dd.read_parquet(trend_path)\n        print(\"reading perquet cmpt\")\n\n        print('len of trend is ',len(trend))\n        trend_filter = trend.query(\"tot_revm1 > 0 or tot_revm2 > 0 or tot_revm3 >0 \")\n        print('len of trend_filter after filter either 3 month total rev grater that 0  is ',len(trend_filter))\n\n\n        # profile\n        dtype = {'account_type': 'float64',\n                 'current_state': 'float64',\n                 'no_of_balances': 'float64',\n                 'primary_offer_id': 'float64',\n                 'subscr_no': 'float64',\n                 'payment_type':'object'}\n        profile = dd.read_csv(os.path.join(purchase_location, file_name_dict.get(\"profile\")), dtype=dtype)\n        profile[PROFILE_DATE_ENTER_ACTIVE] = dd.to_datetime(profile[PROFILE_DATE_ENTER_ACTIVE],\n                                                                       errors='coerce')\n\n        date_threshold = datetime.strptime('2023-07-30', '%Y-%m-%d')\n\n        profile['aon'] = (date_threshold - profile[PROFILE_DATE_ENTER_ACTIVE]).dt.days\n        # trend = trend.merge(trend_filter, on='msisdn', how='left', indicator=True)\n        # trend['active_inactive'] = trend['_merge'].apply(lambda x: 'active' if x == 'both' else 'inactive',\n        #                                                  meta=('str'))\n        # trend = trend.drop(columns=['_merge'])\n\n        # trend_active_aon = trend[['msisdn', 'active_inactive']].merge(profile[['msisdn', 'aon']], on='msisdn',\n        #                                                               how='left')\n        # trend_active_aon = trend_active_aon.fillna(0)\n\n        # print(' head is going to add')\n\n        # trend_active_aon1 = trend_active_aon.head(0)\n        # print('trend_active_aon1 is' ,trend_active_aon1 )\n        # print('type of trend_active_aon1 is' ,type(trend_active_aon1 ))\n        # #trend_active_aon1=trend_active_aon1.compute()\n        # print('trend_active_aon1 is' ,trend_active_aon1 )\n        # trend_active_aon['dag_run_id']=str(dag_run_id)\n\n        # trend_active_aon.head(0).to_sql(\"trend_active_aon\", engine, if_exists=\"append\", index=False)\n        # print(' head added')\n        # @delayed\n        # def insert_partition(partition):\n        #     partition.to_sql(\"trend_active_aon\", engine, if_exists=\"append\", index=False)\n\n        # insertions = [insert_partition(partition) for partition in trend_active_aon.to_delayed()]\n        # dask.compute(*insertions)\n        print(' testing 1  ')\n\n\n        trend_active_aon_f = trend_filter.merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n                                                                      how='inner')\n\n        trend_active_aon_f = trend_active_aon_f.fillna(0)\n        trend_active_aon_f = trend_active_aon_f[trend_active_aon_f['aon'] >=90]\n        trend_active_aon_f = trend_active_aon_f[trend_active_aon_f['current_state'] ==2]\n        trend_active_aon_f = trend_active_aon_f[trend_active_aon_f['payment_type'] =='Prepaid']\n\n        print('len of trend_active_aon_f after aon,current_state and payment_type filter is ',len(trend_active_aon_f))\n        \n\n        #trend_active_aon_f_pd = trend_active_aon_f.to_pandas_df()\n\n        # create a dask dataframe from the pandas dataframe\n        #trend_active_aon_f_dd = trend_active_aon_f.compute()\n\n        profile = profile.compute()\n        trend=trend.compute()\n\n        print(' testing 2 ')\n\n        trend_profie = trend[['msisdn','trend']].merge(profile[['msisdn', 'aon','current_state','payment_type']], on='msisdn',\n                                                                      how='left')\n        \n        trend_profie[[ 'aon','current_state']]=trend_profie[[ 'aon','current_state']].fillna(0)\n        trend_profie['payment_type']=trend_profie['payment_type'].fillna('nil')\n\n\n\n       \n\n        #rend_active_aon_f_msisdn=trend_active_aon_f['msisdn'].tolist()\n\n        trend_profie_active = trend_profie[(trend_profie['payment_type'] == 'Prepaid') & (trend_profie['current_state'] == 2)]\n        trend_profie_active= trend_profie_active[trend_profie_active['msisdn'].isin(trend_filter['msisdn'].compute())]\n        trend_profie_inactive = trend_profie[~trend_profie['msisdn'].isin(trend_profie_active['msisdn'])]\n\n        trend_profie_active['active_inactive'] = 'active'\n        \n        trend_profie_inactive['active_inactive'] = 'inactive'\n   \n        trend_profie_active = dd.from_pandas(trend_profie_active, npartitions=2)\n        trend_profie_inactive = dd.from_pandas(trend_profie_inactive, npartitions=2)\n\n        trend_profie_active_inactive = dd.concat([trend_profie_active,trend_profie_inactive])\n        \n       \n        trend_profie_active_inactive['dag_run_id']=str(dag_run_id)\n\n\n       \n        \n\n        print('type(trend_profie_active_inactive',type(trend_profie_active_inactive))\n  \n#         print(' head added')\n#         @delayed\n#         def insert_partition(partition):\n#             partition.to_sql(\"APA_trend_profie_active_inactive\", engine, if_exists=\"append\", index=False)\n\n#         insertions = [insert_partition(partition) for partition in trend_profie_active_inactive.to_delayed()]\n#         dask.compute(*insertions)\n#         print(' trend_profie_active_inactive added to sql table ')\n\n        rfm = dd.read_parquet(rfm_path)\n\n        trend_profie_active_inactive_trend_rfm = trend_profie_active_inactive.merge(rfm[['msisdn', 'Segment']], on='msisdn',\n                                                                      how='left')\n        trend_profie_active_inactive_trend_rfm['Segment']=trend_profie_active_inactive_trend_rfm['Segment'].fillna('nil')\n\n        trend_profie_active_inactive_trend_rfm.to_parquet(trend_profie_active_inactive_trend_eda_path)\n        \n        trend_active_aon_f.to_parquet(trend_filtered_path)\n\n        # ic(\"the length of trend df \", len(trend))\n        # ic(\"the unique msisdn in trend df \", trend['msisdn'].nunique().compute())\n\n        # ic(\"the length of trend df  after all 3 months active \", len(trend_filter))\n        # ic(\"the unique msisdn in trend df  all 3 months active \", trend_filter['msisdn'].nunique().compute())\n        return trend_active_aon_f.compute()\n        pass\n    except Exception as e:\n        print(e)\n        \n\ndef transform(dataframe):\n    df = status_process(\"manual__2023-07-10T11:06:51\")\n    return df"
      }
    }, {
      "id": "6b8394f4-4963-700f-3ccb-e636808024dc",
      "operation": {
        "id": "643d8706-24db-4674-b5b4-10b5129251fc",
        "name": "Transform"
      },
      "parameters": {
        "Parameters of input Transformer": {

        }
      }
    }, {
      "id": "24abe326-24b1-11b0-7540-16787330d5cd",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\netl_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\n# dag_run_id = \"manual__2023-07-10T11:06:51\"    \nMSISDN_COL_NAME = 'msisdn'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\n\ndef matrix_operations(dag_run_id):\n    matrix_pack_features = pd.read_csv(\n        os.path.join(ml_location, \"pack_features_encoded.csv\"))\n    matrix_user_pack = pd.read_csv(os.path.join(ml_location, \"user_pack_matrix.csv\"))\n    msisdn_list = matrix_user_pack.pop(MSISDN_COL_NAME)\n    product_ids = matrix_user_pack.columns\n\n#     product_ids = [int(x) for x in product_ids]\n    product_ids = [float(x) for x in product_ids]\n    matrix_pack_features = matrix_pack_features[matrix_pack_features[PACK_INFO_PACK_COLUMN_NAME].isin(product_ids)]\n    product_id_ls = matrix_pack_features.pop(PACK_INFO_PACK_COLUMN_NAME)\n    pack_features_cols = matrix_pack_features.columns\n    ic(\"performing a x b\")\n    final_matrix = np.matmul(matrix_user_pack, matrix_pack_features)\n    final_matrix.columns = pack_features_cols\n    final_matrix.index = msisdn_list\n    # normilizing the matrix\n    ic(\"going to normalize \")\n    final_matrix = final_matrix.div(final_matrix.sum(axis=1), axis=0)\n    matrix_pack_features_t = matrix_pack_features.T\n    final_matrix_1 = np.matmul(final_matrix.values, matrix_pack_features_t.values)\n    final_matrix_1 = pd.DataFrame(final_matrix_1, index=msisdn_list, columns=product_id_ls)\n    ic(\"got the final matrix \")\n\n    final_matrix_1.columns = final_matrix_1.columns.astype(int)\n    print(final_matrix_1.columns)\n    final_matrix_1.to_csv(os.path.join(ml_location, \"matrix.csv\"), header=True,\n                          index=True)\n\n    return final_matrix_1\n        \n        \n        \n\n\ndef transform(dataframe):\n    df  = matrix_operations(\"manual__2023-07-10T11:06:51\")\n    return df"
      }
    }, {
      "id": "636d762f-ef80-ecb6-46d3-dedf7a439242",
      "operation": {
        "id": "643d8706-24db-4674-b5b4-10b5129251fc",
        "name": "Transform"
      },
      "parameters": {
        "Parameters of input Transformer": {

        }
      }
    }, {
      "id": "bbe00a0b-37b1-ce58-2702-875a0b3bb296",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "import pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom urllib.parse import quote  \n\n\n\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\nfrom sqlalchemy.orm import relationship\nimport datetime\nfrom sqlalchemy.dialects.mysql import LONGTEXT\n\n\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nimport sklearn\nimport json\nimport dask.dataframe as dd\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\n# import configuration.config as cfg\n# import configuration.features as f\nimport traceback\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom pathlib import Path\nimport requests\n# from sql_app.repositories import AssociationRepo\n\n# config = cfg.Config().to_json()\n# features = f.Features().to_json()\n\n\n\n\nCLUSTER_NEEDED_COLUMNS=['recharge_total_cntm1',\n'recharge_total_cntm2',\n'recharge_total_cntm3',\n'm1_total_voice_usage',\n'm1_total_data_usage',\n'm2_total_voice_usage',\n'm2_total_data_usage',\n'm3_total_voice_usage',\n'm3_total_data_usage',\n'purchase_total_cntm1',\n'purchase_total_cntm2',\n'purchase_total_cntm3', \n'm3_total_revenue_m2_total_revenue_pct_drop',\n'm3_data_revenue_m2_data_revenue_pct_drop',\n'm2_voice_rev_m1_voice_rev_pct_drop',\n'm2_total_revenue_m1_total_revenue_pct_drop',\n'm2_data_revenue_m1_data_revenue_pct_drop',\n'm3_voice_rev_m2_voice_rev_pct_drop',\n'm1_m2_m3_average_voice',\n'm1_m2_m3_average_data', \n'm1_no_of_days',\n'm2_no_of_days',\n'm3_no_of_days',\n'eng_index',\n'consecutive_inactive_days']\n\nPROFILE_DATE_ENTER_ACTIVE = \"date_enter_active\"\nPACK_INFO_VALIDITY_NAME = \"validity_in_days\"\nMSISDN_COL_NAME = 'msisdn'\nTRANSACTION_CATEG_FEATURES = ['cdr_date']\nTRANSACTION_CONTI_FEATURES = ['msisdn', 'product_id', 'total_cnt']\nTRANSACTION_FEATURES = TRANSACTION_CATEG_FEATURES + TRANSACTION_CONTI_FEATURES\nRECHARGE_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                       'total_amt': 'float32'}    # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_DTYPES = {'msisdn': 'float64',\n                      'total_revenue': 'float32'} # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PURCHASE_DATE_NAME = 'cdr_date'   # to run rfm based on recharge and purchase  \nDAILY_TRANSACTION_PURCHASE_DATE_NAME ='fct_dt' # to run rfm based on total_rev\nRECHARGE_TRANSACTION_PRICE_COL_NAME = 'total_amt' # to run rfm based on recharge and purchase\nDAILY_TRANSACTION_PRICE_COL_NAME = 'total_revenue'  # to run rfm based on total_rev\n\nTRANSACTION_COUNT_COL_NAME = 'total_cnt'\nTRANSACTION_NEEDED_COL = ['msisdn', 'total_cnt']\nTRANSACTION_PRODUCT_NAME = 'product_id'\n\nPACK_CONTI_FEATURES = []\nPACK_CATEG_FEATURES = ['product_type', 'validity_in_days', 'unit_in_mb', 'price', 'product_id']\nPACK_INFO_DTYPES = {'validity_in_days': 'int', 'unit_in_mb': 'float32'}\nALL_PACK_FEATURES = PACK_CONTI_FEATURES + PACK_CATEG_FEATURES\nPACK_INFO_CATEGORY = 'product_type'\nPACK_INFO_SUB_CATEGORY = 'bundle_type'\nPACK_INFO_PACK_COLUMN_NAME = 'product_id'\nPACK_INFO_PACK_PRICE_COLUMN_NAME = 'price'\nCUSTOMER_CATEG_FEATURES = []\nCUSTOMER_CONTI_FEATURES = []\nCUSTOMER_CATEG_NEEDED_COL = ['msisdn', 'total_voice_usage', 'total_data_usage']\n# 0 index for inbundle 1 index for outbundled  2 index for total\nCUSTOMER_VOICE_COL_NAME = ['total_voice_usage']\nCUSTOMER_VOICE_REVENUE = [\"\", \"\", \"og_total_voice_revenue\"]\nCUSTOMER_DATA_COL_NAME = ['total_data_usage']\nCUSTOMER_TOTAL_REVENUE = ['total_revenue']\nCUSTOMER_DATA_REVENUE = [\"\", \"\", \"total_data_rev\"]\nCUSTOMER_NEEDED_COLUMN = [\n'onnet_revenue',  'onnet_usage', 'onnet_voice_count', \n'offnet_revenue', 'offnet_usage', 'offnet_voice_count',  \n'idd_revenue', 'idd_usage', 'idd_voice_count',\n'voice_rmg_revenue', 'voice_rmg_usage', 'voice_rmg_count', \n'data_rmg_revenue',  'data_rmg_usage',  \n'data_revenue', 'data_usage', 'sms_revenue', 'sms_usage',  'sms_idd_revenue', \n'sms_idd_usage', 'magik_voice_amount', 'rbt_subscription_rev', 'emergency_credit_rev', \n'package_revenue', 'voice_rev', 'sms_rev', 'onn_rev', 'off_rev', 'total_data_rev', \n'vas_rev', 'vas_rev_others', 'total_revenue', 'total_voice_count', 'total_voice_duration', \n'total_mainaccount_data_usage', 'total_sms_count', 'total_package_count', 'total_other_vas_count', \n'total_voice_usage', 'total_data_usage', 'total_sms_usage'\n]\n\n\n\nCUSTOMER_FEATURES = CUSTOMER_CATEG_FEATURES + CUSTOMER_CONTI_FEATURES\n\nCUSTOMER_DTYPES = {'total_revenue': 'float32'}\nCUSTOMER_DROP_COLUMNS=['msisdn','total_revenue', 'voice_rev', 'data_revenue']\nCUSTOMER_USAGE_COLUMNS=['msisdn','total_data_usage', 'total_voice_usage']\n\n# RECHARGE_DTYPES = {'total_recharge_cnt': 'float32', 'total_rechargeamount': 'float32'}\n# RECHARGE_COUNT_COL_NAME = 'total_recharge_cnt'\n# RECHARGE_NEEDED_COL = ['m'm3_total_revenue', 'm3_voice_rev', 'm3_data_revenue'sisdn', 'total_recharge_cnt']\n\nRECHARGE_DTYPES = {'total_cnt': 'float32', 'total_amt': 'float32'}\nRECHARGE_COUNT_COL_NAME = 'total_cnt'\nRECHARGE_NEEDED_COL = ['msisdn', 'total_cnt']\n\nLABEL1 = 'downtrend'\nLABEL2 = 'uptrend'\nLABEL3 = 'zigzag'\nLABEL4 = 'flat'\n\ndef to_json(self):\n    return {\n        'purchase_features': self.TRANSACTION_FEATURES,\n        'usage_features': self.CUSTOMER_FEATURES,\n        'pack_info_features': self.ALL_PACK_FEATURES,\n        'pack_info_dtypes': self.PACK_INFO_DTYPES,\n        'usage_dtypes': self.CUSTOMER_DTYPES\n\n    }\n\n\nimport logging\nimport os\n\ndef get_file_names():\n    return {\n        \"usage\": {\n            \"m1\": \"usage_jan.csv\",\n            \"m2\": \"usage_feb.csv\",\n            \"m3\": \"usage_march.csv\"},\n\n        \"recharge\": {\n            \"m1\": \"recharge_july_20230411084648.csv\",\n            \"m2\": \"recharge_june_20230411084648.csv\",\n            \"m3\": \"recharge_may_20230411084648.csv\"\n        },\n        \"purchase\": {\n            \"m1\": \"purchase_july_20230411090513.csv\",\n            \"m2\": \"purchase_june_20230411090513.csv\",\n            \"m3\": \"purchase_may_20230411090513.csv\"\n        },\n         \"daily_summerized\": {\n            \"m1\": \"daily_summarized_july_20230517122646.csv\",\n            \"m2\": \"daily_summarized_june_20230517122646.csv\",\n            \"m3\": \"daily_summarized_may_20230517122646.csv\",  \n          \n        },\n        \"weekwise\": \n        {\n          \n           \"m1\": \"weekwise_july_20230517180929.csv\",\n           \"m2\": \"weekwise_june_20230517180929.csv\",\n           \"m3\": \"weekwise_may_20230517180929.csv\", \n          \n        },\n        \"weekly_daily\": \n        {\n          \n            \"m1\": \"daily_weekly_avg_july_20230517163429.csv\",\n            \"m2\": \"daily_weekly_avg_june_20230517163429.csv\",\n            \"m3\": \"daily_weekly_avg_may_20230517163429.csv\" ,  \n          \n        },\n        \"rfm_purchase\": \n        {\n  \n            \"p1\": \"purchase_march_20230411090513.csv\",\n            \"p2\": \"purchase_april_20230411090513.csv\",\n            \"p3\": \"purchase_may_20230411090513.csv\",\n            \"p4\": \"purchase_june_20230411090513.csv\",\n            \"p5\": \"purchase_july_20230411090513.csv\"\n  \n        },\n         \"campaign_data\": \n        {\n            \"c3\": \"campaign_detailed_fct_july_20230517092908.csv\",\n            \"c4\": \"campaign_detailed_fct_june_20230517092908.csv\" ,            \n                      \n        },\n      \n        \"pack\":\n        {\n            \"pack\":\"pack_info.csv\"\n        },\n        \"profile\":\"profile_july_20230801055632.csv\"\n      \n    }\n\n\n\nrule_converter_url = 'http://10.1.31.212:8090/requestconvertor/v1/convert/'\nsegement_names = ['trend', 'rfm']\nsource_purchase_and_etl_location = '/home/tnmops/seahorse3_bkp/'\npurchase_location = '/home/tnmops/seahorse3_bkp/'\nrecharge_location = '/home/tnmops/seahorse3_bkp/'\netl_location = '/home/tnmops/seahorse3_bkp/'\npack_info_location = '/home/tnmops/seahorse3_bkp/'\nml_location = '/home/tnmops/seahorse3_bkp/'\nlog_file = '/home/tnmops/seahorse3_bkp/'\nderived_pack_info_location='/home/tnmops/seahorse3_bkp/'\n\nusage_dtypes = {'total_data_revenue': 'float32', 'og_total_voice_revenue': 'float32'}\n\nusage_no_months = ['m1', 'm2', 'm3']\nrecharge_no_months = ['m1', 'm2', 'm3']\npurchase_no_months = [\"m1\",\"m2\", \"m3\"]\n#purchase_no_months_seg = ['m1',\"m2\", \"m3\"]\npurchase_no_months_seg = ['m1',\"m2\", \"m3\"]\nthreshold = 0.50\nnot_needed_rfm_segment = ['Lost']\n\npack_cols = ['product_id','product_type']\npurchase_no_months_for_next_purchase= ['p1', 'p2', 'p3', 'p4','p5']\n\ncampaign_df= ['c4','c3']\ncampaign_usage= ['m4','m3']\n\ndef to_json(self):\n    return {\n        'purchase_location': self.purchase_location,\n        'usage_location': self.etl_location,\n        'pack_info_location': self.pack_info_location,\n        'ml_location': self.ml_location,\n        'recharge_location':self.recharge_location\n    }\n\nmsisdn_name = MSISDN_COL_NAME\n    \n\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\n\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URL,\n)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        return db\n    finally:\n        db.close()\n\nclass SegmentInformation(Base):\n    __tablename__ = \"APA_segment_information_new\"\n    id = Column(Integer, primary_key=True, index=True)\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\n    end_date = Column(DateTime)\n    current_product = Column(String(80), nullable=True, unique=False)\n    current_products_names = Column(String(200), nullable=True, unique=False)\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\n    predicted_arpu = Column(Integer, nullable=True)\n    current_arpu = Column(Integer, nullable=True)\n    segment_length = Column(String(80), nullable=True, unique=False)\n    rule = Column(LONGTEXT, nullable=True)\n    actual_rule = Column(LONGTEXT, nullable=True)\n    uplift_percent = Column(Float(precision=2), nullable=True)\n    incremental_revenue = Column(Float(precision=2), nullable=True)\n    campaign_type = Column(String(80), nullable=True, unique=False)\n    campaign_name = Column(String(80), nullable=True, unique=False)\n    action_key = Column(String(80), nullable=True, unique=False)\n    robox_id = Column(String(80), nullable=True, unique=False)\n    dag_run_id = Column(String(80), nullable=True, unique=False)\n    samples = Column(Integer, nullable=False)\n    segment_name = Column(String(80), nullable=True, unique=False)\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\n    customer_status = Column(String(80), nullable=True, unique=False)\n    query = Column(LONGTEXT, nullable=True, unique=False)\n    cluster_no = Column(Integer, nullable=True)\n    confidence = Column(Float(precision=2), nullable=True)\n    recommendation_type = Column(String(80), nullable=True, unique=False)\n    cluster_description = Column(LONGTEXT, nullable=True)\n    actual_target_count = Column(Integer, nullable=True)\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\n    total_revenue= Column(Integer, nullable=True)\n    uplift_revenue=Column(Integer, nullable=True)\n\n    def __repr__(self):\n        return 'SegmentInformation(name=%s)' % self.name\n\n\n\nclass SegementInfo(BaseModel):\n    end_date: Optional[str] = None\n    dag_run_id: Optional[str] = None\n    current_product: Optional[str] = None\n    current_products_names: Optional[str] = None\n    recommended_product_id: Optional[str] = None\n    recommended_product_name: Optional[str] = None\n    predicted_arpu: Optional[int] = None\n    current_arpu: Optional[int] = None\n    segment_length: Optional[str] = None\n    rule: Optional[str] = None\n    actual_rule: Optional[str] = None\n    uplift_percent: Optional[float] = None\n    incremental_revenue: Optional[float] = None\n    campaign_type: Optional[str] = None\n    campaign_name: Optional[str] = None\n    action_key: Optional[str] = None\n    robox_id: Optional[str] = None\n    samples: Optional[int] = None\n    segment_name: Optional[str] = None\n    current_ARPU_band: Optional[str] = None\n    current_revenue_impact: Optional[str] = None\n    customer_status: Optional[str] = None\n    query: Optional[str] = None\n    cluster_no: Optional[int] = None\n    confidence: Optional[float] = None\n    recommendation_type: Optional[str] = None\n    cluster_description: Optional[str] = None\n    actual_target_count: Optional[str] = None\n    top_purchased_day_1: Optional[str] = None\n    top_purchased_day_2: Optional[str] = None\n    top_purchased_day_3: Optional[str] = None\n    next_purchase_date_range: Optional[str] = None\n    campaign_response_percentage: Optional[str] = None\n    total_revenue: Optional[int] = None\n    uplift_revenue: Optional[int] = None\n\n\nclass SegementRepo:\n    def create(db: Session, segement: SegementInfo):\n        db_item = SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\n                                            campaign_type=segement.campaign_type,\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\n                                            rule=segement.rule, samples=segement.samples,\n                                            campaign_name=segement.campaign_name,\n                                            recommended_product_id=segement.recommended_product_id,\n                                            recommended_product_name=segement.recommended_product_name,\n                                            current_product=segement.current_product,\n                                            current_products_names=segement.current_products_names,\n                                            segment_length=segement.segment_length,\n                                            current_ARPU_band=segement.current_ARPU_band,\n                                            current_revenue_impact=segement.current_revenue_impact,\n                                            customer_status=segement.customer_status,\n                                            segment_name=segement.segment_name, query=segement.query,\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\n                                            recommendation_type=segement.recommendation_type,\n                                            cluster_description=segement.cluster_description,\n                                            actual_target_count=segement.actual_target_count,\n                                            top_purchased_day_1=segement.top_purchased_day_1,\n                                            top_purchased_day_2=segement.top_purchased_day_2,\n                                            top_purchased_day_3=segement.top_purchased_day_3,\n                                            next_purchase_date_range=segement.next_purchase_date_range,\n                                            campaign_response_percentage=segement.campaign_response_percentage,\n                                            total_revenue=segement.total_revenue,\n                                            uplift_revenue=segement.uplift_revenue,\n                                            )\n        db.add(db_item)\n        db.commit()\n        db.refresh(db_item)\n        return db_item\n\n    def findByAutoPilotIdAndClusterNo(db: Session, _id, cluster_no):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.cluster_no == cluster_no) \\\n            .all()\n\n    def findByAutoPilotIdAndSegementName(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).first()\n\n    def findByAutoPilotIdAndSegementNameAll(db: Session, _id, segement_name, cluster_number):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name) \\\n            .filter(SegmentInformation.cluster_no == cluster_number).all()\n    \n    def findByAutoPilotIdAndSegementNamewithoutcluster(db: Session, _id, segement_name):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.segment_name == segement_name).all()\n            \n\n\n    def findByAutoPilotId(db: Session, _id):\n        return db.query(SegmentInformation).filter(SegmentInformation.dag_run_id == _id).all()\n\n    def findByAutoPilotIdRecommendedId(db: Session, _id,recommended_product_id):\n        return db.query(SegmentInformation) \\\n            .filter(SegmentInformation.dag_run_id == _id) \\\n            .filter(SegmentInformation.recommended_product_id == recommended_product_id).all()\n\n    def deleteById(db: Session, _ids):\n        for id in _ids:\n            db.query(SegmentInformation).filter(SegmentInformation.id == id).delete()\n            db.commit()\n\n    def update(db: Session, item_data):\n        updated_item = db.merge(item_data)\n        db.commit()\n        return updated_item\n\ndef form_data(p2, df, anti_conci):\n    try:\n        product_id = PACK_INFO_PACK_COLUMN_NAME\n        purchase = p2[p2[product_id].isin(anti_conci)]\n        pgp = purchase.copy()\n        # pgp = purchase.groupby(['msisdn', 'product_id']).agg({f.Features.PA: \"sum\"}).reset_index()\n\n        anti_df = pgp[pgp[product_id].isin(anti_conci[:-1])]\n        conci_df = pgp[pgp[product_id] == anti_conci[-1]]\n\n        anti_df_msisdn = anti_df[~anti_df['msisdn'].isin(conci_df['msisdn'].values)]['msisdn'].unique()\n        conci_df_msisdn = conci_df[~conci_df['msisdn'].isin(anti_df['msisdn'].values)]['msisdn'].unique()\n        anti_data = df[df['msisdn'].isin(anti_df_msisdn)]\n        conci_data = df[df['msisdn'].isin(conci_df_msisdn)]\n        print(f\"the length of anti data ios {len(anti_data)} and unique is {anti_data['msisdn'].nunique()}\")\n        print(f\"the length of conci data ios {len(conci_data)} and unique is {conci_data['msisdn'].nunique()}\")\n        anti_data['label'] = 1\n        conci_data['label'] = 0\n        data = pd.concat([anti_data, conci_data], axis=0)\n        print(\"label counts\", data['label'].value_counts())\n        return data\n    except Exception as e:\n        print(\"the error occoured in form_data\", e)\n        raise ValueError(e)\n\n\n\nclass DecisionTreeConverter(object):\n\n    def __init__(self, my_tree=None, features=None, class_names=None, df=None):\n        self.my_tree = my_tree\n        self.features = features\n        self.class_names = class_names\n        self.df = df\n        self.json = None\n\n        self.json_string = \"\"\n\n        # self.recursion(self.my_tree.tree_, 0)\n\n        self.recurse(self.my_tree.tree_, 0)\n\n    def node_to_str(self, tree, node_id, criterion):\n        if True:\n            criterion = \"impurity\"\n\n        value = tree.value[node_id]\n        if tree.n_outputs == 1:\n            value = value[0, :]\n\n        jsonValue = ', '.join([str(x) for x in value])\n\n        if tree.children_left[node_id] == sklearn.tree._tree.TREE_LEAF:\n            l = 1\n            try:\n\n                probablity = np.round(100.0 * value[l] / np.sum(value), 2)\n            except:\n                probablity = np.round(100.0 * value[0] / np.sum(value), 2)\n            return '\"id\": \"%s\", \"criterion\": \"%s\", \"impurity\": \"%s\", \"samples\": \"%s\",\"recomentedPackProbablity\":%s, ' \\\n                   '\"value\": [%s]' \\\n                   % (node_id,\n                      criterion,\n                      tree.impurity[node_id],\n                      tree.n_node_samples[node_id],\n                      probablity,\n                      jsonValue)\n        else:\n\n            if self.features is not None:\n                feature = self.features[tree.feature[node_id]]\n            else:\n                feature = tree.feature[node_id]\n\n            if \"=\" in feature:\n                ruleType = \"=\"\n                ruleValue = \"false\"\n            else:\n                ruleType = \"<=\"\n                ruleValue = \"%.2f\" % tree.threshold[node_id]\n\n            return '\"id\": \"%s\", \"rule\": \"%s %s %s\", \"%s\": \"%s\", \"samples\": \"%s\"' \\\n                   % (node_id,\n                      feature,\n                      ruleType,\n                      ruleValue,\n                      criterion,\n                      tree.impurity[node_id],\n                      tree.n_node_samples[node_id])\n\n    def recurse(self, tree, node_id, criterion='impurity', parent=None, depth=0):\n        tabs = \"  \" * depth\n        self.json_string = \"\"\n\n        left_child = tree.children_left[node_id]\n        right_child = tree.children_right[node_id]\n\n        self.json_string = self.json_string + \"\\n\" + \\\n                           tabs + \"{\\n\" + \\\n                           tabs + \"  \" + self.node_to_str(tree, node_id, criterion)\n\n        if left_child != sklearn.tree._tree.TREE_LEAF:\n            self.json_string = self.json_string + \",\\n\" + \\\n                               tabs + '  \"left\": ' + \\\n                               self.recurse(tree, left_child, criterion=criterion, parent=node_id,\n                                            depth=depth + 1) + \",\\n\" + \\\n                               tabs + '  \"right\": ' + \\\n                               self.recurse(tree,\n                                            right_child,\n                                            criterion=criterion,\n                                            parent=node_id,\n                                            depth=depth + 1)\n\n        self.json_string = self.json_string + tabs + \"\\n\" + \\\n                           tabs + \"}\"\n\n        return self.json_string\n\n    def recursion(self, tree, node_id, parent=None, depth=0, location=None):\n        tabs = \" \" * depth\n        self.json = \"\"\n        left_child = tree.children_left[node_id]\n        right_child = tree.children_right[node_id]\n        self.json = self.json + tabs + \"{\" + tabs + \" \" + self.get_node_to_string(tree, node_id, location)\n        print(f\"the json got in recursion is {self.json}\")\n        if left_child != sklearn.tree._tree.TREE_LEAF:\n            self.json = self.json + \",\" + tabs + '\"left\": ' + self.recursion(tree, left_child, node_id, depth + 1,\n                                                                             \"left\") + \",\" + \\\n                        tabs + '\"right\": ' + self.recursion(tree, right_child, node_id, depth + 1, \"right\")\n            print(\"json formed inside resursion when not tree leaf  \", self.json)\n        self.json = self.json + tabs + tabs + \"}\"\n        print(\"json formed inside resursion when  tree leaf  \", self.json)\n        return self.json\n\n    def get_node_to_string(self, tree, node_id, location):\n        value = tree.value[node_id]\n        print(f\"the value of the node  {node_id}  is -- {value}\")\n        if tree.n_outputs == 1:\n            value = value[0, :]\n            print(f\" the type of value is {type(value)}\")\n        json_val = \", \".join([str(x) for x in value])\n\n        if tree.children_left[node_id] == sklearn.tree._tree.TREE_LEAF:\n            # l = np.argmax(value)\n            l = 1\n            try:\n\n                probablity = np.round(100.0 * value[l] / np.sum(value), 2)\n            except:\n                probablity = np.round(100.0 * value[0] / np.sum(value), 2)\n\n            print(f\"the left child is a leaf node \")\n            return_val = f' \"id\": {node_id}, \"class\":\"{self.class_names[l]}\" ,\"samples\":{tree.n_node_samples[node_id]} , \"recomentedPackProbablity\": {probablity}'\n            print(f\"the return value of convert tree to json when left child is a leaf node is {return_val}\")\n            return return_val\n\n        else:\n            if self.features is not None:\n                print(f\" the features are  {self.features} ,   the node is is {node_id}\")\n                print(location)\n\n                feature = self.features[tree.feature[node_id]]\n            else:\n                print(\"no features list given \")\n\n            rule_val = \"%.2f\" % tree.threshold[node_id]\n\n            if location is not None and location == 'left':\n                minimum_value = self.df[feature].min()\n                if minimum_value == 0:\n                    operator = f\"<= {rule_val}\"\n                else:\n                    operator = f'between {round(float(minimum_value), 2)} and {round(float(rule_val), 2)} '\n\n            else:\n\n                maximum_value = self.df[feature].max()\n                operator = f'between {round(float(rule_val), 2)} and {round(float(maximum_value), 2)} '\n\n            # no need of samples and values\n\n            return_val = f' \"id\":{node_id}, \"rule\": \"{feature} {operator} \" '\n            print(f\"the return value of convert tree to json when left child is a  not leaf node is {return_val}\")\n            return return_val\n\n    def get_json(self):\n        if self.json_string is not None:\n            return self.json_string\n\n\n\ndef replace_rule(rule_condition, columns_for_dummies):\n    try:\n        if rule_condition is None:\n            return rule_condition\n        name = rule_condition.split(\"<=\")[0]\n        if name is None or len(name) == 0:\n            return rule_condition\n\n        split_name = name.split(\":\")\n        key = split_name[0]\n        if key not in columns_for_dummies:\n            return rule_condition\n        value = split_name[1]\n        value = value.strip()\n        rule_m = f\"{key} == '{round(float(value), 2)}' \"\n        return rule_m\n\n    except Exception as e:\n        print(\"the error in replace rule is \", e)\n        return rule_condition\n\n\ndef pruneTree(root, columns_for_dummies):\n    if root.get('recomentedPackProbablity') is not None and root.get('recomentedPackProbablity') < 50:\n        return None\n\n    if root.get('left') is not None:\n        root['rule'] = replace_rule(root.get('rule'), columns_for_dummies)\n        root['left'] = pruneTree(root.get('left'), columns_for_dummies)\n    if root.get('right') is not None:\n        root['rule'] = replace_rule(root.get('rule'), columns_for_dummies)\n        root['right'] = pruneTree(root.get('right'), columns_for_dummies)\n    if root.get('left') is not None or root.get('right') is not None or root.get(\n            'recomentedPackProbablity') is not None and root.get('recomentedPackProbablity') >= 50:\n        # samples =samples +int(root.get('samples'))\n        return root\n    else:\n        return None\n\n\ndef generate_boundries(features, data_json, tree):\n    def add_conditions(feature, count):\n        rule_json = {}\n        rule_json['id'] = -1\n\n        min_max = data_json.get(feature)\n        if min_max.get('min') == 0:\n            rule = f\"{feature} <= {round(float(min_max.get('max')), 2)}\"\n\n\n        else:\n            rule = f\"{feature} between {round(float(min_max.get('min')), 2)} and {round(float(min_max.get('max')), 2)}\"\n\n        rule_json['rule'] = rule\n        if count < len(features) - 1:\n            rule_json['left'] = add_conditions(features[count + 1], count + 1)\n        else:\n            rule_json['left'] = tree\n        return rule_json\n\n    try:\n        print(\"inside generate_boundries\")\n        return add_conditions(features[0], 0)\n\n    except Exception as e:\n        print(f\"error occoured in generating boundries {e}\")\n\n    \n    \n    \ndef negate(rule):\n    mapper = ((\"<=\", \">\"), (\">=\", \"<\"), (\">\", \"<=\"), (\"<\", \">=\"), (\"==\", '!='))\n    for operator, negated in mapper:\n        if operator in rule:\n            return rule.replace(operator, negated)\n    return \"not (\" + rule + \")\"  # Default when operator not recognised\n\n\ndef nodeToLeafPaths(node):\n    if not node:  # base case: nothing in this direction\n        return\n    rule = node.get('rule')\n    if rule is None:  # base case: a leaf with a value\n        # value_list.append(node.get('value'))\n        yield [], int(node.get('samples'))  # empty path\n        return\n\n    negated_rule = negate(rule)\n    for path, value in nodeToLeafPaths(node.get('left')):\n        # print(f\"the left path {path} and value {value} \")\n        if 'between' in rule:\n            between_split = rule.split('between')\n            key = between_split[0].strip()\n            value_temp = between_split[1].strip()\n            ant_split = value_temp.split(\"and\")\n            value1 = ant_split[0].strip()\n            value2 = ant_split[1].strip()\n\n            rule = f\"{key} > {round(float(value1), 2)} and {key} <= {round(float(value2), 2)}\"\n\n        yield [rule, *path], value  # Extend path with current rule\n    for path, value in nodeToLeafPaths(node.get('right')):\n        yield [negated_rule, *path], value\n\n\ndef rootToLeafConjugations(root):\n    print('inside rootToLeafConjugations ')\n    final_dic = {}\n    for path, value in nodeToLeafPaths(root):\n        final_dic[\" and \".join(path)] = value\n        # print({\" AND \".join(path):value  for path ,value in nodeToLeafPaths(root) })\n    return final_dic\n\n\ndef get_prod_diff(product_id_1, product_id_2, packinfo_df):\n    print('packinfo_df.columns', packinfo_df.columns)\n    diff = 0\n    try:\n        if isinstance(product_id_1, str):\n            split = product_id_1.split(\"|\")\n            print('split is ', split)\n            split_length = len(split)\n            if split_length > 1:\n                max = 0\n                for i in range(split_length):\n                    product = int(float(split[i]))\n                    print('product is ', product)\n                    price = packinfo_df[packinfo_df['product_id'] == product].iloc[0][\"price\"]\n                    print('price is', price)\n                    if price > max:\n                        max = price\n\n                product_id_1 = packinfo_df[packinfo_df['price'] == max].iloc[0][\"product_id\"]\n                print('product_id_1 is', product_id_1)\n\n            else:\n                product_id_1 = int(split[0])\n\n        if packinfo_df is not None:\n            print('product_id_1 is ', product_id_1)\n            print('product_id_2 is ', product_id_2)\n            #print(\"packinfo_df['product_id'].values\", packinfo_df['product_id'].values)\n            if product_id_1 in packinfo_df['product_id'].values:\n                print('product_id_1 ture')\n\n            print('packinfo_df.columns', packinfo_df.columns)\n            if product_id_2 in packinfo_df['product_id'].values:\n                print('product_id_2 ture')\n\n            else:\n                print('product_id_2 fasle')\n\n            if ((product_id_1 in packinfo_df['product_id'].values) and (\n                    product_id_2 in packinfo_df['product_id'].values)):\n\n                print('going to calcualte the diff ')\n\n                diff = abs(int(float(packinfo_df[packinfo_df['product_id'] == product_id_1].iloc[0][\"price\"]) -\n                               float(packinfo_df[packinfo_df['product_id'] == product_id_2].iloc[0][\"price\"])))\n\n                print(\"the difference is \", diff)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in get_prod_diff \", e)\n\n    return diff\n\n\n\ndef make_request(body):\n    rule_main = None\n    try:\n        url = rule_converter_url\n        x = requests.post(url, json=body)\n        response_json = x.json()\n        if response_json.get('respCode') is None or response_json.get('respCode') != 'SUCCESS':\n            raise ValueError(\"rule engine gave error respose \" + x.text)\n\n        return json.dumps(response_json.get('guiRequest'))\n\n\n    except Exception as e:\n        print(\"error occoured in http requerst\", e)\n        raise RuntimeError(e)\n\n\ndef add_clusters_rules(features, features_val, tree):\n    def add_conditions(feature, count):\n        rule_json = {}\n        rule_json['id'] = -1\n\n        rule = f\"{features[count]} == {features_val[count]}\"\n        rule_json['rule'] = rule\n        if count < len(features) - 1:\n            rule_json['left'] = add_conditions(features[count + 1], count + 1)\n        else:\n            rule_json['left'] = tree\n        return rule_json\n\n    try:\n        print(\"inside generate_boundries\")\n        return add_conditions(features[0], 0)\n\n    except Exception as e:\n        print(f\"error occoured in generating boundries {e}\")\n\n\ndef GetCurrentPackprice(product_id, packinfo_df):\n    price = 0\n    \n    try:\n        print('product_id in GetCurrentPackprice is ',product_id)\n        print('type of product_id in GetCurrentPackprice is ',type(product_id))\n        if (packinfo_df is not None):\n            if isinstance(product_id, str):\n                split = product_id.split(\"|\")\n                \n                print('split in GetCurrentPackprice is ', split)\n                split_length = len(split)\n                if split_length > 1:\n                    highest = 0\n                    for i in range(split_length):\n                        product = int(float(split[i]))\n                        print('product is ', product)\n                        price = packinfo_df[packinfo_df['product_id'] == product].iloc[0][\"price\"]\n                        print('price is', price)\n                        if price > highest:\n                            highest = price\n                \n                    print(\"the price  of \", product_id, \" is \", price)\n                \n                else:\n                    \n                    product_id = int(split[0])\n                    \n                    price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\n                    highest = price\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackprice \", e)\n\n    return highest\n    \n    \ndef getpackprice(product_id, packinfo_df):\n    price = 0\n    try:\n        if (packinfo_df is not None):\n            if (product_id in packinfo_df['product_id'].values):\n\n                price = packinfo_df[packinfo_df['product_id'] == product_id].iloc[0][\"price\"]\n                print(\"the price  of \", product_id, \" is \", price)\n            else:\n                print(\"product_id is not in dataframe  \")\n        else:\n            print(\"some problem with the  dataframe  \")\n\n    except Exception as e:\n        print(\"error occurred in getpackprice \", e)\n\n    return price\n    \ndef train_model(data):\n    def run_decision_tree(X_train, X_test, y_train, y_test):\n        clf = DecisionTreeClassifier(max_leaf_nodes=12)\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        print('Accuracy run_decision_tree: ', accuracy_score(y_test, y_pred))\n        #print(\"confusion matrix run_decision_tree\", confusion_matrix(y_test, y_pred))\n        print(classification_report(y_test, y_pred))\n        return clf\n\n    X = data.drop(['msisdn', 'label'], axis=1)\n    y = data['label']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    print(f\"X_train.shape = {X_train.shape}, X_test.shape =  {X_test.shape}\")\n    # sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\n    sel = SelectFromModel(DecisionTreeClassifier(max_leaf_nodes=12))\n    sel.fit(X_train, y_train)\n    sel.get_support()\n    features = X_train.columns[sel.get_support()]\n    print(\"the feature selection features are \", features)\n    X_train_rfc = sel.transform(X_train)\n    X_test_rfc = sel.transform(X_test)\n    clf1 = run_decision_tree(X_train_rfc, X_test_rfc, y_train, y_test)\n    features_importance = list(clf1.feature_importances_)\n    dic = {features[i]: features_importance[i] for i in range(len(features))}\n    t = None\n    try:\n        t = pd.read_csv('temp.csv')\n        tt = t.append(dic, ignore_index=True)\n    except:\n        t = pd.DataFrame(columns=list(X_train.columns))\n        tt = t.append(dic, ignore_index=True)\n\n    print(tt.head(5))\n    tt.to_csv('temp.csv', header=True, index=False)\n    return clf1, features\n\n\n\n\n\n\nclass RuleGenerator(object):\n    def __init__(self, df, dag_run_id, cluster_name, cluster_number, purchase_filtered, pack_info):\n\n        self.df = df\n        self.dag_run_id = dag_run_id\n        self.cluster_name = cluster_name\n        self.cluster_name = cluster_number\n        self.purchase = purchase_filtered\n        self.usage_filter2 = None\n        self.pack_info = pack_info\n        self.op = None\n        pass\n\n    def generate_rules(self, segementss, usage):\n        segment_list = []\n        dftnm_ls = []\n        data_dict_query = {}\n        \n        # print(type(segementss))\n        # print(segements)\n        for segements in segementss:\n            if segements.recommended_product_id is None:\n                continue\n            usage_filter1 = usage[usage['msisdn'].isin(self.df['msisdn'])]\n            # usage_filter1 = usage_filter1.compute()\n            conci = int(float(segements.recommended_product_id))\n            anti = [int(float(x)) for x in segements.current_product.split(\"|\")]\n            anti.append(conci)\n            df = form_data(p2=self.purchase, df=usage_filter1, anti_conci=anti)\n\n            print(\"df columns ==>\",df.columns)\n            self.usage_filter2 = usage_filter1.merge(self.df[['msisdn', 'trend']], on='msisdn', how='inner')\n            value_counts = df['label'].value_counts()\n            if 0 in value_counts.index and 1 in value_counts.index:\n                if value_counts.loc[0] >= 10 and value_counts.loc[1] >= 10:\n                    print(\"Both values have at least two entries\")\n                    # path_temp = os.path.join(cfg.Config.ml_location, self.dag_run_id, \"model\")\n                    # Path(path_temp).mkdir(parents=True, exist_ok=True)\n                    # df.to_csv(os.path.join(path_temp, segements.segment_name + \".csv\"), header=True, index=False)\n                    # ic(\"outputerrrrddddd\")\n                else:\n                    print(\"At least one value doesn't have two entries\")\n                    continue\n            else:\n                print(\"Both values are not present in the column\")\n                continue\n\n            clf1, features = train_model(df)\n           \n            print('features -------------------', features)\n\n            \n            # for BTC purpose end\n            \n            # features = list(features)\n            # print('features is ', features)\n            # print('type features is ', type(features))\n            # features.append('m1_total_revenue')\n            # features = list(set(features))\n            decision_tree_obj = DecisionTreeConverter(clf1, features, ['differentpack', 'whatsappPack'],\n                                                          df[df['label'] == 1])\n            treetojson = decision_tree_obj.get_json()\n            #print(\"tree fromed\", treetojson)\n            prune_tree = pruneTree(root=json.loads(treetojson), columns_for_dummies=[])\n            df_temp = df[df['label'] == 1]\n            df1 = df_temp.agg(['min', 'max'])\n            df_json = json.loads(df1.to_json())\n            prune_tree = generate_boundries(features, df_json, prune_tree)\n            segement_names = ['trend', 'rfm']\n            segement_values = segements.segment_name.split(\"-\")[:-1]\n            prune_tree1 = add_clusters_rules(segement_names, segement_values, prune_tree)\n\n            final = rootToLeafConjugations(prune_tree)\n            print('after  rootToLeafConjugations ')\n            samples = sum(final.values())\n            rule = \" or \".join(list(final.keys()))\n\n            temp = self.usage_filter2.query(rule)\n\n            # for BTC purpose\n            dftnm = temp.copy()\n            msisdns = dftnm.pop(\"msisdn\")\n            dftnm = dftnm[features]\n            X = dftnm.drop(['msisdn', 'label'], axis=1, errors=\"ignore\")\n            # y = dftnm['label']\n            predictions = clf1.predict(X)\n            prediction_probabilities = clf1.predict_proba(X)\n            dftnm['predictions'] = predictions\n            dftnm['predict_proba_0'] = prediction_probabilities[:, 0]  # Probability of class 0\n            dftnm['predict_proba_1'] = prediction_probabilities[:, 1]  # Probability of class 1\n            dftnm['msisdn'] = msisdns\n            dftnm[\"segement_id\"] = segements.id\n            dftnm[\"recommended_product_id\"] = segements.recommended_product_id\n            segement_names = segement_names\n            segement_values = segements.segment_name.split(\"-\")[:-1]\n            dftnm[segement_names[0]] = segement_values[0]\n            dftnm[segement_names[1]] = segement_values[1]\n            dftnm[\"segement_name\"] = segements.segment_name\n            dftnm = dftnm[\n                [\"msisdn\",\"segement_id\",\"segement_name\", segement_names[0], segement_names[1], \"predictions\", \"predict_proba_0\", \"predict_proba_1\",\"recommended_product_id\"]]\n            # path_op = os.path.join(cfg.Config.ml_location, self.dag_run_id, \"output_data\")\n            # Path(path_op).mkdir(parents=True, exist_ok=True)\n            # dftnm.to_csv(os.path.join(path_op, segements.segment_name + \".csv\"), header=True, index=False)\n            dftnm_ls.append(dftnm)\n            dftnm_final = pd.concat(dftnm_ls)\n            self.op = dftnm_final\n\n\n            # segements.rule = json.dumps(prune_tree1)\n\n            # segements.rule = make_request(prune_tree1)\n            segements.rule = \"testing\"\n            segements.query = rule\n            segements.actual_target_count = len(dftnm)\n            \n            #print('rule is', rule)\n            print('going to query')\n            #print('columns of self.usage_filter2 is ', self.usage_filter2.columns)\n            \n\n            filename = os.path.join(ml_location,  'query_count.pickle')\n            if os.path.isfile(filename):\n                # Read data from file\n                with open(filename, 'rb') as handle:\n                    data_dict_query = pickle.load(handle)\n\n                    \n            print('len of temp is ', len(temp))\n            segment_name_qr = segements.segment_name\n            print('segment_name_qr is ',segment_name_qr)\n            print('type of segement_values is', type(segment_name_qr))\n\n            data_dict_query[segment_name_qr] = len(temp)\n\n            print('len appended')\n           \n            with open(filename, 'wb') as handle:\n                pickle.dump(data_dict_query, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n            \n\n            \n            \n            if temp is None or len(temp) == 0:\n                continue\n            print('len of temp is ', len(temp))\n\n            op_path_dir = os.path.join(ml_location,  'segment_data')\n            Path(op_path_dir).mkdir(parents=True, exist_ok=True)\n\n            print(int(float(segements.recommended_product_id)))\n            anti_list = [str(x) for x in segements.current_product.split(\"|\")]\n            anti_prod = ''.join(str(e) for e in anti_list)\n            print('anti_prod is', anti_prod)\n            print('segement_values is',segement_values)\n            filename = '_'.join(str(e) for e in segement_values)+'_'+str(int(float(segements.recommended_product_id)))+'_'+anti_prod+'.csv'\n            print('filename is ',filename)\n\n            \n            df_sub=df.query(rule)\n            \n            df_sub.to_csv(os.path.join(op_path_dir,filename), header=True, index=False)\n\n            segements.samples = samples\n\n            #segements.current_arpu = int(temp['m1_total_revenue'].mean())\n            total_revenue = int(temp['m1_total_revenue'].sum())\n            segements.total_revenue=total_revenue\n\n            current_product_id = segements.current_product\n            rec_product_id = int(float(segements.recommended_product_id))\n\n            incremental_revenue = get_prod_diff(current_product_id, rec_product_id, self.pack_info) * samples\n            incremental_revenue = (incremental_revenue / 80) * 100\n            initial_sum = temp['m1_total_revenue'].sum()\n            #uplift = (incremental_revenue / initial_sum) * 100\n            \n            current_product_price = GetCurrentPackprice(current_product_id, self.pack_info)\n            reco_product_price = getpackprice(rec_product_id,self.pack_info)\n            segements.uplift_revenue = reco_product_price-current_product_price\n\n            prodd_diff= reco_product_price-current_product_price\n            # uplift=(prodd_diff/current_product_price) *100\n            # segements.uplift = round(uplift, 2)\n            segements.incremental_revenue = incremental_revenue\n\n\n            segment_length=str(len(self.df))\n            print('segment_length is' ,segment_length)\n\n            segements.segment_length = segment_length\n\n\n\n            #segements.predicted_arpu = int(segements.current_arpu + ((segements.current_arpu*uplift)/100))\n\n            segment_list.append(segements)\n#             print('segment_list============>',segment_list)\n        return segment_list\n\n\ndef load_picke_file(filename):\n    with open(filename, 'rb') as handle:\n        data = pickle.load(handle)\n    return data\n\n\n\ndef filter_data(temp_df):\n    cols = [p + \"_\" + s for s in CUSTOMER_NEEDED_COLUMN for p in usage_no_months]\n    cols.append(\"msisdn\")\n    temp_df = temp_df[cols]\n    return temp_df\n\n\ndef otliner_removal(df, per=0.97):\n    try:\n        # df = df[\"needed_col\"]\n        tot_rev = CUSTOMER_TOTAL_REVENUE[0]\n        q = df[tot_rev].quantile(per)\n        print(\"the length brfore is\", len(df))\n        df = df[df[tot_rev] < q]\n        print(\"the length after is\", len(df))\n        return df\n    except Exception as e:\n        print(e)\n        raise RuntimeError(e)\n        \n        \n\ndef rule_generation(dag_run_id, db):\n\n\n    file_name_dict = get_file_names()\n    print(\"finding trend ongoing \")\n    data = {}\n    for month in usage_no_months:\n        data[month] = dd.read_csv(os.path.join(etl_location, file_name_dict.get(\"usage\").get(month)),\n                                  dtype=CUSTOMER_DTYPES)\n    months =usage_no_months\n    temp_df = None\n\n    for month in months:\n        df = data.get(month)\n        df = df.fillna(0)\n        total_revenue = CUSTOMER_TOTAL_REVENUE[0]\n        # df['tot_rev'] = df[total_revenue]\n        df = otliner_removal(df.copy())\n        df = df.add_prefix(f\"{month}_\")\n        df = df.rename(columns={f\"{month}_msisdn\": \"msisdn\"})\n        if temp_df is None:\n            temp_df = df\n        else:\n\n            temp_df = temp_df.merge(df, on='msisdn', how=\"left\")\n            temp_df = temp_df.fillna(0)\n    temp_df = temp_df.compute()\n    temp_df = filter_data(temp_df)\n    result_dict_path = os.path.join(ml_location,  \"purchased_for_association\", 'dict.pickle')\n    data_path = os.path.join(ml_location,  \"dict.pickle\")\n    data_dict = load_picke_file(data_path)\n    pack_info = os.path.join(etl_location, 'pack_info.csv')\n    pack_info_df = pd.read_csv(pack_info)\n    data = load_picke_file(result_dict_path)\n\n    # for key, value in data_dict.items():\n    #     data_dict[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n\n    # for key, value in data.items():\n    #     if value is None:\n    #         continue\n    #     data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", \"/opt/seahorse_docker_data/\")\n\n    filtered_dict = {k: v for k, v in data.items()}\n    filtered_data__dict = {k: v for k, v in data_dict.items()}\n\n    # btc purpose\n    output_d_list = []\n    # btc purpose end\n\n\n    for item, val in filtered_dict.items():\n        # data = pd.read_csv(filtered_data__dict.get(item))\n        # if filtered_data__dict.get(item) is None:\n        #     continue\n        # else:\n        data = pd.read_csv(filtered_data__dict.get(item))\n            \n            \n        try:\n            purchase = pd.read_csv(val)\n        except:\n            purchase_none_segements = SegementRepo.findByAutoPilotIdAndSegementNamewithoutcluster(db=db, _id=dag_run_id,\n                                                                        segement_name=segements_name)\n            for seg in purchase_none_segements:\n                print(f\"the segment name{seg.segment_name} and segment id is {seg.id}\")\n                ic(\"deleteing segement if they dont have purchase \")\n                SegementRepo.deleteById(db=db, _ids=[seg.id])\n            continue\n\n\n        #for 1 month purchase \n        #purchase = pd.read_csv('/log/btc_autopilot/etl_files/purchase_feb.csv')\n\n        #for 1 month purchase\n\n\n        for cluster in data['label'].unique():\n            segements_name = f\"{item}-{str(cluster)}\"\n            print('segements_name============>',segements_name)\n            data_temp = data[data['label'] == cluster]\n            #purchase_filtered = purchase[purchase[msisdn_name].isin(data_temp[msisdn_name])]\n\n            purchase_filtered = purchase[purchase[msisdn_name].isin(data_temp[msisdn_name])]\n\n            print('len of purchase in association_process ', len(purchase_filtered))\n            rg = RuleGenerator(df=data_temp, dag_run_id=dag_run_id, cluster_name=item, cluster_number=cluster,\n                               purchase_filtered=purchase_filtered, pack_info=pack_info_df)\n            segements = SegementRepo.findByAutoPilotIdAndSegementNameAll(db=db, _id=dag_run_id,\n                                                                         segement_name=segements_name,\n                                                                         cluster_number=int(cluster))\n\n\n\n            print(\"len of seg---------\",len(segements))\n            if segements is None:\n                print('segments is none')\n                continue\n\n            # pd.to_csv(\"seg.csv\")\n\n            # check service info for which segement to give which serives\n            # service_df = pd.read_sql_table('service_info', engine)\n            # segements_data = item.split(\"-\")\n            # trent = segements_data[0]\n            # rfm = segements_data[1]\n            # q = f\"trend == '{trent}' and rfm == '{rfm}'\"\n            # service_df = service_df.query(q).iloc[:, 3:]\n            # if not service_df.empty and service_df.all().all():\n            #     print(\"All boolean columns are True.\")\n            # else:\n            #     print(\"Not all boolean columns are True or no rows matched the query.\")\n\n            segements_new = rg.generate_rules(segements, temp_df)\n            # btc purpose\n            if rg.op is  not None :\n                output_d_list.append(rg.op)\n            # btc purpose end\n            for seg in segements_new:\n                if seg is not None:\n                    if seg.rule is not None:\n                        SegementRepo.update(db=db, item_data=seg)\n                    else:\n                        print(\"deleting segement\")\n                        SegementRepo.deleteById(db=db, _ids=[seg.id])\n            segements_two = SegementRepo.findByAutoPilotIdAndSegementNameAll(db=db, _id=dag_run_id,\n                                                                        segement_name=segements_name,\n                                                                        cluster_number=int(cluster))\n\n\n            for seg in segements_two:\n                print(f\"the segment name{seg.segment_name} and segment id is {seg.id}\")\n                if seg.samples < 10:\n                    ic(\"deleteing segement with less samples \")\n                    SegementRepo.deleteById(db=db, _ids=[seg.id])\n                if seg.rule is  None or len(seg.rule) < 3:\n                    ic(\"deleteing segement if rule is none or len is less than 3 \")\n                    print(\"seg.id is \",seg.id)\n                    SegementRepo.deleteById(db=db, _ids=[seg.id])\n                if seg.recommended_product_id is  None or seg.recommended_product_id ==0:\n                    ic(\"deleteing segement if recommended_product_id is none \")\n                    print(\"seg.id is \",seg.id)\n                    SegementRepo.deleteById(db=db, _ids=[seg.id])\n                for rfm_seg in not_needed_rfm_segment:\n                    if  rfm_seg in seg.segment_name:\n                        ic(\"deleteing segements that have no needed rfm segments\")\n                        SegementRepo.deleteById(db=db, _ids=[seg.id])\n                # recommended_pid = int(seg.recommended_product_id)\n                # print('recommended_pid is ',recommended_pid)\n                # print('type recommended_pid is ',type(recommended_pid))\n\n                # print('type next_purchase_date_df is ',next_purchase_date_df.dtypes)\n\n                # print('len is ', weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id'] == recommended_pid])\n                # print(\"weekday is \", weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0])\n                # seg.top_purchased_day_1 = weekday_product_purchase_count_df.loc[weekday_product_purchase_count_df['product_id'] == recommended_pid, 'top_purchased_day_1'].iloc[0]\n                # seg.top_purchased_day_2=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_2'].iloc[0]\n                # seg.top_purchased_day_3=weekday_product_purchase_count_df[weekday_product_purchase_count_df['product_id']==recommended_pid]['top_purchased_day_3'].iloc[0]\n                # seg.next_purchase_date_range=next_purchase_date_df[next_purchase_date_df['recommended_product_id']==recommended_pid]['range'].iloc[0]\n                # SegementRepo.update(db=db, item_data=seg)\n\n\n    # btc purpose\n    path_op = os.path.join(ml_location,  \"output_data\")\n    Path(path_op).mkdir(parents=True, exist_ok=True)\n    print(output_d_list)        \n    dftnm = pd.concat(output_d_list)\n    dftnm['dag_run_id']=dag_run_id\n\n\n    dftnm.to_csv(os.path.join(path_op, \"APA_output.csv\"), header=True, index=False)\n\n    # for df_chunk in pd.read_csv(os.path.join(path_op, \"APA_output.csv\"),\n    #                             chunksize=10000):\n    #     # insert each chunk into database table\n    #     df_chunk.to_sql('APA_output', engine, if_exists='append', index=False)\n\n\n#         btc purpose end\n    return dftnm\n\n\n\n\ndef transform(dataframe):\n    db = get_db()\n    df = rule_generation(\"manual__2023-07-10T11:06:51\",db)\n    return df"
      }
    }, {
      "id": "571786c1-c973-9808-b04f-ecad634d7974",
      "operation": {
        "id": "a721fe2a-5d7f-44b3-a1e7-aade16252ead",
        "name": "Python Transformation"
      },
      "parameters": {
        "code": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nimport sklearn\nimport json\nimport dask.dataframe as dd\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom icecream import ic\nimport dask.dataframe as dd\nimport os\n# import configuration.config as cfg\n# import configuration.features as f\nimport traceback\nimport numpy as np\n# from fastapi import Depends, FastAPI, HTTPException\nfrom pathlib import Path\nimport pickle\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom pathlib import Path\nimport requests\n# from sql_app.repositories import AssociationRepo\nimport pandas as pd\nimport pickle\nimport dask.dataframe as dd\nimport os\nimport traceback\nimport numpy as np\n\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom urllib.parse import quote  \n\n\n\n\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import Column, ForeignKey, Integer, String, Float, DateTime,Boolean\nfrom sqlalchemy.orm import relationship\nimport datetime\nfrom sqlalchemy.dialects.mysql import LONGTEXT\n\n\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\n\n\n\nSQLALCHEMY_DATABASE_URL = 'mysql+mysqlconnector://ruleuser:%s@10.0.0.69:3306/ROBOX'% quote('ruleuser@6D')\n\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URL,\n)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        return db\n    finally:\n        db.close()\n        \n        \n        \n\nml_location = '/home/tnmops/seahorse3_bkp/'\n\n\nclass SegementInfo(BaseModel):\n    end_date: Optional[str] = None\n    dag_run_id: Optional[str] = None\n    current_product: Optional[str] = None\n    current_products_names: Optional[str] = None\n    recommended_product_id: Optional[str] = None\n    recommended_product_name: Optional[str] = None\n    predicted_arpu: Optional[int] = None\n    current_arpu: Optional[int] = None\n    segment_length: Optional[str] = None\n    rule: Optional[str] = None\n    actual_rule: Optional[str] = None\n    uplift_percent: Optional[float] = None\n    incremental_revenue: Optional[float] = None\n    campaign_type: Optional[str] = None\n    campaign_name: Optional[str] = None\n    action_key: Optional[str] = None\n    robox_id: Optional[str] = None\n    samples: Optional[int] = None\n    segment_name: Optional[str] = None\n    current_ARPU_band: Optional[str] = None\n    current_revenue_impact: Optional[str] = None\n    customer_status: Optional[str] = None\n    query: Optional[str] = None\n    cluster_no: Optional[int] = None\n    confidence: Optional[float] = None\n    recommendation_type: Optional[str] = None\n    cluster_description: Optional[str] = None\n    actual_target_count: Optional[str] = None\n    top_purchased_day_1: Optional[str] = None\n    top_purchased_day_2: Optional[str] = None\n    top_purchased_day_3: Optional[str] = None\n    next_purchase_date_range: Optional[str] = None\n    campaign_response_percentage: Optional[str] = None\n    total_revenue: Optional[int] = None\n    uplift_revenue: Optional[int] = None\n    \n    \n    \nclass SegmentInformation(Base):\n    __tablename__ = \"APA_segment_information_new\"\n    id = Column(Integer, primary_key=True, index=True)\n    created_date = Column(DateTime, default=datetime.datetime.utcnow)\n    end_date = Column(DateTime)\n    current_product = Column(String(80), nullable=True, unique=False)\n    current_products_names = Column(String(200), nullable=True, unique=False)\n    recommended_product_id = Column(String(80), nullable=True, unique=False)\n    recommended_product_name = Column(String(80), nullable=True, unique=False)\n    predicted_arpu = Column(Integer, nullable=True)\n    current_arpu = Column(Integer, nullable=True)\n    segment_length = Column(String(80), nullable=True, unique=False)\n    rule = Column(LONGTEXT, nullable=True)\n    actual_rule = Column(LONGTEXT, nullable=True)\n    uplift_percent = Column(Float(precision=2), nullable=True)\n    incremental_revenue = Column(Float(precision=2), nullable=True)\n    campaign_type = Column(String(80), nullable=True, unique=False)\n    campaign_name = Column(String(80), nullable=True, unique=False)\n    action_key = Column(String(80), nullable=True, unique=False)\n    robox_id = Column(String(80), nullable=True, unique=False)\n    dag_run_id = Column(String(80), nullable=True, unique=False)\n    samples = Column(Integer, nullable=False)\n    segment_name = Column(String(80), nullable=True, unique=False)\n    current_ARPU_band = Column(String(80), nullable=True, unique=False)\n    current_revenue_impact = Column(String(80), nullable=True, unique=False)\n    customer_status = Column(String(80), nullable=True, unique=False)\n    query = Column(LONGTEXT, nullable=True, unique=False)\n    cluster_no = Column(Integer, nullable=True)\n    confidence = Column(Float(precision=2), nullable=True)\n    recommendation_type = Column(String(80), nullable=True, unique=False)\n    cluster_description = Column(LONGTEXT, nullable=True)\n    actual_target_count = Column(Integer, nullable=True)\n    top_purchased_day_1= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_2= Column(String(80), nullable=True, unique=False)\n    top_purchased_day_3= Column(String(80), nullable=True, unique=False)\n    next_purchase_date_range= Column(String(80), nullable=True, unique=False)\n    campaign_response_percentage = Column(String(80), nullable=True, unique=False)\n    total_revenue= Column(Integer, nullable=True)\n    uplift_revenue=Column(Integer, nullable=True)\n\n    def __repr__(self):\n        return 'SegmentInformation(name=%s)' % self.name\n        \n        \nclass SegementRepo:\n    def create(db: Session, segement: SegementInfo):\n        db_item = SegmentInformation(dag_run_id=segement.dag_run_id, actual_rule=segement.actual_rule,\n                                            campaign_type=segement.campaign_type,\n                                            current_arpu=segement.current_arpu, predicted_arpu=segement.predicted_arpu,\n                                            uplift_percent=segement.uplift_percent, incremental_revenue=segement.incremental_revenue,\n                                            rule=segement.rule, samples=segement.samples,\n                                            campaign_name=segement.campaign_name,\n                                            recommended_product_id=segement.recommended_product_id,\n                                            recommended_product_name=segement.recommended_product_name,\n                                            current_product=segement.current_product,\n                                            current_products_names=segement.current_products_names,\n                                            segment_length=segement.segment_length,\n                                            current_ARPU_band=segement.current_ARPU_band,\n                                            current_revenue_impact=segement.current_revenue_impact,\n                                            customer_status=segement.customer_status,\n                                            segment_name=segement.segment_name, query=segement.query,\n                                            cluster_no=segement.cluster_no, confidence=segement.confidence,\n                                            recommendation_type=segement.recommendation_type,\n                                            cluster_description=segement.cluster_description,\n                                            actual_target_count=segement.actual_target_count,\n                                            top_purchased_day_1=segement.top_purchased_day_1,\n                                            top_purchased_day_2=segement.top_purchased_day_2,\n                                            top_purchased_day_3=segement.top_purchased_day_3,\n                                            next_purchase_date_range=segement.next_purchase_date_range,\n                                            campaign_response_percentage=segement.campaign_response_percentage,\n                                            total_revenue=segement.total_revenue,\n                                            uplift_revenue=segement.uplift_revenue\n                                            )\n        db.add(db_item)\n        db.commit()\n        db.refresh(db_item)\n        return db_item\n        \n        \nclass RuleExtreaction(object):\n    def __init__(self, dag_run_id=None, features_path=None, path_d=None, db=None):\n        self.path_d = path_d\n        self.dag_run_id = dag_run_id\n        self.features_path = features_path\n        self.db = db\n\n        # going to load this varibles\n        self.data = None\n        self.filtered_dict = None\n        self.data_feature = None\n        # loading\n\n        self.load_pickle()\n        self.filter_dict()\n\n    def load_pickle(self):\n        self.data = load_picke_file(self.path_d)\n        self.data_feature = load_picke_file(self.features_path)\n        \n        \n    def filter_dict(self):\n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\n        # self.filtered_dict = {k: v for k, v in self.data.items() if k in needed_segements}\n        self.filtered_dict = {k: v for k, v in self.data.items()}\n\n    def execute(self):\n        \n        \n        \n        for item, val in self.filtered_dict.items():\n            # val is the path item is the segment name\n            \n            df = pd.read_csv(val)            \n            \n            cluster_conditions = extract_rules(df, self.data_feature[item])\n            print('++++++++++++++++++++++++++++++++++++++++++',cluster_conditions)\n            if cluster_conditions is None:\n                print(f\" the segment {item} is none ,  the path is {val}\")\n            \n            for cluster, rule in cluster_conditions.items():\n                try:\n                    \n                    print('cluster===',cluster)\n                    print('rule==',rule)\n\n                    info = SegementInfo()\n                    # info.actual_rule\n                    info.dag_run_id = self.dag_run_id\n                    info.segment_name = f\"{item}-{str(cluster)}\"\n                    info.segment_length = str(len(df))\n                    info.customer_status = \"active\"\n                    info.cluster_description = rule\n                    info.cluster_no = int(cluster)\n                    if len(rule) > 1:\n                        info.samples = 15\n                    else:\n                        info.samples = 0\n                    SegementRepo.create(db=self.db, segement=info)\n                except Exception as e:\n                    print(\" error occorured in rule insertion \")\n                    print(e)\n                    raise ValueError(e)\n                    \n                    \ndef load_picke_file(filename):\n    with open(filename, 'rb') as handle:\n        data = pickle.load(handle)\n    return data\n\n\n\ndef extract_rules(df, features):\n    \n    cluster_conditions = {}\n    try:\n    \n\n        df1 = df[features + [\"label\"]]       \n        \n        cluster_modes = {}       \n        \n        for cluster in df1['label'].unique():\n            \n            print('-----------------',cluster)\n            print(df1.info())\n            temp = df1[df1['label'] == cluster]\n            cluster_modes[cluster] = temp[features].mean().to_dict()\n#             cluster_df = pd.get_dummies(temp[temp.columns[temp.dtypes == 'object']], prefix_sep=\":\",columns=temp.columns[temp.dtypes == 'object'])\n#             cluster_modes[cluster] = cluster_df.mode().loc[cluster_df.mode().sum(axis=1).idxmax()].to_dict()\n\n\n        # Step 3: Form the conditions for each cluster\n        \n        \n        for cluster, modes in cluster_modes.items():\n            op_li = []\n            for key, value in modes.items():\n                op = f\"{key} >= {value}\"\n                op_li.append(op)\n            condition = \" & \".join(op_li)\n            cluster_conditions[int(cluster)] = f\"{condition}\"\n    except Exception as e:\n        print(\"extract_rules: error occurred \" + str(e))\n        return cluster_conditions\n    return cluster_conditions\n        \n#         for cluster, modes in cluster_modes.items():\n            \n#             print('cluster=============++++++',cluster)\n#             print('modes=============++++++',modes)\n            \n#             modes = {k: v for k, v in modes.items() if v == 1}\n#             print('modes =======',modes)\n#             op_li = []\n#             for key in modes.keys():\n                \n#                 print('keys=========',key)\n                \n#                 k = key.split(\":\")[0]\n#                 v = key.split(\":\")[1]\n#                 op = f\"{k} == '{v}'\"\n#                 op_li.append(op)\n#             condition = \" & \".join(op_li)\n#             # condition = ' & '.join([f\"{col} == {mode}\" for col, mode in modes.items() if col != 'label'])\n#             cluster_conditions[int(cluster)] = f\"{condition}\"\n#     except Exception as e:\n#         print(\"extract_rules :  error occoured \" + str(e))\n#         return cluster_conditions\n#     return cluster_conditions\n\n\ndef rule_extraction(dag_run_id, db):\n    try:\n\n        path_d = os.path.join(ml_location, \"dict.pickle\")\n        features_path = os.path.join(ml_location, \"features.pickle\")\n        print(\"features_path\",features_path)\n        \n        data = load_picke_file(path_d)\n        print(\"data loaded \")\n        \n        data_feature = load_picke_file(features_path)\n        print(\"data_feature loaded \")\n        \n        for key, value in data.items():\n            data[key] = value.replace(\"/home/tnmops/seahorse3_bkp/\", r\"/opt/seahorse_docker_data/\")\n\n        re = RuleExtreaction(dag_run_id=dag_run_id, db=db, features_path=features_path, path_d=path_d)\n        re.execute()\n        \n        # needed_segements = [\"Uptrend_Champions\", \"Uptrend_Loyal_Customers\"]\n        # filtered_dict = {k: v for k, v in data.items() if k in needed_segements}\n        # for item, val in filtered_dict.items():\n        #     # val is the path item is the segment name\n        #     df = pd.read_csv(val)\n        #     print(f\"the cluster is {item}\")\n        #     cluster_conditions = extract_rules(df, data_feature[item])\n        #     if cluster_conditions is None:\n        #         print(f\" the segment {item} is none ,  the path is {val}\")\n        #     for cluster, rule in cluster_conditions:\n        #         try:\n        #\n        #             info = schemas.SegementInfo\n        #             info.dag_run_id = dag_run_id\n        #             info.segment_name = f\"{item}_{str(cluster)}\"\n        #             info.segment_length = str(len(df))\n        #             info.customer_status = \"active\"\n        #             info.query = rule\n        #             info.samples = len(df.query(rule))\n        #             SegementRepo.create(db=db, segement=info)\n        #         except Exception as e:\n        #             print(\" error occorured in rule insertion \")\n        #             print(e)\n        #             raise e\n        #\n\n    except Exception as e:\n        print(\"the error occoured in form_data\", e)\n        raise ValueError(e)\n        \n        \ndef transform(dataframe):\n    dag_run_id = \"manual__2023-07-10T11:06:51\"\n    db = get_db()\n\n    rule_extraction(dag_run_id, db)\n    return dataframe"
      }
    }],
    "connections": [{
      "from": {
        "nodeId": "403b70cb-2a48-4121-1946-9e40d96be773",
        "portIndex": 0
      },
      "to": {
        "nodeId": "428ec8be-3805-61e3-4b04-7c5d29922ad2",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "e0b0c6c6-54d9-c40e-d98f-364b6e84656e",
        "portIndex": 0
      },
      "to": {
        "nodeId": "c894bdc6-be02-71a9-46ea-2d76497b7381",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "b26bb055-2532-1ef4-c64b-7682ac02f864",
        "portIndex": 0
      },
      "to": {
        "nodeId": "6b8394f4-4963-700f-3ccb-e636808024dc",
        "portIndex": 1
      }
    }, {
      "from": {
        "nodeId": "1d706b8d-0eb5-1544-ed5f-335dc4e47cbd",
        "portIndex": 0
      },
      "to": {
        "nodeId": "61e42a89-d515-6c47-5856-b0329358e276",
        "portIndex": 1
      }
    }, {
      "from": {
        "nodeId": "03b7c3a8-5da5-b3dd-8ef8-9acc3ee69547",
        "portIndex": 0
      },
      "to": {
        "nodeId": "7aa6bc57-2961-2e85-cb94-e1d6e61dbe32",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "571786c1-c973-9808-b04f-ecad634d7974",
        "portIndex": 0
      },
      "to": {
        "nodeId": "e776af2a-0657-f30b-22fb-dc047d638e11",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "091ce8c8-ea88-0cc9-f4ff-e70b94f458e9",
        "portIndex": 0
      },
      "to": {
        "nodeId": "7088c32f-af5e-aa2a-0f18-91a1f307ade4",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "5e6b9e27-3486-806c-2578-216fe8adc948",
        "portIndex": 0
      },
      "to": {
        "nodeId": "24abe326-24b1-11b0-7540-16787330d5cd",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "44f1d3eb-74ab-318d-319c-40f04be64a9a",
        "portIndex": 0
      },
      "to": {
        "nodeId": "3ce83530-6bff-53a8-4f0e-3617ef1503bc",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "6416e64c-29a5-4e80-3773-e8e815082a8b",
        "portIndex": 0
      },
      "to": {
        "nodeId": "e0b0c6c6-54d9-c40e-d98f-364b6e84656e",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "29fc9508-6060-6ef9-5b6b-543b69d6b22d",
        "portIndex": 0
      },
      "to": {
        "nodeId": "b26bb055-2532-1ef4-c64b-7682ac02f864",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "b9ff44b6-80a4-5885-1234-fa06a9ed8928",
        "portIndex": 0
      },
      "to": {
        "nodeId": "636d762f-ef80-ecb6-46d3-dedf7a439242",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "25f2b9ac-2efa-a3bb-990a-c58ecab04555",
        "portIndex": 0
      },
      "to": {
        "nodeId": "6b8394f4-4963-700f-3ccb-e636808024dc",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "61e42a89-d515-6c47-5856-b0329358e276",
        "portIndex": 0
      },
      "to": {
        "nodeId": "636d762f-ef80-ecb6-46d3-dedf7a439242",
        "portIndex": 1
      }
    }, {
      "from": {
        "nodeId": "60f149ca-2650-9efb-dc53-09b00343bf36",
        "portIndex": 0
      },
      "to": {
        "nodeId": "2e771569-425d-6d2d-acca-baefc3094ecc",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "7aa6bc57-2961-2e85-cb94-e1d6e61dbe32",
        "portIndex": 0
      },
      "to": {
        "nodeId": "571786c1-c973-9808-b04f-ecad634d7974",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "3ce83530-6bff-53a8-4f0e-3617ef1503bc",
        "portIndex": 0
      },
      "to": {
        "nodeId": "6416e64c-29a5-4e80-3773-e8e815082a8b",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "e776af2a-0657-f30b-22fb-dc047d638e11",
        "portIndex": 0
      },
      "to": {
        "nodeId": "bbe00a0b-37b1-ce58-2702-875a0b3bb296",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "636d762f-ef80-ecb6-46d3-dedf7a439242",
        "portIndex": 0
      },
      "to": {
        "nodeId": "2e771569-425d-6d2d-acca-baefc3094ecc",
        "portIndex": 1
      }
    }, {
      "from": {
        "nodeId": "b827c05d-8b4f-901c-d11f-2af2ae56201c",
        "portIndex": 0
      },
      "to": {
        "nodeId": "b26bb055-2532-1ef4-c64b-7682ac02f864",
        "portIndex": 1
      }
    }, {
      "from": {
        "nodeId": "428ec8be-3805-61e3-4b04-7c5d29922ad2",
        "portIndex": 0
      },
      "to": {
        "nodeId": "1d706b8d-0eb5-1544-ed5f-335dc4e47cbd",
        "portIndex": 1
      }
    }, {
      "from": {
        "nodeId": "444458a0-a78d-e08c-16d2-3cb867961cbf",
        "portIndex": 0
      },
      "to": {
        "nodeId": "091ce8c8-ea88-0cc9-f4ff-e70b94f458e9",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "bbe00a0b-37b1-ce58-2702-875a0b3bb296",
        "portIndex": 0
      },
      "to": {
        "nodeId": "444458a0-a78d-e08c-16d2-3cb867961cbf",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "a6b06cd3-da8e-8500-9273-9945bf7ed450",
        "portIndex": 0
      },
      "to": {
        "nodeId": "1d706b8d-0eb5-1544-ed5f-335dc4e47cbd",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "6b8394f4-4963-700f-3ccb-e636808024dc",
        "portIndex": 0
      },
      "to": {
        "nodeId": "03b7c3a8-5da5-b3dd-8ef8-9acc3ee69547",
        "portIndex": 0
      }
    }, {
      "from": {
        "nodeId": "f75dc863-e698-0fbf-5269-078bcac87233",
        "portIndex": 0
      },
      "to": {
        "nodeId": "61e42a89-d515-6c47-5856-b0329358e276",
        "portIndex": 0
      }
    }]
  },
  "thirdPartyData": {
    "gui": {
      "name": "Copy of Copy of Autopilot_version_1.1",
      "description": "The complete flow of autopilot",
      "nodes": {
        "091ce8c8-ea88-0cc9-f4ff-e70b94f458e9": {
          "uiName": "next_purchase_day",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4593,
            "y": 7807
          }
        },
        "e776af2a-0657-f30b-22fb-dc047d638e11": {
          "uiName": "association_process",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4589,
            "y": 7509
          }
        },
        "29fc9508-6060-6ef9-5b6b-543b69d6b22d": {
          "uiName": "preprocess",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4936,
            "y": 7075
          }
        },
        "5665b055-50b8-1edf-40a9-d5b884a0ee2f": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4931,
            "y": 7355
          }
        },
        "571786c1-c973-9808-b04f-ecad634d7974": {
          "uiName": "rule_extraction",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4595,
            "y": 7386
          }
        },
        "f75dc863-e698-0fbf-5269-078bcac87233": {
          "uiName": "for segmentation",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5479,
            "y": 7484
          }
        },
        "2e771569-425d-6d2d-acca-baefc3094ecc": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5750,
            "y": 7885
          }
        },
        "444458a0-a78d-e08c-16d2-3cb867961cbf": {
          "uiName": "churn_prediction",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4595,
            "y": 7704
          }
        },
        "c894bdc6-be02-71a9-46ea-2d76497b7381": {
          "uiName": "purchase_process",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4321,
            "y": 7525
          }
        },
        "428ec8be-3805-61e3-4b04-7c5d29922ad2": {
          "uiName": "after merging usage and per_drop",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5744,
            "y": 7272
          }
        },
        "7088c32f-af5e-aa2a-0f18-91a1f307ade4": {
          "uiName": "campaign_response_model",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4596,
            "y": 7913
          }
        },
        "e0b0c6c6-54d9-c40e-d98f-364b6e84656e": {
          "uiName": "recharge_process",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4332,
            "y": 7407
          }
        },
        "3ce83530-6bff-53a8-4f0e-3617ef1503bc": {
          "uiName": "usage_process",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4347,
            "y": 7204
          }
        },
        "7aa6bc57-2961-2e85-cb94-e1d6e61dbe32": {
          "uiName": "feature_selection",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5170,
            "y": 7529
          }
        },
        "1d706b8d-0eb5-1544-ed5f-335dc4e47cbd": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5751,
            "y": 7416
          }
        },
        "63d7e415-bda3-ac95-06d8-d4289cbf19e5": {
          "uiName": "segmentation",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4319,
            "y": 7618
          }
        },
        "44f1d3eb-74ab-318d-319c-40f04be64a9a": {
          "uiName": "rfm_purchase",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4359,
            "y": 7111
          }
        },
        "24abe326-24b1-11b0-7540-16787330d5cd": {
          "uiName": "matrix_multiplication",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4279,
            "y": 7812
          }
        },
        "403b70cb-2a48-4121-1946-9e40d96be773": {
          "uiName": "to perform usage_process",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5467,
            "y": 7162
          }
        },
        "b9ff44b6-80a4-5885-1234-fa06a9ed8928": {
          "uiName": "matrix_operation",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5483,
            "y": 7635
          }
        },
        "b26bb055-2532-1ef4-c64b-7682ac02f864": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5150,
            "y": 7161
          }
        },
        "5e6b9e27-3486-806c-2578-216fe8adc948": {
          "uiName": "read_and_preprocess",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4310,
            "y": 7709
          }
        },
        "6416e64c-29a5-4e80-3773-e8e815082a8b": {
          "uiName": "active_inactive",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4337,
            "y": 7306
          }
        },
        "a075c8d5-77f3-cc04-594e-c81571900515": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5660,
            "y": 7062
          }
        },
        "60f149ca-2650-9efb-dc53-09b00343bf36": {
          "uiName": "ml_operations",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5487,
            "y": 7779
          }
        },
        "b827c05d-8b4f-901c-d11f-2af2ae56201c": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5126,
            "y": 6999
          }
        },
        "d3229fcd-d746-b2de-b09c-f5722655b83c": {
          "uiName": "matrix_filter",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5200,
            "y": 7774
          }
        },
        "a6b06cd3-da8e-8500-9273-9945bf7ed450": {
          "uiName": "fro purchase,recharge process and rfm",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5474,
            "y": 7308
          }
        },
        "25f2b9ac-2efa-a3bb-990a-c58ecab04555": {
          "uiName": "matrix_operation",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4933,
            "y": 7206
          }
        },
        "03b7c3a8-5da5-b3dd-8ef8-9acc3ee69547": {
          "uiName": "k_modes",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5144,
            "y": 7423
          }
        },
        "bbe00a0b-37b1-ce58-2702-875a0b3bb296": {
          "uiName": "rule_generation",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4585,
            "y": 7605
          }
        },
        "61e42a89-d515-6c47-5856-b0329358e276": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5756,
            "y": 7587
          }
        },
        "44acfe6c-f2ea-48b3-e490-5affa1c7cd15": {
          "uiName": "rfm_daily_summerised",
          "color": "#00B1EB",
          "coordinates": {
            "x": 4352,
            "y": 7014
          }
        },
        "6b8394f4-4963-700f-3ccb-e636808024dc": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5157,
            "y": 7292
          }
        },
        "636d762f-ef80-ecb6-46d3-dedf7a439242": {
          "uiName": "",
          "color": "#00B1EB",
          "coordinates": {
            "x": 5761,
            "y": 7745
          }
        }
      }
    },
    "notebooks": {

    },
    "datasources": [{
      "accessLevel": "writeRead",
      "params": {
        "name": "usage_march",
        "libraryFileParams": {
          "libraryPath": "library://usage_march.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-05T07:51:16.999Z",
      "id": "013f9012-1d89-3589-271b-f424e59d96e1",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "purchase_data",
        "libraryFileParams": {
          "libraryPath": "library://purchase_data.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-04T09:44:02.112Z",
      "id": "99d0d438-78fd-33d0-ee5c-cd8dcc77cdc0",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "weekwise_feb_20230517180929",
        "libraryFileParams": {
          "libraryPath": "library://weekwise_feb_20230517180929.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-06-27T07:01:30.231Z",
      "id": "d01b979c-6a9f-b6c3-f6fd-ce2d4831ea59",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "daily_summarized_feb",
        "libraryFileParams": {
          "libraryPath": "library://daily_summarized_feb (1).csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-04T09:01:08.358Z",
      "id": "9d8e63c0-0548-42a9-47ce-3f6850e65fe3",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "daily_summarized_jan",
        "libraryFileParams": {
          "libraryPath": "library://daily_summarized_jan (1).csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-04T09:01:18.527Z",
      "id": "6aa96681-19eb-9c6e-2b38-7cdd30eebcb2",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "usage_feb",
        "libraryFileParams": {
          "libraryPath": "library://usage_feb.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-05T07:51:05.613Z",
      "id": "a8dbba2f-156d-8cab-8734-e714679a3f9e",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "daily_weekly_avg_march_20230517163429 (1)",
        "libraryFileParams": {
          "libraryPath": "library://daily_weekly_avg_march_20230517163429 (1).csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-05T09:41:10.805Z",
      "id": "5fa1f504-ff85-225e-296d-4aae662140ae",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "purchase_dec",
        "libraryFileParams": {
          "libraryPath": "library://purchase_dec.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-20T08:06:14.361Z",
      "id": "70c171c2-cf38-7864-b7c9-e26211f7eff7",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "daily_summarized_data",
        "libraryFileParams": {
          "libraryPath": "library://daily_summarized_data (1).csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-04T09:00:55.662Z",
      "id": "dbc838c5-c9cc-2e19-a447-324fc9da023f",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "daily_summarized_march",
        "libraryFileParams": {
          "libraryPath": "library://daily_summarized_march (1).csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-04T09:01:34.586Z",
      "id": "e9fc88d5-3cff-7e48-7f59-c4f0721215f0",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "recharge_feb",
        "libraryFileParams": {
          "libraryPath": "library://recharge_feb.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-14T10:18:10.796Z",
      "id": "338c9523-aa3a-be34-ee4d-30aa1ac21285",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "daily_weekly_avg_jan_20230517163429 (1)",
        "libraryFileParams": {
          "libraryPath": "library://daily_weekly_avg_jan_20230517163429 (1).csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-05T09:41:05.417Z",
      "id": "54bc9c01-1208-efeb-c15a-a3c542b1796c",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "purchase_feb",
        "libraryFileParams": {
          "libraryPath": "library://purchase_feb.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-14T09:11:45.079Z",
      "id": "a6a0b872-eeba-819a-941b-d196150fe68c",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "usage_jan",
        "libraryFileParams": {
          "libraryPath": "library://usage_jan.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-05T07:51:10.915Z",
      "id": "1b4b1059-db26-da32-868c-2ce0fbbb95d0",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "recharge_march",
        "libraryFileParams": {
          "libraryPath": "library://recharge_march.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-14T10:18:21.850Z",
      "id": "a34ae56c-0b70-a4b0-b8e9-eecead0f4bf1",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "weekwise_march_20230517180929",
        "libraryFileParams": {
          "libraryPath": "library://weekwise_march_20230517180929.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-06-27T07:03:13.070Z",
      "id": "21b9451d-8670-f075-08fd-7d296cefa0b8",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "purchase_march",
        "libraryFileParams": {
          "libraryPath": "library://purchase_march.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-14T09:11:51.870Z",
      "id": "9d1079c4-b1f0-c724-b05c-a64d65aeaead",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "recharge_jan",
        "libraryFileParams": {
          "libraryPath": "library://recharge_jan.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-14T10:18:16.680Z",
      "id": "48b2dd6e-b3be-82ba-a4b6-0fb518c686dc",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "daily_weekly_avg_feb_20230517163429 (1)",
        "libraryFileParams": {
          "libraryPath": "library://daily_weekly_avg_feb_20230517163429 (1).csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-05T09:41:00.562Z",
      "id": "bf2eaebe-4cae-c159-8f33-e315608a1420",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "purchase_jan",
        "libraryFileParams": {
          "libraryPath": "library://purchase_jan.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-07-14T09:11:37.168Z",
      "id": "f5ea927d-16f9-4c18-87e3-e23f1933f5e5",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }, {
      "accessLevel": "writeRead",
      "params": {
        "name": "weekwise_jan_20230517180929",
        "libraryFileParams": {
          "libraryPath": "library://weekwise_jan_20230517180929.csv",
          "fileFormat": "csv",
          "csvFileFormatParams": {
            "includeHeader": true,
            "convert01ToBoolean": false,
            "separatorType": "comma",
            "customSeparator": null
          }
        },
        "downloadUri": null,
        "googleSpreadsheetParams": null,
        "jdbcParams": null,
        "hdfsParams": null,
        "externalFileParams": null,
        "datasourceType": "libraryFile",
        "visibility": "privateVisibility"
      },
      "creationDateTime": "2023-06-27T07:02:13.640Z",
      "id": "f2e8fd17-ac58-a93b-b542-830c5d1cd101",
      "ownerName": "user",
      "ownerId": "00000000-0000-0000-0000-000000000001"
    }]
  },
  "variables": {

  },
  "id": "a928001e-1786-4e1e-8e1c-5c8684a8f325",
  "metadata": {
    "type": "batch",
    "apiVersion": "1.4.3"
  }
}